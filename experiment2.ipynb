{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agavrilenko/anaconda3/lib/python3.6/site-packages/xgboost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.dirname(xgb.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round: int\n",
      "        Number of boosting iterations.\n",
      "    evals: list of pairs (DMatrix, string)\n",
      "        List of items to be evaluated during training, this allows user to watch\n",
      "        performance on the validation set.\n",
      "    obj : function\n",
      "        Customized objective function.\n",
      "    feval : function\n",
      "        Customized evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Validation error needs to decrease at least\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        If there's more than one, will use the last.\n",
      "        Returns the model from the last iteration (not the best one).\n",
      "        If early stopping occurs, the model will have three additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "        (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "        ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "    evals_result: dict\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval : bool or int\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    learning_rates: list or function (deprecated - use callback API instead)\n",
      "        List of learning rate for each boosting round\n",
      "        or a customized function that calculates eta in terms of\n",
      "        current number of round and the total number of boosting round (e.g. yields\n",
      "        learning rate decay)\n",
      "    xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(xgboost.training.train)\n",
    "help(xgb.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = loadtxt('data/data.txt', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data = X_train, label = y_train, silent= True)\n",
    "\n",
    "deval = xgb.DMatrix(data = X_test, label = y_test, silent= True)\n",
    "# evals_list = []\n",
    "# evals_list.append((deval, \"eval matrix\"))\n",
    "num_epochs = 500\n",
    "params = {}\n",
    "params['num_boost_round'] = num_epochs\n",
    "params['early_stopping_rounds'] = 1000\n",
    "#params['verbose_eval'] = 1\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['evals_result'] = {}\n",
    "params['max_depth'] = 3\n",
    "\n",
    "params['subsample'] = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = []\n",
    "tresholds = []\n",
    "learning_rates.append(0.1)\n",
    "counter = 0\n",
    "best = []\n",
    "progress = dict()\n",
    "wait = []\n",
    "old_lr = learning_rates[0]\n",
    "new_lr = learning_rates[0]\n",
    "def reduceLRonPlateau(i,n):\n",
    "\n",
    "    factor = 0.3\n",
    "    min_lr = 1e-30\n",
    "    min_delta = 1e-4\n",
    "    patience = 2\n",
    "    verbose = 0\n",
    "    cooldown = 3\n",
    "    cooldown_counter = 0  # Cooldown counter.\n",
    "    #wait = 0\n",
    "    #best = 0\n",
    "    mode = 'min'  \n",
    "    monitor_op = None\n",
    "    \n",
    "    \n",
    "    def in_cooldown():\n",
    "            return cooldown_counter > 0\n",
    "    \n",
    "    def _reset(mode):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\"\"\"\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "        if (mode == 'min' or\n",
    "           (mode == 'auto' and 'acc' not in monitor)):\n",
    "            monitor_op = lambda a, b: np.less(a, b - min_delta)\n",
    "            best = np.Inf\n",
    "        else:\n",
    "            monitor_op = lambda a, b: np.greater(a, b + min_delta)\n",
    "            best = -np.Inf\n",
    "            \n",
    "        cooldown_counter = 0\n",
    "        del wait[:]\n",
    "        return wait, cooldown_counter\n",
    "    \n",
    "    def checker(wait,best, new_lr, learning_rates):\n",
    "        if (i == 0):\n",
    "            wait, counter = _reset(mode)\n",
    "            print(\"initialization...\")\n",
    "            print(\"lr: \", learning_rates[0])\n",
    "            best.append(100)\n",
    "            \n",
    "        if (i>0):\n",
    "            old_lr = learning_rates[i-1]\n",
    "        \n",
    "            #new_lr = learning_rates[i-1]\n",
    "            current = progress['eval']['logloss'][i-1]\n",
    "            if in_cooldown():\n",
    "                cooldown_counter -= 1\n",
    "                wait = 0\n",
    "              \n",
    "            print(\"iter: \", i, \" Current_loss: \", current, \"best: \",best[i-1] - min_delta, \"less: \",np.less(current, best[i-1] - min_delta))\n",
    "            if np.less(current, best[i-1] - min_delta):\n",
    "                \n",
    "                best.append(current)\n",
    "                del wait[:]\n",
    "             \n",
    "            elif not in_cooldown():\n",
    "              #  print(\"not in_cooldown\")\n",
    "                best.append(best[-1])\n",
    "                wait.append(1)\n",
    "                if len(wait) >= patience:\n",
    "                    \n",
    "                    if old_lr > min_lr:\n",
    "                        new_lr = old_lr * factor\n",
    "                        new_lr = max(new_lr, min_lr)\n",
    "                        print(\"iter: \", i, \"reducing lr -- \", \" old_lr: \", old_lr, \" new_lr: \", new_lr)\n",
    "                        \n",
    "#                         if self.verbose > 0:\n",
    "#                             print('\\nEpoch %05d: ReduceLROnPlateau reducing '\n",
    "#                                   'learning rate to %s.' % (epoch + 1, new_lr))\n",
    "                        cooldown_counter = cooldown\n",
    "                        del wait[:]\n",
    "                    else:\n",
    "                        new_lr = old_lr\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "       # print(\"step \",progress['train']['logloss'][i-1], \"i = \", i)\n",
    "\n",
    "            \n",
    "    new_lr = learning_rates[i-1]\n",
    "    new_lr = checker(wait,best,new_lr,learning_rates)\n",
    "    learning_rates.append(new_lr)\n",
    "    \n",
    "        \n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_gbm(dtrain, dvalid, param, evals_result, learning_rate):\n",
    "    # check training arguments in param\n",
    "    n_round = param.get('num_boost_round', 100)\n",
    "    early_stop = param.get('early_stopping_rounds', 0)\n",
    "    verbose_eval = param.get('verbose_eval', 5)\n",
    "    # specify validations set to watch performance\n",
    "    watchlist = [(dtrain,'train') ,(deval,'eval')]\n",
    "    #callbacks_list = [learning_rates]\n",
    "\n",
    "    bst = xgb.train(params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=n_round,\n",
    "            evals=watchlist,\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval = verbose_eval,\n",
    "            evals_result = evals_result, \n",
    "            callbacks = [xgb.callback.reset_learning_rate(reduceLRonPlateau)])\n",
    "                   \n",
    "    return bst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization...\n",
      "lr:  0.1\n",
      "[0]\ttrain-logloss:0.693146\teval-logloss:0.693146\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 1000 rounds.\n",
      "iter:  1  Current_loss:  0.693146 best:  99.9999 less:  True\n",
      "iter:  2  Current_loss:  0.662099 best:  0.661491 less:  False\n",
      "iter:  3  Current_loss:  0.635526 best:  0.635263 less:  False\n",
      "iter:  3 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  4  Current_loss:  0.627557 best:  0.613748 less:  False\n",
      "iter:  5  Current_loss:  0.602885 best:  0.591777 less:  False\n",
      "iter:  5 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[5]\ttrain-logloss:0.582094\teval-logloss:0.596992\n",
      "iter:  6  Current_loss:  0.596992 best:  0.572395 less:  False\n",
      "iter:  7  Current_loss:  0.578699 best:  0.561965 less:  False\n",
      "iter:  7 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  8  Current_loss:  0.573216 best:  0.550544 less:  False\n",
      "iter:  9  Current_loss:  0.560081 best:  0.540796 less:  False\n",
      "iter:  9 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  10  Current_loss:  0.557006 best:  0.531817 less:  False\n",
      "[10]\ttrain-logloss:0.518415\teval-logloss:0.54165\n",
      "iter:  11  Current_loss:  0.54165 best:  0.525811 less:  False\n",
      "iter:  11 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  12  Current_loss:  0.538841 best:  0.514083 less:  False\n",
      "iter:  13  Current_loss:  0.527947 best:  0.507322 less:  False\n",
      "iter:  13 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  14  Current_loss:  0.52576 best:  0.500967 less:  False\n",
      "iter:  15  Current_loss:  0.51609 best:  0.49892600000000004 less:  False\n",
      "iter:  15 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[15]\ttrain-logloss:0.481315\teval-logloss:0.514144\n",
      "iter:  16  Current_loss:  0.514144 best:  0.493142 less:  False\n",
      "iter:  17  Current_loss:  0.506344 best:  0.491221 less:  False\n",
      "iter:  17 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  18  Current_loss:  0.504123 best:  0.487072 less:  False\n",
      "iter:  19  Current_loss:  0.499165 best:  0.48504200000000003 less:  False\n",
      "iter:  19 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  20  Current_loss:  0.497773 best:  0.480306 less:  False\n",
      "[20]\ttrain-logloss:0.444363\teval-logloss:0.49287\n",
      "iter:  21  Current_loss:  0.49287 best:  0.478864 less:  False\n",
      "iter:  21 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  22  Current_loss:  0.491388 best:  0.477711 less:  False\n",
      "iter:  23  Current_loss:  0.482793 best:  0.477711 less:  False\n",
      "iter:  23 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  24  Current_loss:  0.481531 best:  0.477711 less:  False\n",
      "iter:  25  Current_loss:  0.476036 best:  0.477711 less:  True\n",
      "[25]\ttrain-logloss:0.422441\teval-logloss:0.475772\n",
      "iter:  26  Current_loss:  0.475772 best:  0.477711 less:  True\n",
      "iter:  27  Current_loss:  0.474593 best:  0.477711 less:  True\n",
      "iter:  28  Current_loss:  0.474153 best:  0.477711 less:  True\n",
      "iter:  29  Current_loss:  0.471999 best:  0.477711 less:  True\n",
      "iter:  30  Current_loss:  0.471897 best:  0.477711 less:  True\n",
      "[30]\ttrain-logloss:0.403982\teval-logloss:0.46945\n",
      "iter:  31  Current_loss:  0.46945 best:  0.477711 less:  True\n",
      "iter:  32  Current_loss:  0.469426 best:  0.477711 less:  True\n",
      "iter:  33  Current_loss:  0.471079 best:  0.477711 less:  True\n",
      "iter:  34  Current_loss:  0.471069 best:  0.477711 less:  True\n",
      "iter:  35  Current_loss:  0.467654 best:  0.477711 less:  True\n",
      "[35]\ttrain-logloss:0.392465\teval-logloss:0.467653\n",
      "iter:  36  Current_loss:  0.467653 best:  0.477711 less:  True\n",
      "iter:  37  Current_loss:  0.466315 best:  0.477711 less:  True\n",
      "iter:  38  Current_loss:  0.466315 best:  0.477711 less:  True\n",
      "iter:  39  Current_loss:  0.46252 best:  0.477711 less:  True\n",
      "iter:  40  Current_loss:  0.46252 best:  0.477711 less:  True\n",
      "[40]\ttrain-logloss:0.377716\teval-logloss:0.461007\n",
      "iter:  41  Current_loss:  0.461007 best:  0.477711 less:  True\n",
      "iter:  42  Current_loss:  0.461006 best:  0.477711 less:  True\n",
      "iter:  43  Current_loss:  0.459959 best:  0.477711 less:  True\n",
      "iter:  44  Current_loss:  0.459959 best:  0.477711 less:  True\n",
      "iter:  45  Current_loss:  0.456597 best:  0.477711 less:  True\n",
      "[45]\ttrain-logloss:0.368883\teval-logloss:0.456597\n",
      "iter:  46  Current_loss:  0.456597 best:  0.477711 less:  True\n",
      "iter:  47  Current_loss:  0.454564 best:  0.477711 less:  True\n",
      "iter:  48  Current_loss:  0.454564 best:  0.477711 less:  True\n",
      "iter:  49  Current_loss:  0.453401 best:  0.477711 less:  True\n",
      "iter:  50  Current_loss:  0.453401 best:  0.477711 less:  True\n",
      "[50]\ttrain-logloss:0.357913\teval-logloss:0.451846\n",
      "iter:  51  Current_loss:  0.451846 best:  0.477711 less:  True\n",
      "iter:  52  Current_loss:  0.451846 best:  0.477711 less:  True\n",
      "iter:  53  Current_loss:  0.451648 best:  0.477711 less:  True\n",
      "iter:  54  Current_loss:  0.451648 best:  0.477711 less:  True\n",
      "iter:  55  Current_loss:  0.451443 best:  0.477711 less:  True\n",
      "[55]\ttrain-logloss:0.352813\teval-logloss:0.451443\n",
      "iter:  56  Current_loss:  0.451443 best:  0.477711 less:  True\n",
      "iter:  57  Current_loss:  0.454504 best:  0.477711 less:  True\n",
      "iter:  58  Current_loss:  0.454504 best:  0.477711 less:  True\n",
      "iter:  59  Current_loss:  0.459458 best:  0.477711 less:  True\n",
      "iter:  60  Current_loss:  0.459458 best:  0.477711 less:  True\n",
      "[60]\ttrain-logloss:0.343891\teval-logloss:0.580369\n",
      "iter:  61  Current_loss:  0.580369 best:  0.477711 less:  False\n",
      "iter:  62  Current_loss:  0.580369 best:  0.477711 less:  False\n",
      "iter:  62 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  63  Current_loss:  0.580762 best:  0.477711 less:  False\n",
      "iter:  64  Current_loss:  0.580762 best:  0.477711 less:  False\n",
      "iter:  64 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  65  Current_loss:  0.580757 best:  0.477711 less:  False\n",
      "[65]\ttrain-logloss:0.34245\teval-logloss:0.580757\n",
      "iter:  66  Current_loss:  0.580757 best:  0.477711 less:  False\n",
      "iter:  66 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  67  Current_loss:  0.580572 best:  0.477711 less:  False\n",
      "iter:  68  Current_loss:  0.580572 best:  0.477711 less:  False\n",
      "iter:  68 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  69  Current_loss:  0.580368 best:  0.477711 less:  False\n",
      "iter:  70  Current_loss:  0.580368 best:  0.477711 less:  False\n",
      "iter:  70 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[70]\ttrain-logloss:0.340428\teval-logloss:0.580987\n",
      "iter:  71  Current_loss:  0.580987 best:  0.477711 less:  False\n",
      "iter:  72  Current_loss:  0.580987 best:  0.477711 less:  False\n",
      "iter:  72 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  73  Current_loss:  0.580378 best:  0.477711 less:  False\n",
      "iter:  74  Current_loss:  0.580378 best:  0.477711 less:  False\n",
      "iter:  74 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  75  Current_loss:  0.580081 best:  0.477711 less:  False\n",
      "[75]\ttrain-logloss:0.339133\teval-logloss:0.580081\n",
      "iter:  76  Current_loss:  0.580081 best:  0.477711 less:  False\n",
      "iter:  76 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  77  Current_loss:  0.579719 best:  0.477711 less:  False\n",
      "iter:  78  Current_loss:  0.579719 best:  0.477711 less:  False\n",
      "iter:  78 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  79  Current_loss:  0.579856 best:  0.477711 less:  False\n",
      "iter:  80  Current_loss:  0.579856 best:  0.477711 less:  False\n",
      "iter:  80 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[80]\ttrain-logloss:0.336688\teval-logloss:0.580254\n",
      "iter:  81  Current_loss:  0.580254 best:  0.477711 less:  False\n",
      "iter:  82  Current_loss:  0.580254 best:  0.477711 less:  False\n",
      "iter:  82 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  83  Current_loss:  0.579837 best:  0.477711 less:  False\n",
      "iter:  84  Current_loss:  0.579837 best:  0.477711 less:  False\n",
      "iter:  84 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  85  Current_loss:  0.579798 best:  0.477711 less:  False\n",
      "[85]\ttrain-logloss:0.33484\teval-logloss:0.579798\n",
      "iter:  86  Current_loss:  0.579798 best:  0.477711 less:  False\n",
      "iter:  86 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  87  Current_loss:  0.57901 best:  0.477711 less:  False\n",
      "iter:  88  Current_loss:  0.57901 best:  0.477711 less:  False\n",
      "iter:  88 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  89  Current_loss:  0.579148 best:  0.477711 less:  False\n",
      "iter:  90  Current_loss:  0.579148 best:  0.477711 less:  False\n",
      "iter:  90 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[90]\ttrain-logloss:0.333081\teval-logloss:0.578742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  91  Current_loss:  0.578742 best:  0.477711 less:  False\n",
      "iter:  92  Current_loss:  0.578742 best:  0.477711 less:  False\n",
      "iter:  92 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  93  Current_loss:  0.578359 best:  0.477711 less:  False\n",
      "iter:  94  Current_loss:  0.578359 best:  0.477711 less:  False\n",
      "iter:  94 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  95  Current_loss:  0.578744 best:  0.477711 less:  False\n",
      "[95]\ttrain-logloss:0.331323\teval-logloss:0.578744\n",
      "iter:  96  Current_loss:  0.578744 best:  0.477711 less:  False\n",
      "iter:  96 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  97  Current_loss:  0.577541 best:  0.477711 less:  False\n",
      "iter:  98  Current_loss:  0.577541 best:  0.477711 less:  False\n",
      "iter:  98 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  99  Current_loss:  0.577414 best:  0.477711 less:  False\n",
      "iter:  100  Current_loss:  0.577414 best:  0.477711 less:  False\n",
      "iter:  100 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[100]\ttrain-logloss:0.329577\teval-logloss:0.576669\n",
      "iter:  101  Current_loss:  0.576669 best:  0.477711 less:  False\n",
      "iter:  102  Current_loss:  0.576669 best:  0.477711 less:  False\n",
      "iter:  102 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  103  Current_loss:  0.576891 best:  0.477711 less:  False\n",
      "iter:  104  Current_loss:  0.576891 best:  0.477711 less:  False\n",
      "iter:  104 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  105  Current_loss:  0.576682 best:  0.477711 less:  False\n",
      "[105]\ttrain-logloss:0.328142\teval-logloss:0.576682\n",
      "iter:  106  Current_loss:  0.576682 best:  0.477711 less:  False\n",
      "iter:  106 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  107  Current_loss:  0.576579 best:  0.477711 less:  False\n",
      "iter:  108  Current_loss:  0.576579 best:  0.477711 less:  False\n",
      "iter:  108 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  109  Current_loss:  0.576417 best:  0.477711 less:  False\n",
      "iter:  110  Current_loss:  0.576417 best:  0.477711 less:  False\n",
      "iter:  110 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[110]\ttrain-logloss:0.326511\teval-logloss:0.576176\n",
      "iter:  111  Current_loss:  0.576176 best:  0.477711 less:  False\n",
      "iter:  112  Current_loss:  0.576176 best:  0.477711 less:  False\n",
      "iter:  112 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  113  Current_loss:  0.575688 best:  0.477711 less:  False\n",
      "iter:  114  Current_loss:  0.575688 best:  0.477711 less:  False\n",
      "iter:  114 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  115  Current_loss:  0.57569 best:  0.477711 less:  False\n",
      "[115]\ttrain-logloss:0.324924\teval-logloss:0.57569\n",
      "iter:  116  Current_loss:  0.57569 best:  0.477711 less:  False\n",
      "iter:  116 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  117  Current_loss:  0.574809 best:  0.477711 less:  False\n",
      "iter:  118  Current_loss:  0.574809 best:  0.477711 less:  False\n",
      "iter:  118 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  119  Current_loss:  0.574747 best:  0.477711 less:  False\n",
      "iter:  120  Current_loss:  0.574747 best:  0.477711 less:  False\n",
      "iter:  120 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[120]\ttrain-logloss:0.322928\teval-logloss:0.574985\n",
      "iter:  121  Current_loss:  0.574985 best:  0.477711 less:  False\n",
      "iter:  122  Current_loss:  0.574985 best:  0.477711 less:  False\n",
      "iter:  122 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  123  Current_loss:  0.5754 best:  0.477711 less:  False\n",
      "iter:  124  Current_loss:  0.5754 best:  0.477711 less:  False\n",
      "iter:  124 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  125  Current_loss:  0.57574 best:  0.477711 less:  False\n",
      "[125]\ttrain-logloss:0.32178\teval-logloss:0.57574\n",
      "iter:  126  Current_loss:  0.57574 best:  0.477711 less:  False\n",
      "iter:  126 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  127  Current_loss:  0.576523 best:  0.477711 less:  False\n",
      "iter:  128  Current_loss:  0.576523 best:  0.477711 less:  False\n",
      "iter:  128 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  129  Current_loss:  0.576189 best:  0.477711 less:  False\n",
      "iter:  130  Current_loss:  0.576189 best:  0.477711 less:  False\n",
      "iter:  130 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "[130]\ttrain-logloss:0.319546\teval-logloss:0.576463\n",
      "iter:  131  Current_loss:  0.576463 best:  0.477711 less:  False\n",
      "iter:  132  Current_loss:  0.576463 best:  0.477711 less:  False\n",
      "iter:  132 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  133  Current_loss:  0.576414 best:  0.477711 less:  False\n",
      "iter:  134  Current_loss:  0.576414 best:  0.477711 less:  False\n",
      "iter:  134 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  135  Current_loss:  0.576644 best:  0.477711 less:  False\n",
      "[135]\ttrain-logloss:0.318697\teval-logloss:0.576644\n",
      "iter:  136  Current_loss:  0.576644 best:  0.477711 less:  False\n",
      "iter:  136 reducing lr --   old_lr:  0.1  new_lr:  0.03\n",
      "iter:  137  Current_loss:  0.576352 best:  0.477711 less:  False\n",
      "iter:  138  Current_loss:  0.576352 best:  0.477711 less:  False\n",
      "iter:  138 reducing lr --   old_lr:  0.03  new_lr:  0.009\n",
      "iter:  139  Current_loss:  0.576491 best:  0.477711 less:  False\n",
      "iter:  140  Current_loss:  0.576491 best:  0.477711 less:  False\n",
      "iter:  140 reducing lr --   old_lr:  0.009  new_lr:  0.0026999999999999997\n",
      "[140]\ttrain-logloss:0.317896\teval-logloss:0.576508\n",
      "iter:  141  Current_loss:  0.576508 best:  0.477711 less:  False\n",
      "iter:  142  Current_loss:  0.576508 best:  0.477711 less:  False\n",
      "iter:  142 reducing lr --   old_lr:  0.0026999999999999997  new_lr:  0.0008099999999999998\n",
      "iter:  143  Current_loss:  0.576523 best:  0.477711 less:  False\n",
      "iter:  144  Current_loss:  0.576523 best:  0.477711 less:  False\n",
      "iter:  144 reducing lr --   old_lr:  0.0008099999999999998  new_lr:  0.00024299999999999994\n",
      "iter:  145  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[145]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  146  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  146 reducing lr --   old_lr:  0.00024299999999999994  new_lr:  7.289999999999998e-05\n",
      "iter:  147  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  148  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  148 reducing lr --   old_lr:  7.289999999999998e-05  new_lr:  2.1869999999999996e-05\n",
      "iter:  149  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  150  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  150 reducing lr --   old_lr:  2.1869999999999996e-05  new_lr:  6.560999999999999e-06\n",
      "[150]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  151  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  152  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  152 reducing lr --   old_lr:  6.560999999999999e-06  new_lr:  1.9682999999999994e-06\n",
      "iter:  153  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  154  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  154 reducing lr --   old_lr:  1.9682999999999994e-06  new_lr:  5.904899999999998e-07\n",
      "iter:  155  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[155]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  156  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  156 reducing lr --   old_lr:  5.904899999999998e-07  new_lr:  1.7714699999999994e-07\n",
      "iter:  157  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  158  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  158 reducing lr --   old_lr:  1.7714699999999994e-07  new_lr:  5.314409999999998e-08\n",
      "iter:  159  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  160  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  160 reducing lr --   old_lr:  5.314409999999998e-08  new_lr:  1.5943229999999993e-08\n",
      "[160]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  161  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  162  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  162 reducing lr --   old_lr:  1.5943229999999993e-08  new_lr:  4.782968999999998e-09\n",
      "iter:  163  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  164  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  164 reducing lr --   old_lr:  4.782968999999998e-09  new_lr:  1.4348906999999994e-09\n",
      "iter:  165  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[165]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  166  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  166 reducing lr --   old_lr:  1.4348906999999994e-09  new_lr:  4.304672099999998e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  167  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  168  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  168 reducing lr --   old_lr:  4.304672099999998e-10  new_lr:  1.2914016299999993e-10\n",
      "iter:  169  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  170  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  170 reducing lr --   old_lr:  1.2914016299999993e-10  new_lr:  3.874204889999998e-11\n",
      "[170]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  171  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  172  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  172 reducing lr --   old_lr:  3.874204889999998e-11  new_lr:  1.1622614669999993e-11\n",
      "iter:  173  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  174  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  174 reducing lr --   old_lr:  1.1622614669999993e-11  new_lr:  3.4867844009999975e-12\n",
      "iter:  175  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[175]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  176  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  176 reducing lr --   old_lr:  3.4867844009999975e-12  new_lr:  1.0460353202999991e-12\n",
      "iter:  177  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  178  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  178 reducing lr --   old_lr:  1.0460353202999991e-12  new_lr:  3.1381059608999974e-13\n",
      "iter:  179  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  180  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  180 reducing lr --   old_lr:  3.1381059608999974e-13  new_lr:  9.414317882699992e-14\n",
      "[180]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  181  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  182  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  182 reducing lr --   old_lr:  9.414317882699992e-14  new_lr:  2.8242953648099973e-14\n",
      "iter:  183  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  184  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  184 reducing lr --   old_lr:  2.8242953648099973e-14  new_lr:  8.472886094429992e-15\n",
      "iter:  185  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[185]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  186  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  186 reducing lr --   old_lr:  8.472886094429992e-15  new_lr:  2.5418658283289977e-15\n",
      "iter:  187  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  188  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  188 reducing lr --   old_lr:  2.5418658283289977e-15  new_lr:  7.625597484986993e-16\n",
      "iter:  189  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  190  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  190 reducing lr --   old_lr:  7.625597484986993e-16  new_lr:  2.2876792454960977e-16\n",
      "[190]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  191  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  192  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  192 reducing lr --   old_lr:  2.2876792454960977e-16  new_lr:  6.863037736488293e-17\n",
      "iter:  193  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  194  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  194 reducing lr --   old_lr:  6.863037736488293e-17  new_lr:  2.058911320946488e-17\n",
      "iter:  195  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[195]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  196  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  196 reducing lr --   old_lr:  2.058911320946488e-17  new_lr:  6.176733962839464e-18\n",
      "iter:  197  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  198  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  198 reducing lr --   old_lr:  6.176733962839464e-18  new_lr:  1.853020188851839e-18\n",
      "iter:  199  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  200  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  200 reducing lr --   old_lr:  1.853020188851839e-18  new_lr:  5.559060566555516e-19\n",
      "[200]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  201  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  202  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  202 reducing lr --   old_lr:  5.559060566555516e-19  new_lr:  1.6677181699666549e-19\n",
      "iter:  203  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  204  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  204 reducing lr --   old_lr:  1.6677181699666549e-19  new_lr:  5.0031545098999645e-20\n",
      "iter:  205  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[205]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  206  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  206 reducing lr --   old_lr:  5.0031545098999645e-20  new_lr:  1.5009463529699892e-20\n",
      "iter:  207  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  208  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  208 reducing lr --   old_lr:  1.5009463529699892e-20  new_lr:  4.5028390589099676e-21\n",
      "iter:  209  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  210  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  210 reducing lr --   old_lr:  4.5028390589099676e-21  new_lr:  1.3508517176729902e-21\n",
      "[210]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  211  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  212  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  212 reducing lr --   old_lr:  1.3508517176729902e-21  new_lr:  4.05255515301897e-22\n",
      "iter:  213  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  214  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  214 reducing lr --   old_lr:  4.05255515301897e-22  new_lr:  1.215766545905691e-22\n",
      "iter:  215  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[215]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  216  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  216 reducing lr --   old_lr:  1.215766545905691e-22  new_lr:  3.647299637717073e-23\n",
      "iter:  217  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  218  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  218 reducing lr --   old_lr:  3.647299637717073e-23  new_lr:  1.094189891315122e-23\n",
      "iter:  219  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  220  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  220 reducing lr --   old_lr:  1.094189891315122e-23  new_lr:  3.2825696739453657e-24\n",
      "[220]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  221  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  222  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  222 reducing lr --   old_lr:  3.2825696739453657e-24  new_lr:  9.847709021836097e-25\n",
      "iter:  223  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  224  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  224 reducing lr --   old_lr:  9.847709021836097e-25  new_lr:  2.954312706550829e-25\n",
      "iter:  225  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[225]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  226  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  226 reducing lr --   old_lr:  2.954312706550829e-25  new_lr:  8.862938119652486e-26\n",
      "iter:  227  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  228  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  228 reducing lr --   old_lr:  8.862938119652486e-26  new_lr:  2.6588814358957457e-26\n",
      "iter:  229  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  230  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  230 reducing lr --   old_lr:  2.6588814358957457e-26  new_lr:  7.976644307687237e-27\n",
      "[230]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  231  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  232  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  232 reducing lr --   old_lr:  7.976644307687237e-27  new_lr:  2.392993292306171e-27\n",
      "iter:  233  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  234  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  234 reducing lr --   old_lr:  2.392993292306171e-27  new_lr:  7.178979876918513e-28\n",
      "iter:  235  Current_loss:  0.576522 best:  0.477711 less:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  236  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  236 reducing lr --   old_lr:  7.178979876918513e-28  new_lr:  2.153693963075554e-28\n",
      "iter:  237  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  238  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  238 reducing lr --   old_lr:  2.153693963075554e-28  new_lr:  6.461081889226662e-29\n",
      "iter:  239  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  240  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  240 reducing lr --   old_lr:  6.461081889226662e-29  new_lr:  1.9383245667679985e-29\n",
      "[240]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  241  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  242  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  242 reducing lr --   old_lr:  1.9383245667679985e-29  new_lr:  5.814973700303995e-30\n",
      "iter:  243  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  244  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  244 reducing lr --   old_lr:  5.814973700303995e-30  new_lr:  1.7444921100911986e-30\n",
      "iter:  245  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[245]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  246  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  246 reducing lr --   old_lr:  1.7444921100911986e-30  new_lr:  1e-30\n",
      "iter:  247  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  248  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  249  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  250  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[250]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  251  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  252  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  253  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  254  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  255  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[255]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  256  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  257  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  258  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  259  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  260  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[260]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  261  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  262  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  263  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  264  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  265  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[265]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  266  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  267  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  268  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  269  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  270  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[270]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  271  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  272  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  273  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  274  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  275  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[275]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  276  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  277  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  278  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  279  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  280  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[280]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  281  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  282  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  283  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  284  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  285  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[285]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  286  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  287  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  288  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  289  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  290  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[290]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  291  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  292  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  293  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  294  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  295  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[295]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  296  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  297  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  298  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  299  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  300  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[300]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  301  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  302  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  303  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  304  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  305  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[305]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  306  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  307  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  308  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  309  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  310  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[310]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  311  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  312  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  313  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  314  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  315  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[315]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  316  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  317  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  318  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  319  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  320  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[320]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  321  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  322  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  323  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  324  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  325  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[325]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  326  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  327  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  328  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  329  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  330  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[330]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  331  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  332  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  333  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  334  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  335  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[335]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  336  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  337  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  338  Current_loss:  0.576522 best:  0.477711 less:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  339  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  340  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[340]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  341  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  342  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  343  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  344  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  345  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[345]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  346  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  347  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  348  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  349  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  350  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[350]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  351  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  352  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  353  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  354  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  355  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[355]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  356  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  357  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  358  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  359  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  360  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[360]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  361  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  362  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  363  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  364  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  365  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[365]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  366  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  367  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  368  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  369  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  370  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[370]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  371  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  372  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  373  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  374  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  375  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[375]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  376  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  377  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  378  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  379  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  380  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[380]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  381  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  382  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  383  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  384  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  385  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[385]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  386  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  387  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  388  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  389  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  390  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[390]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  391  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  392  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  393  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  394  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  395  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[395]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  396  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  397  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  398  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  399  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  400  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[400]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  401  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  402  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  403  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  404  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  405  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[405]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  406  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  407  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  408  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  409  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  410  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[410]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  411  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  412  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  413  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  414  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  415  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[415]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  416  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  417  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  418  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  419  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  420  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[420]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  421  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  422  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  423  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  424  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  425  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[425]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  426  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  427  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  428  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  429  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  430  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[430]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  431  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  432  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  433  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  434  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  435  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[435]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  436  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  437  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  438  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  439  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  440  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[440]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  441  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  442  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  443  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  444  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  445  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[445]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  446  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  447  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  448  Current_loss:  0.576522 best:  0.477711 less:  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  449  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  450  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[450]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  451  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  452  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  453  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  454  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  455  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[455]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  456  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  457  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  458  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  459  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  460  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[460]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  461  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  462  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  463  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  464  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  465  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[465]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  466  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  467  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  468  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  469  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  470  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[470]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  471  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  472  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  473  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  474  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  475  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[475]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  476  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  477  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  478  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  479  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  480  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[480]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  481  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  482  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  483  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  484  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  485  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[485]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  486  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  487  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  488  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  489  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  490  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[490]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  491  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  492  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  493  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  494  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  495  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[495]\ttrain-logloss:0.31787\teval-logloss:0.576522\n",
      "iter:  496  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  497  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  498  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "iter:  499  Current_loss:  0.576522 best:  0.477711 less:  False\n",
      "[499]\ttrain-logloss:0.31787\teval-logloss:0.576522\n"
     ]
    }
   ],
   "source": [
    "model1 = run_gbm(dtrain, deval, params,progress, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, eta=0.2, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=500, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.7)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "\n",
    "model = XGBClassifier(n_estimators=num_epochs, learning_rate=0.1, subsample=0.7, eta = 0.2 )\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  54  min customEval: 0.451443\n",
      "iter:  39  min Eval: 0.453349\n"
     ]
    }
   ],
   "source": [
    "# retrieve performance metrics\n",
    "results = model.evals_result()\n",
    "#results1 = model1.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, 200)\n",
    "\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(10, 6))\n",
    "ax.plot(x_axis, results['validation_0']['logloss'][:200:], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'][:200:], label='Eval')\n",
    "ax.plot(x_axis, progress['train']['logloss'][:200:], label = 'customTrain')\n",
    "ax.plot(x_axis, progress['eval']['logloss'][:200:], label = 'customEval')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "\n",
    "pyplot.show()\n",
    "print(\"iter: \", progress['eval']['logloss'][:200:].index(min(progress['eval']['logloss'][:200:])), \" min customEval:\", min(progress['eval']['logloss'][:100:]))\n",
    "print(\"iter: \", results['validation_1']['logloss'][:200:].index(min(results['validation_1']['logloss'][:200:])), \" min Eval:\", min(results['validation_1']['logloss'][:100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classification error\n",
    "# fig, ax = pyplot.subplots()\n",
    "# ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "# ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "# ax.legend()\n",
    "# pyplot.ylabel('Classification Error')\n",
    "# pyplot.title('XGBoost Classification Error')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

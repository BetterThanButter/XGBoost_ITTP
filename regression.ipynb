{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agavrilenko/anaconda3/lib/python3.6/site-packages/xgboost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.dirname(xgb.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round: int\n",
      "        Number of boosting iterations.\n",
      "    evals: list of pairs (DMatrix, string)\n",
      "        List of items to be evaluated during training, this allows user to watch\n",
      "        performance on the validation set.\n",
      "    obj : function\n",
      "        Customized objective function.\n",
      "    feval : function\n",
      "        Customized evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Validation error needs to decrease at least\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        If there's more than one, will use the last.\n",
      "        Returns the model from the last iteration (not the best one).\n",
      "        If early stopping occurs, the model will have three additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "        (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "        ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "    evals_result: dict\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval : bool or int\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    learning_rates: list or function (deprecated - use callback API instead)\n",
      "        List of learning rate for each boosting round\n",
      "        or a customized function that calculates eta in terms of\n",
      "        current number of round and the total number of boosting round (e.g. yields\n",
      "        learning rate decay)\n",
      "    xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(xgboost.training.train)\n",
    "help(xgb.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df_usa = pd.read_csv('data/houses/housesalesprediction/kc_house_data.csv', delimiter=\",\")\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "\n",
    "df_usa.drop(['id', 'date'], axis=1, inplace=True)\n",
    "X = df_usa.drop(\"price\",axis=1).values\n",
    "y = df_usa[\"price\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=3)\n",
    "\n",
    "# split data into X and y\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# # split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data = X_train, label = y_train, silent= True)\n",
    "\n",
    "deval = xgb.DMatrix(data = X_test, label = y_test, silent= True)\n",
    "# evals_list = []\n",
    "# evals_list.append((deval, \"eval matrix\"))\n",
    "num_epochs = 600\n",
    "params = {}\n",
    "params['num_boost_round'] = num_epochs\n",
    "params['early_stopping_rounds'] = 1000\n",
    "#params['verbose_eval'] = 1\n",
    "params['eval_metric'] = 'rmse'\n",
    "params['evals_result'] = {}\n",
    "params['max_depth'] = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = []\n",
    "tresholds = []\n",
    "learning_rates.append(0.1)\n",
    "counter = 0\n",
    "best = []\n",
    "progress = dict()\n",
    "\n",
    "def reduceLRonPlateau(i,n):\n",
    "\n",
    "    factor = 0.2\n",
    "    min_lr = 1e-30\n",
    "    min_delta = 1e-10\n",
    "    patience = 0\n",
    "    verbose = 0\n",
    "    cooldown = 3\n",
    "    cooldown_counter = 0  # Cooldown counter.\n",
    "    wait = 0\n",
    "    #best = 0\n",
    "    mode = 'min'  \n",
    "    monitor_op = None\n",
    "    old_lr = learning_rates[0]\n",
    "    new_lr = learning_rates[0] \n",
    "    \n",
    "    def in_cooldown():\n",
    "            return cooldown_counter > 0\n",
    "    \n",
    "    def _reset(mode):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\"\"\"\n",
    "#         if mode not in ['auto', 'min', 'max']:\n",
    "#             warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n",
    "#                           'fallback to auto mode.' % (mode),\n",
    "#                           RuntimeWarning)\n",
    "#             mode = 'auto'\n",
    "#         if (mode == 'min' or\n",
    "#            (mode == 'auto' and 'acc' not in monitor)):\n",
    "#             monitor_op = lambda a, b: np.less(a, b - min_delta)\n",
    "#             best = np.Inf\n",
    "#         else:\n",
    "#             monitor_op = lambda a, b: np.greater(a, b + min_delta)\n",
    "#             best = -np.Inf\n",
    "            \n",
    "        cooldown_counter = 0\n",
    "        wait = 0\n",
    "        return wait, cooldown_counter\n",
    "    \n",
    "    def checker(wait,best, new_lr, learning_rates):\n",
    "        if (i == 0):\n",
    "            wait, counter = _reset(mode)\n",
    "            print(\"initialization...\")\n",
    "            print(\"lr: \", learning_rates[0])\n",
    "            best.append(100000000)\n",
    "            \n",
    "        if (i>0):\n",
    "            old_lr = learning_rates[i-1]\n",
    "            #new_lr = learning_rates[i-1]\n",
    "            current = progress['eval']['rmse'][i-1]\n",
    "            if in_cooldown():\n",
    "                cooldown_counter -= 1\n",
    "                wait = 0\n",
    "              \n",
    "            print(\"iter: \", i, \" Current_loss: \", current, \"best: \", best[i-1])\n",
    "            if np.less(current, best[i-1] - min_delta):\n",
    "                best.append(current)\n",
    "                wait = 0\n",
    "             \n",
    "            elif not in_cooldown():\n",
    "                #rint(\"not in_cooldown\")\n",
    "                best.append(best[-1])\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    \n",
    "                    if old_lr > min_lr:\n",
    "                        new_lr = old_lr * factor\n",
    "                        new_lr = max(new_lr, min_lr)\n",
    "                        print(\"iter: \", i, \"reducing lr -- \", \" old_lr: \", old_lr, \" new_lr: \", new_lr)\n",
    "                        \n",
    "#                         if self.verbose > 0:\n",
    "#                             print('\\nEpoch %05d: ReduceLROnPlateau reducing '\n",
    "#                                   'learning rate to %s.' % (epoch + 1, new_lr))\n",
    "                        cooldown_counter = cooldown\n",
    "                        wait = 0\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "       # print(\"step \",progress['train']['logloss'][i-1], \"i = \", i)\n",
    "\n",
    "            \n",
    "    \n",
    "    new_lr = checker(wait,best,new_lr,learning_rates)\n",
    "    learning_rates.append(new_lr)\n",
    "        \n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_gbm(dtrain, dvalid, param, evals_result, learning_rate):\n",
    "    # check training arguments in param\n",
    "    n_round = param.get('num_boost_round', 100)\n",
    "    early_stop = param.get('early_stopping_rounds', 0)\n",
    "    verbose_eval = param.get('verbose_eval', 50)\n",
    "    # specify validations set to watch performance\n",
    "    watchlist = [(dtrain,'train') ,(deval,'eval')]\n",
    "    #callbacks_list = [learning_rates]\n",
    "\n",
    "    bst = xgb.train(params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=n_round,\n",
    "            evals=watchlist,\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval = verbose_eval,\n",
    "            evals_result = evals_result, \n",
    "            callbacks = [xgb.callback.reset_learning_rate(reduceLRonPlateau)])\n",
    "                   \n",
    "    return bst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization...\n",
      "lr:  0.1\n",
      "[0]\ttrain-rmse:597658\teval-rmse:590266\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 1000 rounds.\n",
      "iter:  1  Current_loss:  590265.5 best:  100000000\n",
      "iter:  2  Current_loss:  539130.9375 best:  590265.5\n",
      "iter:  3  Current_loss:  493740.15625 best:  539130.9375\n",
      "iter:  4  Current_loss:  453324.34375 best:  493740.15625\n",
      "iter:  5  Current_loss:  417079.59375 best:  453324.34375\n",
      "iter:  6  Current_loss:  385368.84375 best:  417079.59375\n",
      "iter:  7  Current_loss:  357464.09375 best:  385368.84375\n",
      "iter:  8  Current_loss:  332269.125 best:  357464.09375\n",
      "iter:  9  Current_loss:  310484.65625 best:  332269.125\n",
      "iter:  10  Current_loss:  290920.0 best:  310484.65625\n",
      "iter:  11  Current_loss:  273306.21875 best:  290920.0\n",
      "iter:  12  Current_loss:  258614.453125 best:  273306.21875\n",
      "iter:  13  Current_loss:  245829.46875 best:  258614.453125\n",
      "iter:  14  Current_loss:  233888.078125 best:  245829.46875\n",
      "iter:  15  Current_loss:  222381.3125 best:  233888.078125\n",
      "iter:  16  Current_loss:  212981.96875 best:  222381.3125\n",
      "iter:  17  Current_loss:  205294.609375 best:  212981.96875\n",
      "iter:  18  Current_loss:  198489.578125 best:  205294.609375\n",
      "iter:  19  Current_loss:  191642.0625 best:  198489.578125\n",
      "iter:  20  Current_loss:  186847.484375 best:  191642.0625\n",
      "iter:  21  Current_loss:  182536.53125 best:  186847.484375\n",
      "iter:  22  Current_loss:  178683.59375 best:  182536.53125\n",
      "iter:  23  Current_loss:  175017.453125 best:  178683.59375\n",
      "iter:  24  Current_loss:  171210.4375 best:  175017.453125\n",
      "iter:  25  Current_loss:  168738.21875 best:  171210.4375\n",
      "iter:  26  Current_loss:  165576.28125 best:  168738.21875\n",
      "iter:  27  Current_loss:  163297.0 best:  165576.28125\n",
      "iter:  28  Current_loss:  161055.546875 best:  163297.0\n",
      "iter:  29  Current_loss:  159055.078125 best:  161055.546875\n",
      "iter:  30  Current_loss:  157250.828125 best:  159055.078125\n",
      "iter:  31  Current_loss:  155680.75 best:  157250.828125\n",
      "iter:  32  Current_loss:  154154.921875 best:  155680.75\n",
      "iter:  33  Current_loss:  152804.734375 best:  154154.921875\n",
      "iter:  34  Current_loss:  151355.109375 best:  152804.734375\n",
      "iter:  35  Current_loss:  150350.78125 best:  151355.109375\n",
      "iter:  36  Current_loss:  149572.34375 best:  150350.78125\n",
      "iter:  37  Current_loss:  148521.203125 best:  149572.34375\n",
      "iter:  38  Current_loss:  147606.015625 best:  148521.203125\n",
      "iter:  39  Current_loss:  146937.578125 best:  147606.015625\n",
      "iter:  40  Current_loss:  145837.609375 best:  146937.578125\n",
      "iter:  41  Current_loss:  144997.125 best:  145837.609375\n",
      "iter:  42  Current_loss:  143939.5 best:  144997.125\n",
      "iter:  43  Current_loss:  143286.421875 best:  143939.5\n",
      "iter:  44  Current_loss:  141782.203125 best:  143286.421875\n",
      "iter:  45  Current_loss:  140610.578125 best:  141782.203125\n",
      "iter:  46  Current_loss:  140050.09375 best:  140610.578125\n",
      "iter:  47  Current_loss:  139330.984375 best:  140050.09375\n",
      "iter:  48  Current_loss:  138988.71875 best:  139330.984375\n",
      "iter:  49  Current_loss:  138296.78125 best:  138988.71875\n",
      "iter:  50  Current_loss:  137717.46875 best:  138296.78125\n",
      "[50]\ttrain-rmse:131990\teval-rmse:136758\n",
      "iter:  51  Current_loss:  136757.765625 best:  137717.46875\n",
      "iter:  52  Current_loss:  136271.234375 best:  136757.765625\n",
      "iter:  53  Current_loss:  135987.40625 best:  136271.234375\n",
      "iter:  54  Current_loss:  135562.40625 best:  135987.40625\n",
      "iter:  55  Current_loss:  134720.0 best:  135562.40625\n",
      "iter:  56  Current_loss:  134227.1875 best:  134720.0\n",
      "iter:  57  Current_loss:  133810.171875 best:  134227.1875\n",
      "iter:  58  Current_loss:  133503.6875 best:  133810.171875\n",
      "iter:  59  Current_loss:  132845.625 best:  133503.6875\n",
      "iter:  60  Current_loss:  132548.171875 best:  132845.625\n",
      "iter:  61  Current_loss:  132390.65625 best:  132548.171875\n",
      "iter:  62  Current_loss:  131861.21875 best:  132390.65625\n",
      "iter:  63  Current_loss:  131589.96875 best:  131861.21875\n",
      "iter:  64  Current_loss:  131402.6875 best:  131589.96875\n",
      "iter:  65  Current_loss:  131140.421875 best:  131402.6875\n",
      "iter:  66  Current_loss:  130636.210938 best:  131140.421875\n",
      "iter:  67  Current_loss:  130307.796875 best:  130636.210938\n",
      "iter:  68  Current_loss:  130245.40625 best:  130307.796875\n",
      "iter:  69  Current_loss:  130144.679688 best:  130245.40625\n",
      "iter:  70  Current_loss:  130019.046875 best:  130144.679688\n",
      "iter:  71  Current_loss:  129489.15625 best:  130019.046875\n",
      "iter:  72  Current_loss:  129283.632812 best:  129489.15625\n",
      "iter:  73  Current_loss:  129133.789062 best:  129283.632812\n",
      "iter:  74  Current_loss:  128816.570312 best:  129133.789062\n",
      "iter:  75  Current_loss:  128598.890625 best:  128816.570312\n",
      "iter:  76  Current_loss:  128436.023438 best:  128598.890625\n",
      "iter:  77  Current_loss:  128191.703125 best:  128436.023438\n",
      "iter:  78  Current_loss:  128086.445312 best:  128191.703125\n",
      "iter:  79  Current_loss:  127677.398438 best:  128086.445312\n",
      "iter:  80  Current_loss:  127529.578125 best:  127677.398438\n",
      "iter:  81  Current_loss:  127489.53125 best:  127529.578125\n",
      "iter:  82  Current_loss:  127347.429688 best:  127489.53125\n",
      "iter:  83  Current_loss:  127051.328125 best:  127347.429688\n",
      "iter:  84  Current_loss:  126817.765625 best:  127051.328125\n",
      "iter:  85  Current_loss:  126751.78125 best:  126817.765625\n",
      "iter:  86  Current_loss:  126624.9375 best:  126751.78125\n",
      "iter:  87  Current_loss:  126476.023438 best:  126624.9375\n",
      "iter:  88  Current_loss:  126379.390625 best:  126476.023438\n",
      "iter:  89  Current_loss:  126267.445312 best:  126379.390625\n",
      "iter:  90  Current_loss:  125875.429688 best:  126267.445312\n",
      "iter:  91  Current_loss:  125725.742188 best:  125875.429688\n",
      "iter:  92  Current_loss:  125672.4375 best:  125725.742188\n",
      "iter:  93  Current_loss:  125604.367188 best:  125672.4375\n",
      "iter:  94  Current_loss:  125441.75 best:  125604.367188\n",
      "iter:  95  Current_loss:  125332.476562 best:  125441.75\n",
      "iter:  96  Current_loss:  125275.773438 best:  125332.476562\n",
      "iter:  97  Current_loss:  125120.835938 best:  125275.773438\n",
      "iter:  98  Current_loss:  125066.96875 best:  125120.835938\n",
      "iter:  99  Current_loss:  124972.359375 best:  125066.96875\n",
      "iter:  100  Current_loss:  124765.054688 best:  124972.359375\n",
      "[100]\ttrain-rmse:116419\teval-rmse:124757\n",
      "iter:  101  Current_loss:  124756.992188 best:  124765.054688\n",
      "iter:  102  Current_loss:  124692.40625 best:  124756.992188\n",
      "iter:  103  Current_loss:  124562.695312 best:  124692.40625\n",
      "iter:  104  Current_loss:  124128.164062 best:  124562.695312\n",
      "iter:  105  Current_loss:  123971.984375 best:  124128.164062\n",
      "iter:  106  Current_loss:  123918.601562 best:  123971.984375\n",
      "iter:  107  Current_loss:  123873.273438 best:  123918.601562\n",
      "iter:  108  Current_loss:  123818.742188 best:  123873.273438\n",
      "iter:  109  Current_loss:  123688.726562 best:  123818.742188\n",
      "iter:  110  Current_loss:  123391.648438 best:  123688.726562\n",
      "iter:  111  Current_loss:  123255.132812 best:  123391.648438\n",
      "iter:  112  Current_loss:  123150.210938 best:  123255.132812\n",
      "iter:  113  Current_loss:  123039.5625 best:  123150.210938\n",
      "iter:  114  Current_loss:  122876.929688 best:  123039.5625\n",
      "iter:  115  Current_loss:  122857.71875 best:  122876.929688\n",
      "iter:  116  Current_loss:  122800.742188 best:  122857.71875\n",
      "iter:  117  Current_loss:  122680.828125 best:  122800.742188\n",
      "iter:  118  Current_loss:  122653.15625 best:  122680.828125\n",
      "iter:  119  Current_loss:  122638.835938 best:  122653.15625\n",
      "iter:  120  Current_loss:  122517.804688 best:  122638.835938\n",
      "iter:  121  Current_loss:  122390.046875 best:  122517.804688\n",
      "iter:  122  Current_loss:  122171.460938 best:  122390.046875\n",
      "iter:  123  Current_loss:  122060.273438 best:  122171.460938\n",
      "iter:  124  Current_loss:  122018.6875 best:  122060.273438\n",
      "iter:  125  Current_loss:  121913.132812 best:  122018.6875\n",
      "iter:  126  Current_loss:  121911.5625 best:  121913.132812\n",
      "iter:  127  Current_loss:  121856.8125 best:  121911.5625\n",
      "iter:  128  Current_loss:  121772.070312 best:  121856.8125\n",
      "iter:  129  Current_loss:  121588.0 best:  121772.070312\n",
      "iter:  130  Current_loss:  121496.257812 best:  121588.0\n",
      "iter:  131  Current_loss:  121409.03125 best:  121496.257812\n",
      "iter:  132  Current_loss:  121350.75 best:  121409.03125\n",
      "iter:  133  Current_loss:  121259.726562 best:  121350.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  134  Current_loss:  121107.046875 best:  121259.726562\n",
      "iter:  135  Current_loss:  121011.234375 best:  121107.046875\n",
      "iter:  136  Current_loss:  121005.46875 best:  121011.234375\n",
      "iter:  137  Current_loss:  120957.53125 best:  121005.46875\n",
      "iter:  138  Current_loss:  120903.921875 best:  120957.53125\n",
      "iter:  139  Current_loss:  120814.71875 best:  120903.921875\n",
      "iter:  140  Current_loss:  120776.023438 best:  120814.71875\n",
      "iter:  141  Current_loss:  120747.125 best:  120776.023438\n",
      "iter:  142  Current_loss:  120673.234375 best:  120747.125\n",
      "iter:  143  Current_loss:  120617.757812 best:  120673.234375\n",
      "iter:  144  Current_loss:  120559.195312 best:  120617.757812\n",
      "iter:  145  Current_loss:  120514.46875 best:  120559.195312\n",
      "iter:  146  Current_loss:  120392.351562 best:  120514.46875\n",
      "iter:  147  Current_loss:  120090.507812 best:  120392.351562\n",
      "iter:  148  Current_loss:  120051.8125 best:  120090.507812\n",
      "iter:  149  Current_loss:  120039.710938 best:  120051.8125\n",
      "iter:  150  Current_loss:  120093.492188 best:  120039.710938\n",
      "iter:  150 reducing lr --   old_lr:  0.1  new_lr:  0.020000000000000004\n",
      "[150]\ttrain-rmse:108956\teval-rmse:120063\n",
      "iter:  151  Current_loss:  120063.453125 best:  120039.710938\n",
      "iter:  151 reducing lr --   old_lr:  0.1  new_lr:  0.020000000000000004\n",
      "iter:  152  Current_loss:  120034.453125 best:  120039.710938\n",
      "iter:  153  Current_loss:  119950.40625 best:  120034.453125\n",
      "iter:  154  Current_loss:  119893.59375 best:  119950.40625\n",
      "iter:  155  Current_loss:  119866.164062 best:  119893.59375\n",
      "iter:  156  Current_loss:  119836.648438 best:  119866.164062\n",
      "iter:  157  Current_loss:  119798.328125 best:  119836.648438\n",
      "iter:  158  Current_loss:  119747.695312 best:  119798.328125\n",
      "iter:  159  Current_loss:  119643.828125 best:  119747.695312\n",
      "iter:  160  Current_loss:  119603.835938 best:  119643.828125\n",
      "iter:  161  Current_loss:  119592.078125 best:  119603.835938\n",
      "iter:  162  Current_loss:  119458.617188 best:  119592.078125\n",
      "iter:  163  Current_loss:  119412.53125 best:  119458.617188\n",
      "iter:  164  Current_loss:  119454.265625 best:  119412.53125\n",
      "iter:  164 reducing lr --   old_lr:  0.1  new_lr:  0.020000000000000004\n",
      "iter:  165  Current_loss:  119449.15625 best:  119412.53125\n",
      "iter:  165 reducing lr --   old_lr:  0.1  new_lr:  0.020000000000000004\n",
      "iter:  166  Current_loss:  119448.195312 best:  119412.53125\n",
      "iter:  166 reducing lr --   old_lr:  0.020000000000000004  new_lr:  0.004000000000000001\n",
      "iter:  167  Current_loss:  119447.367188 best:  119412.53125\n",
      "iter:  167 reducing lr --   old_lr:  0.020000000000000004  new_lr:  0.004000000000000001\n",
      "iter:  168  Current_loss:  119446.898438 best:  119412.53125\n",
      "iter:  168 reducing lr --   old_lr:  0.004000000000000001  new_lr:  0.0008000000000000003\n",
      "iter:  169  Current_loss:  119446.242188 best:  119412.53125\n",
      "iter:  169 reducing lr --   old_lr:  0.004000000000000001  new_lr:  0.0008000000000000003\n",
      "iter:  170  Current_loss:  119445.617188 best:  119412.53125\n",
      "iter:  170 reducing lr --   old_lr:  0.0008000000000000003  new_lr:  0.00016000000000000007\n",
      "iter:  171  Current_loss:  119445.460938 best:  119412.53125\n",
      "iter:  171 reducing lr --   old_lr:  0.0008000000000000003  new_lr:  0.00016000000000000007\n",
      "iter:  172  Current_loss:  119445.320312 best:  119412.53125\n",
      "iter:  172 reducing lr --   old_lr:  0.00016000000000000007  new_lr:  3.200000000000001e-05\n",
      "iter:  173  Current_loss:  119445.3125 best:  119412.53125\n",
      "iter:  173 reducing lr --   old_lr:  0.00016000000000000007  new_lr:  3.200000000000001e-05\n",
      "iter:  174  Current_loss:  119445.28125 best:  119412.53125\n",
      "iter:  174 reducing lr --   old_lr:  3.200000000000001e-05  new_lr:  6.400000000000003e-06\n",
      "iter:  175  Current_loss:  119445.273438 best:  119412.53125\n",
      "iter:  175 reducing lr --   old_lr:  3.200000000000001e-05  new_lr:  6.400000000000003e-06\n",
      "iter:  176  Current_loss:  119445.257812 best:  119412.53125\n",
      "iter:  176 reducing lr --   old_lr:  6.400000000000003e-06  new_lr:  1.2800000000000007e-06\n",
      "iter:  177  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  177 reducing lr --   old_lr:  6.400000000000003e-06  new_lr:  1.2800000000000007e-06\n",
      "iter:  178  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  178 reducing lr --   old_lr:  1.2800000000000007e-06  new_lr:  2.560000000000001e-07\n",
      "iter:  179  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  179 reducing lr --   old_lr:  1.2800000000000007e-06  new_lr:  2.560000000000001e-07\n",
      "iter:  180  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  180 reducing lr --   old_lr:  2.560000000000001e-07  new_lr:  5.120000000000003e-08\n",
      "iter:  181  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  181 reducing lr --   old_lr:  2.560000000000001e-07  new_lr:  5.120000000000003e-08\n",
      "iter:  182  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  182 reducing lr --   old_lr:  5.120000000000003e-08  new_lr:  1.0240000000000006e-08\n",
      "iter:  183  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  183 reducing lr --   old_lr:  5.120000000000003e-08  new_lr:  1.0240000000000006e-08\n",
      "iter:  184  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  184 reducing lr --   old_lr:  1.0240000000000006e-08  new_lr:  2.048000000000001e-09\n",
      "iter:  185  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  185 reducing lr --   old_lr:  1.0240000000000006e-08  new_lr:  2.048000000000001e-09\n",
      "iter:  186  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  186 reducing lr --   old_lr:  2.048000000000001e-09  new_lr:  4.0960000000000027e-10\n",
      "iter:  187  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  187 reducing lr --   old_lr:  2.048000000000001e-09  new_lr:  4.0960000000000027e-10\n",
      "iter:  188  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  188 reducing lr --   old_lr:  4.0960000000000027e-10  new_lr:  8.192000000000006e-11\n",
      "iter:  189  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  189 reducing lr --   old_lr:  4.0960000000000027e-10  new_lr:  8.192000000000006e-11\n",
      "iter:  190  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  190 reducing lr --   old_lr:  8.192000000000006e-11  new_lr:  1.6384000000000014e-11\n",
      "iter:  191  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  191 reducing lr --   old_lr:  8.192000000000006e-11  new_lr:  1.6384000000000014e-11\n",
      "iter:  192  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  192 reducing lr --   old_lr:  1.6384000000000014e-11  new_lr:  3.276800000000003e-12\n",
      "iter:  193  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  193 reducing lr --   old_lr:  1.6384000000000014e-11  new_lr:  3.276800000000003e-12\n",
      "iter:  194  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  194 reducing lr --   old_lr:  3.276800000000003e-12  new_lr:  6.553600000000007e-13\n",
      "iter:  195  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  195 reducing lr --   old_lr:  3.276800000000003e-12  new_lr:  6.553600000000007e-13\n",
      "iter:  196  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  196 reducing lr --   old_lr:  6.553600000000007e-13  new_lr:  1.3107200000000014e-13\n",
      "iter:  197  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  197 reducing lr --   old_lr:  6.553600000000007e-13  new_lr:  1.3107200000000014e-13\n",
      "iter:  198  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  198 reducing lr --   old_lr:  1.3107200000000014e-13  new_lr:  2.6214400000000028e-14\n",
      "iter:  199  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  199 reducing lr --   old_lr:  1.3107200000000014e-13  new_lr:  2.6214400000000028e-14\n",
      "iter:  200  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  200 reducing lr --   old_lr:  2.6214400000000028e-14  new_lr:  5.242880000000006e-15\n",
      "[200]\ttrain-rmse:107904\teval-rmse:119445\n",
      "iter:  201  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  201 reducing lr --   old_lr:  2.6214400000000028e-14  new_lr:  5.242880000000006e-15\n",
      "iter:  202  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  202 reducing lr --   old_lr:  5.242880000000006e-15  new_lr:  1.0485760000000013e-15\n",
      "iter:  203  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  203 reducing lr --   old_lr:  5.242880000000006e-15  new_lr:  1.0485760000000013e-15\n",
      "iter:  204  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  204 reducing lr --   old_lr:  1.0485760000000013e-15  new_lr:  2.0971520000000026e-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  205  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  205 reducing lr --   old_lr:  1.0485760000000013e-15  new_lr:  2.0971520000000026e-16\n",
      "iter:  206  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  206 reducing lr --   old_lr:  2.0971520000000026e-16  new_lr:  4.1943040000000054e-17\n",
      "iter:  207  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  207 reducing lr --   old_lr:  2.0971520000000026e-16  new_lr:  4.1943040000000054e-17\n",
      "iter:  208  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  208 reducing lr --   old_lr:  4.1943040000000054e-17  new_lr:  8.388608000000012e-18\n",
      "iter:  209  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  209 reducing lr --   old_lr:  4.1943040000000054e-17  new_lr:  8.388608000000012e-18\n",
      "iter:  210  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  210 reducing lr --   old_lr:  8.388608000000012e-18  new_lr:  1.6777216000000024e-18\n",
      "iter:  211  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  211 reducing lr --   old_lr:  8.388608000000012e-18  new_lr:  1.6777216000000024e-18\n",
      "iter:  212  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  212 reducing lr --   old_lr:  1.6777216000000024e-18  new_lr:  3.355443200000005e-19\n",
      "iter:  213  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  213 reducing lr --   old_lr:  1.6777216000000024e-18  new_lr:  3.355443200000005e-19\n",
      "iter:  214  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  214 reducing lr --   old_lr:  3.355443200000005e-19  new_lr:  6.71088640000001e-20\n",
      "iter:  215  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  215 reducing lr --   old_lr:  3.355443200000005e-19  new_lr:  6.71088640000001e-20\n",
      "iter:  216  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  216 reducing lr --   old_lr:  6.71088640000001e-20  new_lr:  1.342177280000002e-20\n",
      "iter:  217  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  217 reducing lr --   old_lr:  6.71088640000001e-20  new_lr:  1.342177280000002e-20\n",
      "iter:  218  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  218 reducing lr --   old_lr:  1.342177280000002e-20  new_lr:  2.6843545600000044e-21\n",
      "iter:  219  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  219 reducing lr --   old_lr:  1.342177280000002e-20  new_lr:  2.6843545600000044e-21\n",
      "iter:  220  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  220 reducing lr --   old_lr:  2.6843545600000044e-21  new_lr:  5.368709120000009e-22\n",
      "iter:  221  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  221 reducing lr --   old_lr:  2.6843545600000044e-21  new_lr:  5.368709120000009e-22\n",
      "iter:  222  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  222 reducing lr --   old_lr:  5.368709120000009e-22  new_lr:  1.0737418240000017e-22\n",
      "iter:  223  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  223 reducing lr --   old_lr:  5.368709120000009e-22  new_lr:  1.0737418240000017e-22\n",
      "iter:  224  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  224 reducing lr --   old_lr:  1.0737418240000017e-22  new_lr:  2.1474836480000036e-23\n",
      "iter:  225  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  225 reducing lr --   old_lr:  1.0737418240000017e-22  new_lr:  2.1474836480000036e-23\n",
      "iter:  226  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  226 reducing lr --   old_lr:  2.1474836480000036e-23  new_lr:  4.2949672960000076e-24\n",
      "iter:  227  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  227 reducing lr --   old_lr:  2.1474836480000036e-23  new_lr:  4.2949672960000076e-24\n",
      "iter:  228  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  228 reducing lr --   old_lr:  4.2949672960000076e-24  new_lr:  8.589934592000016e-25\n",
      "iter:  229  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  229 reducing lr --   old_lr:  4.2949672960000076e-24  new_lr:  8.589934592000016e-25\n",
      "iter:  230  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  230 reducing lr --   old_lr:  8.589934592000016e-25  new_lr:  1.7179869184000032e-25\n",
      "iter:  231  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  231 reducing lr --   old_lr:  8.589934592000016e-25  new_lr:  1.7179869184000032e-25\n",
      "iter:  232  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  232 reducing lr --   old_lr:  1.7179869184000032e-25  new_lr:  3.4359738368000067e-26\n",
      "iter:  233  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  233 reducing lr --   old_lr:  1.7179869184000032e-25  new_lr:  3.4359738368000067e-26\n",
      "iter:  234  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  234 reducing lr --   old_lr:  3.4359738368000067e-26  new_lr:  6.871947673600013e-27\n",
      "iter:  235  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  235 reducing lr --   old_lr:  3.4359738368000067e-26  new_lr:  6.871947673600013e-27\n",
      "iter:  236  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  236 reducing lr --   old_lr:  6.871947673600013e-27  new_lr:  1.3743895347200028e-27\n",
      "iter:  237  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  237 reducing lr --   old_lr:  6.871947673600013e-27  new_lr:  1.3743895347200028e-27\n",
      "iter:  238  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  238 reducing lr --   old_lr:  1.3743895347200028e-27  new_lr:  2.7487790694400056e-28\n",
      "iter:  239  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  239 reducing lr --   old_lr:  1.3743895347200028e-27  new_lr:  2.7487790694400056e-28\n",
      "iter:  240  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  240 reducing lr --   old_lr:  2.7487790694400056e-28  new_lr:  5.497558138880012e-29\n",
      "iter:  241  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  241 reducing lr --   old_lr:  2.7487790694400056e-28  new_lr:  5.497558138880012e-29\n",
      "iter:  242  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  242 reducing lr --   old_lr:  5.497558138880012e-29  new_lr:  1.0995116277760024e-29\n",
      "iter:  243  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  243 reducing lr --   old_lr:  5.497558138880012e-29  new_lr:  1.0995116277760024e-29\n",
      "iter:  244  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  244 reducing lr --   old_lr:  1.0995116277760024e-29  new_lr:  2.199023255552005e-30\n",
      "iter:  245  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  245 reducing lr --   old_lr:  1.0995116277760024e-29  new_lr:  2.199023255552005e-30\n",
      "iter:  246  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  246 reducing lr --   old_lr:  2.199023255552005e-30  new_lr:  1e-30\n",
      "iter:  247  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  247 reducing lr --   old_lr:  2.199023255552005e-30  new_lr:  1e-30\n",
      "iter:  248  Current_loss:  119445.265625 best:  119412.53125\n",
      "iter:  249  Current_loss:  119368.75 best:  119412.53125\n",
      "iter:  250  Current_loss:  119342.25 best:  119368.75\n",
      "[250]\ttrain-rmse:107481\teval-rmse:119225\n",
      "iter:  251  Current_loss:  119225.101562 best:  119342.25\n",
      "iter:  252  Current_loss:  119203.171875 best:  119225.101562\n",
      "iter:  253  Current_loss:  119169.015625 best:  119203.171875\n",
      "iter:  254  Current_loss:  119247.398438 best:  119169.015625\n",
      "iter:  254 reducing lr --   old_lr:  0.1  new_lr:  0.020000000000000004\n",
      "iter:  255  Current_loss:  119223.359375 best:  119169.015625\n",
      "iter:  255 reducing lr --   old_lr:  0.1  new_lr:  0.020000000000000004\n",
      "iter:  256  Current_loss:  119215.148438 best:  119169.015625\n",
      "iter:  256 reducing lr --   old_lr:  0.020000000000000004  new_lr:  0.004000000000000001\n",
      "iter:  257  Current_loss:  119212.695312 best:  119169.015625\n",
      "iter:  257 reducing lr --   old_lr:  0.020000000000000004  new_lr:  0.004000000000000001\n",
      "iter:  258  Current_loss:  119212.195312 best:  119169.015625\n",
      "iter:  258 reducing lr --   old_lr:  0.004000000000000001  new_lr:  0.0008000000000000003\n",
      "iter:  259  Current_loss:  119211.8125 best:  119169.015625\n",
      "iter:  259 reducing lr --   old_lr:  0.004000000000000001  new_lr:  0.0008000000000000003\n",
      "iter:  260  Current_loss:  119211.046875 best:  119169.015625\n",
      "iter:  260 reducing lr --   old_lr:  0.0008000000000000003  new_lr:  0.00016000000000000007\n",
      "iter:  261  Current_loss:  119211.0 best:  119169.015625\n",
      "iter:  261 reducing lr --   old_lr:  0.0008000000000000003  new_lr:  0.00016000000000000007\n",
      "iter:  262  Current_loss:  119210.90625 best:  119169.015625\n",
      "iter:  262 reducing lr --   old_lr:  0.00016000000000000007  new_lr:  3.200000000000001e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  263  Current_loss:  119210.914062 best:  119169.015625\n",
      "iter:  263 reducing lr --   old_lr:  0.00016000000000000007  new_lr:  3.200000000000001e-05\n",
      "iter:  264  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  264 reducing lr --   old_lr:  3.200000000000001e-05  new_lr:  6.400000000000003e-06\n",
      "iter:  265  Current_loss:  119210.851562 best:  119169.015625\n",
      "iter:  265 reducing lr --   old_lr:  3.200000000000001e-05  new_lr:  6.400000000000003e-06\n",
      "iter:  266  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  266 reducing lr --   old_lr:  6.400000000000003e-06  new_lr:  1.2800000000000007e-06\n",
      "iter:  267  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  267 reducing lr --   old_lr:  6.400000000000003e-06  new_lr:  1.2800000000000007e-06\n",
      "iter:  268  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  268 reducing lr --   old_lr:  1.2800000000000007e-06  new_lr:  2.560000000000001e-07\n",
      "iter:  269  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  269 reducing lr --   old_lr:  1.2800000000000007e-06  new_lr:  2.560000000000001e-07\n",
      "iter:  270  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  270 reducing lr --   old_lr:  2.560000000000001e-07  new_lr:  5.120000000000003e-08\n",
      "iter:  271  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  271 reducing lr --   old_lr:  2.560000000000001e-07  new_lr:  5.120000000000003e-08\n",
      "iter:  272  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  272 reducing lr --   old_lr:  5.120000000000003e-08  new_lr:  1.0240000000000006e-08\n",
      "iter:  273  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  273 reducing lr --   old_lr:  5.120000000000003e-08  new_lr:  1.0240000000000006e-08\n",
      "iter:  274  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  274 reducing lr --   old_lr:  1.0240000000000006e-08  new_lr:  2.048000000000001e-09\n",
      "iter:  275  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  275 reducing lr --   old_lr:  1.0240000000000006e-08  new_lr:  2.048000000000001e-09\n",
      "iter:  276  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  276 reducing lr --   old_lr:  2.048000000000001e-09  new_lr:  4.0960000000000027e-10\n",
      "iter:  277  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  277 reducing lr --   old_lr:  2.048000000000001e-09  new_lr:  4.0960000000000027e-10\n",
      "iter:  278  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  278 reducing lr --   old_lr:  4.0960000000000027e-10  new_lr:  8.192000000000006e-11\n",
      "iter:  279  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  279 reducing lr --   old_lr:  4.0960000000000027e-10  new_lr:  8.192000000000006e-11\n",
      "iter:  280  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  280 reducing lr --   old_lr:  8.192000000000006e-11  new_lr:  1.6384000000000014e-11\n",
      "iter:  281  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  281 reducing lr --   old_lr:  8.192000000000006e-11  new_lr:  1.6384000000000014e-11\n",
      "iter:  282  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  282 reducing lr --   old_lr:  1.6384000000000014e-11  new_lr:  3.276800000000003e-12\n",
      "iter:  283  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  283 reducing lr --   old_lr:  1.6384000000000014e-11  new_lr:  3.276800000000003e-12\n",
      "iter:  284  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  284 reducing lr --   old_lr:  3.276800000000003e-12  new_lr:  6.553600000000007e-13\n",
      "iter:  285  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  285 reducing lr --   old_lr:  3.276800000000003e-12  new_lr:  6.553600000000007e-13\n",
      "iter:  286  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  286 reducing lr --   old_lr:  6.553600000000007e-13  new_lr:  1.3107200000000014e-13\n",
      "iter:  287  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  287 reducing lr --   old_lr:  6.553600000000007e-13  new_lr:  1.3107200000000014e-13\n",
      "iter:  288  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  288 reducing lr --   old_lr:  1.3107200000000014e-13  new_lr:  2.6214400000000028e-14\n",
      "iter:  289  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  289 reducing lr --   old_lr:  1.3107200000000014e-13  new_lr:  2.6214400000000028e-14\n",
      "iter:  290  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  290 reducing lr --   old_lr:  2.6214400000000028e-14  new_lr:  5.242880000000006e-15\n",
      "iter:  291  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  291 reducing lr --   old_lr:  2.6214400000000028e-14  new_lr:  5.242880000000006e-15\n",
      "iter:  292  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  292 reducing lr --   old_lr:  5.242880000000006e-15  new_lr:  1.0485760000000013e-15\n",
      "iter:  293  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  293 reducing lr --   old_lr:  5.242880000000006e-15  new_lr:  1.0485760000000013e-15\n",
      "iter:  294  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  294 reducing lr --   old_lr:  1.0485760000000013e-15  new_lr:  2.0971520000000026e-16\n",
      "iter:  295  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  295 reducing lr --   old_lr:  1.0485760000000013e-15  new_lr:  2.0971520000000026e-16\n",
      "iter:  296  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  296 reducing lr --   old_lr:  2.0971520000000026e-16  new_lr:  4.1943040000000054e-17\n",
      "iter:  297  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  297 reducing lr --   old_lr:  2.0971520000000026e-16  new_lr:  4.1943040000000054e-17\n",
      "iter:  298  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  298 reducing lr --   old_lr:  4.1943040000000054e-17  new_lr:  8.388608000000012e-18\n",
      "iter:  299  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  299 reducing lr --   old_lr:  4.1943040000000054e-17  new_lr:  8.388608000000012e-18\n",
      "iter:  300  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  300 reducing lr --   old_lr:  8.388608000000012e-18  new_lr:  1.6777216000000024e-18\n",
      "[300]\ttrain-rmse:107211\teval-rmse:119211\n",
      "iter:  301  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  301 reducing lr --   old_lr:  8.388608000000012e-18  new_lr:  1.6777216000000024e-18\n",
      "iter:  302  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  302 reducing lr --   old_lr:  1.6777216000000024e-18  new_lr:  3.355443200000005e-19\n",
      "iter:  303  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  303 reducing lr --   old_lr:  1.6777216000000024e-18  new_lr:  3.355443200000005e-19\n",
      "iter:  304  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  304 reducing lr --   old_lr:  3.355443200000005e-19  new_lr:  6.71088640000001e-20\n",
      "iter:  305  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  305 reducing lr --   old_lr:  3.355443200000005e-19  new_lr:  6.71088640000001e-20\n",
      "iter:  306  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  306 reducing lr --   old_lr:  6.71088640000001e-20  new_lr:  1.342177280000002e-20\n",
      "iter:  307  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  307 reducing lr --   old_lr:  6.71088640000001e-20  new_lr:  1.342177280000002e-20\n",
      "iter:  308  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  308 reducing lr --   old_lr:  1.342177280000002e-20  new_lr:  2.6843545600000044e-21\n",
      "iter:  309  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  309 reducing lr --   old_lr:  1.342177280000002e-20  new_lr:  2.6843545600000044e-21\n",
      "iter:  310  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  310 reducing lr --   old_lr:  2.6843545600000044e-21  new_lr:  5.368709120000009e-22\n",
      "iter:  311  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  311 reducing lr --   old_lr:  2.6843545600000044e-21  new_lr:  5.368709120000009e-22\n",
      "iter:  312  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  312 reducing lr --   old_lr:  5.368709120000009e-22  new_lr:  1.0737418240000017e-22\n",
      "iter:  313  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  313 reducing lr --   old_lr:  5.368709120000009e-22  new_lr:  1.0737418240000017e-22\n",
      "iter:  314  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  314 reducing lr --   old_lr:  1.0737418240000017e-22  new_lr:  2.1474836480000036e-23\n",
      "iter:  315  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  315 reducing lr --   old_lr:  1.0737418240000017e-22  new_lr:  2.1474836480000036e-23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  316  Current_loss:  119210.859375 best:  119169.015625\n",
      "iter:  316 reducing lr --   old_lr:  2.1474836480000036e-23  new_lr:  4.2949672960000076e-24\n"
     ]
    }
   ],
   "source": [
    "model1 = run_gbm(dtrain, deval, params,progress, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "model = XGBRegressor(n_estimators=num_epochs, learning_rate=0.1)\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=[\"error\", \"rmse\"], eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve performance metrics\n",
    "results = model.evals_result()\n",
    "#results1 = model1.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(25, epochs)\n",
    "\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(10, 10))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'][25::], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'][25::], label='Eval')\n",
    "ax.plot(x_axis, progress['train']['rmse'][25::], label = 'customTrain')\n",
    "ax.plot(x_axis, progress['eval']['rmse'][25::], label = 'customEval')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "\n",
    "pyplot.show()\n",
    "print(\"min customEval:\", min(progress['eval']['rmse']))\n",
    "print(\"min Eval:\", min(results['validation_1']['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classification error\n",
    "# fig, ax = pyplot.subplots()\n",
    "# ax.plot(x_axis, results['validation_0']['error'][25::], label='Train')\n",
    "# ax.plot(x_axis, results['validation_1']['error'][25::], label='Test')\n",
    "# ax.legend()\n",
    "# pyplot.ylabel('Classification Error')\n",
    "# pyplot.title('XGBoost Classification Error')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

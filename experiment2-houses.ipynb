{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agavrilenko/anaconda3/lib/python3.6/site-packages/xgboost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.dirname(xgb.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round: int\n",
      "        Number of boosting iterations.\n",
      "    evals: list of pairs (DMatrix, string)\n",
      "        List of items to be evaluated during training, this allows user to watch\n",
      "        performance on the validation set.\n",
      "    obj : function\n",
      "        Customized objective function.\n",
      "    feval : function\n",
      "        Customized evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Validation error needs to decrease at least\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        If there's more than one, will use the last.\n",
      "        Returns the model from the last iteration (not the best one).\n",
      "        If early stopping occurs, the model will have three additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "        (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "        ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "    evals_result: dict\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval : bool or int\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    learning_rates: list or function (deprecated - use callback API instead)\n",
      "        List of learning rate for each boosting round\n",
      "        or a customized function that calculates eta in terms of\n",
      "        current number of round and the total number of boosting round (e.g. yields\n",
      "        learning rate decay)\n",
      "    xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(xgboost.training.train)\n",
    "help(xgb.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df_usa = pd.read_csv('data/houses/housesalesprediction/kc_house_data.csv', delimiter=\",\")\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "\n",
    "df_usa.drop(['id', 'date'], axis=1, inplace=True)\n",
    "X = df_usa.drop(\"price\",axis=1).values\n",
    "y = df_usa[\"price\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=3)\n",
    "\n",
    "# split data into X and y\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# # split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data = X_train, label = y_train, silent= True)\n",
    "\n",
    "deval = xgb.DMatrix(data = X_test, label = y_test, silent= True)\n",
    "# evals_list = []\n",
    "# evals_list.append((deval, \"eval matrix\"))\n",
    "num_epochs = 1000\n",
    "params = {}\n",
    "params['num_boost_round'] = num_epochs\n",
    "params['early_stopping_rounds'] = 1000\n",
    "#params['verbose_eval'] = 1\n",
    "params['eval_metric'] = 'rmse'\n",
    "params['evals_result'] = {}\n",
    "params['max_depth'] = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = []\n",
    "tresholds = []\n",
    "learning_rates.append(0.1)\n",
    "counter = 0\n",
    "best = []\n",
    "progress = dict()\n",
    "wait = []\n",
    "def reduceLRonPlateau(i,n):\n",
    "\n",
    "    factor = 0.5\n",
    "    min_lr = 1e-15\n",
    "    min_delta = 20\n",
    "    patience = 10\n",
    "    verbose = 0\n",
    "    cooldown = 0\n",
    "    cooldown_counter = 0  # Cooldown counter.\n",
    "    \n",
    "    #best = 0\n",
    "    mode = 'min'  \n",
    "    monitor_op = None\n",
    "    old_lr = learning_rates[0]\n",
    "    new_lr = learning_rates[0] \n",
    "    \n",
    "    def in_cooldown():\n",
    "            return cooldown_counter > 0\n",
    "    \n",
    "    def _reset(mode):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\"\"\"\n",
    "#         if mode not in ['auto', 'min', 'max']:\n",
    "#             warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n",
    "#                           'fallback to auto mode.' % (mode),\n",
    "#                           RuntimeWarning)\n",
    "#             mode = 'auto'\n",
    "#         if (mode == 'min' or\n",
    "#            (mode == 'auto' and 'acc' not in monitor)):\n",
    "#             monitor_op = lambda a, b: np.less(a, b - min_delta)\n",
    "#             best = np.Inf\n",
    "#         else:\n",
    "#             monitor_op = lambda a, b: np.greater(a, b + min_delta)\n",
    "#             best = -np.Inf\n",
    "            \n",
    "        cooldown_counter = 0\n",
    "        del wait[:]\n",
    "        return wait, cooldown_counter\n",
    "    \n",
    "    def checker(wait,best, new_lr, learning_rates):\n",
    "        if (i == 0):\n",
    "            wait, counter = _reset(mode)\n",
    "            print(\"initialization...\")\n",
    "            print(\"lr: \", learning_rates[0])\n",
    "            best.append(100000000)\n",
    "            \n",
    "        if (i>0):\n",
    "            old_lr = learning_rates[i-1]\n",
    "            #new_lr = learning_rates[i-1]\n",
    "            current = progress['eval']['rmse'][i-1]\n",
    "            if in_cooldown():\n",
    "                cooldown_counter -= 1\n",
    "                del wait[:]\n",
    "              \n",
    "            print(\"iter: \", i, \" Current_loss: \", current, \"best: \", best[i-1])\n",
    "            if np.less(current, best[i-1] - min_delta):\n",
    "                best.append(current)\n",
    "                del wait[:]\n",
    "             \n",
    "            elif not in_cooldown():\n",
    "                #rint(\"not in_cooldown\")\n",
    "                best.append(best[-1])\n",
    "                wait.append(1)\n",
    "                if len(wait) >= patience:\n",
    "                    \n",
    "                    if old_lr > min_lr:\n",
    "                        new_lr = old_lr * factor\n",
    "                        new_lr = max(new_lr, min_lr)\n",
    "                        print(\"iter: \", i, \"reducing lr -- \", \" old_lr: \", old_lr, \" new_lr: \", new_lr)\n",
    "                        \n",
    "#                         if self.verbose > 0:\n",
    "#                             print('\\nEpoch %05d: ReduceLROnPlateau reducing '\n",
    "#                                   'learning rate to %s.' % (epoch + 1, new_lr))\n",
    "                        cooldown_counter = cooldown\n",
    "                        del wait[:]\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "       # print(\"step \",progress['train']['logloss'][i-1], \"i = \", i)\n",
    "\n",
    "            \n",
    "    \n",
    "    new_lr = checker(wait,best,new_lr,learning_rates)\n",
    "    learning_rates.append(new_lr)\n",
    "        \n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_gbm(dtrain, dvalid, param, evals_result, learning_rate):\n",
    "    # check training arguments in param\n",
    "    n_round = param.get('num_boost_round', 100)\n",
    "    early_stop = param.get('early_stopping_rounds', 0)\n",
    "    verbose_eval = param.get('verbose_eval', 50)\n",
    "    # specify validations set to watch performance\n",
    "    watchlist = [(dtrain,'train') ,(deval,'eval')]\n",
    "    #callbacks_list = [learning_rates]\n",
    "\n",
    "    bst = xgb.train(params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=n_round,\n",
    "            evals=watchlist,\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval = verbose_eval,\n",
    "            evals_result = evals_result, \n",
    "            callbacks = [xgb.callback.reset_learning_rate(reduceLRonPlateau)])\n",
    "                   \n",
    "    return bst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization...\n",
      "lr:  0.1\n",
      "[0]\ttrain-rmse:597658\teval-rmse:590266\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 1000 rounds.\n",
      "iter:  1  Current_loss:  590265.5 best:  100000000\n",
      "iter:  2  Current_loss:  539130.9375 best:  590265.5\n",
      "iter:  3  Current_loss:  493740.15625 best:  539130.9375\n",
      "iter:  4  Current_loss:  453324.34375 best:  493740.15625\n",
      "iter:  5  Current_loss:  417079.59375 best:  453324.34375\n",
      "iter:  6  Current_loss:  385368.84375 best:  417079.59375\n",
      "iter:  7  Current_loss:  357464.09375 best:  385368.84375\n",
      "iter:  8  Current_loss:  332269.125 best:  357464.09375\n",
      "iter:  9  Current_loss:  310484.65625 best:  332269.125\n",
      "iter:  10  Current_loss:  290920.0 best:  310484.65625\n",
      "iter:  11  Current_loss:  273306.21875 best:  290920.0\n",
      "iter:  12  Current_loss:  258614.453125 best:  273306.21875\n",
      "iter:  13  Current_loss:  245829.46875 best:  258614.453125\n",
      "iter:  14  Current_loss:  233888.078125 best:  245829.46875\n",
      "iter:  15  Current_loss:  222381.3125 best:  233888.078125\n",
      "iter:  16  Current_loss:  212981.96875 best:  222381.3125\n",
      "iter:  17  Current_loss:  205294.609375 best:  212981.96875\n",
      "iter:  18  Current_loss:  198489.578125 best:  205294.609375\n",
      "iter:  19  Current_loss:  191642.0625 best:  198489.578125\n",
      "iter:  20  Current_loss:  186847.484375 best:  191642.0625\n",
      "iter:  21  Current_loss:  182536.53125 best:  186847.484375\n",
      "iter:  22  Current_loss:  178683.59375 best:  182536.53125\n",
      "iter:  23  Current_loss:  175017.453125 best:  178683.59375\n",
      "iter:  24  Current_loss:  171210.4375 best:  175017.453125\n",
      "iter:  25  Current_loss:  168738.21875 best:  171210.4375\n",
      "iter:  26  Current_loss:  165576.28125 best:  168738.21875\n",
      "iter:  27  Current_loss:  163297.0 best:  165576.28125\n",
      "iter:  28  Current_loss:  161055.546875 best:  163297.0\n",
      "iter:  29  Current_loss:  159055.078125 best:  161055.546875\n",
      "iter:  30  Current_loss:  157250.828125 best:  159055.078125\n",
      "iter:  31  Current_loss:  155680.75 best:  157250.828125\n",
      "iter:  32  Current_loss:  154154.921875 best:  155680.75\n",
      "iter:  33  Current_loss:  152804.734375 best:  154154.921875\n",
      "iter:  34  Current_loss:  151355.109375 best:  152804.734375\n",
      "iter:  35  Current_loss:  150350.78125 best:  151355.109375\n",
      "iter:  36  Current_loss:  149572.34375 best:  150350.78125\n",
      "iter:  37  Current_loss:  148521.203125 best:  149572.34375\n",
      "iter:  38  Current_loss:  147606.015625 best:  148521.203125\n",
      "iter:  39  Current_loss:  146937.578125 best:  147606.015625\n",
      "iter:  40  Current_loss:  145837.609375 best:  146937.578125\n",
      "iter:  41  Current_loss:  144997.125 best:  145837.609375\n",
      "iter:  42  Current_loss:  143939.5 best:  144997.125\n",
      "iter:  43  Current_loss:  143286.421875 best:  143939.5\n",
      "iter:  44  Current_loss:  141782.203125 best:  143286.421875\n",
      "iter:  45  Current_loss:  140610.578125 best:  141782.203125\n",
      "iter:  46  Current_loss:  140050.09375 best:  140610.578125\n",
      "iter:  47  Current_loss:  139330.984375 best:  140050.09375\n",
      "iter:  48  Current_loss:  138988.71875 best:  139330.984375\n",
      "iter:  49  Current_loss:  138296.78125 best:  138988.71875\n",
      "iter:  50  Current_loss:  137717.46875 best:  138296.78125\n",
      "[50]\ttrain-rmse:131990\teval-rmse:136758\n",
      "iter:  51  Current_loss:  136757.765625 best:  137717.46875\n",
      "iter:  52  Current_loss:  136271.234375 best:  136757.765625\n",
      "iter:  53  Current_loss:  135987.40625 best:  136271.234375\n",
      "iter:  54  Current_loss:  135562.40625 best:  135987.40625\n",
      "iter:  55  Current_loss:  134720.0 best:  135562.40625\n",
      "iter:  56  Current_loss:  134227.1875 best:  134720.0\n",
      "iter:  57  Current_loss:  133810.171875 best:  134227.1875\n",
      "iter:  58  Current_loss:  133503.6875 best:  133810.171875\n",
      "iter:  59  Current_loss:  132845.625 best:  133503.6875\n",
      "iter:  60  Current_loss:  132548.171875 best:  132845.625\n",
      "iter:  61  Current_loss:  132390.65625 best:  132548.171875\n",
      "iter:  62  Current_loss:  131861.21875 best:  132390.65625\n",
      "iter:  63  Current_loss:  131589.96875 best:  131861.21875\n",
      "iter:  64  Current_loss:  131402.6875 best:  131589.96875\n",
      "iter:  65  Current_loss:  131140.421875 best:  131402.6875\n",
      "iter:  66  Current_loss:  130636.210938 best:  131140.421875\n",
      "iter:  67  Current_loss:  130307.796875 best:  130636.210938\n",
      "iter:  68  Current_loss:  130245.40625 best:  130307.796875\n",
      "iter:  69  Current_loss:  130144.679688 best:  130245.40625\n",
      "iter:  70  Current_loss:  130019.046875 best:  130144.679688\n",
      "iter:  71  Current_loss:  129489.15625 best:  130019.046875\n",
      "iter:  72  Current_loss:  129283.632812 best:  129489.15625\n",
      "iter:  73  Current_loss:  129133.789062 best:  129283.632812\n",
      "iter:  74  Current_loss:  128816.570312 best:  129133.789062\n",
      "iter:  75  Current_loss:  128598.890625 best:  128816.570312\n",
      "iter:  76  Current_loss:  128436.023438 best:  128598.890625\n",
      "iter:  77  Current_loss:  128191.703125 best:  128436.023438\n",
      "iter:  78  Current_loss:  128086.445312 best:  128191.703125\n",
      "iter:  79  Current_loss:  127677.398438 best:  128086.445312\n",
      "iter:  80  Current_loss:  127529.578125 best:  127677.398438\n",
      "iter:  81  Current_loss:  127489.53125 best:  127529.578125\n",
      "iter:  82  Current_loss:  127347.429688 best:  127489.53125\n",
      "iter:  83  Current_loss:  127051.328125 best:  127347.429688\n",
      "iter:  84  Current_loss:  126817.765625 best:  127051.328125\n",
      "iter:  85  Current_loss:  126751.78125 best:  126817.765625\n",
      "iter:  86  Current_loss:  126624.9375 best:  126751.78125\n",
      "iter:  87  Current_loss:  126476.023438 best:  126624.9375\n",
      "iter:  88  Current_loss:  126379.390625 best:  126476.023438\n",
      "iter:  89  Current_loss:  126267.445312 best:  126379.390625\n",
      "iter:  90  Current_loss:  125875.429688 best:  126267.445312\n",
      "iter:  91  Current_loss:  125725.742188 best:  125875.429688\n",
      "iter:  92  Current_loss:  125672.4375 best:  125725.742188\n",
      "iter:  93  Current_loss:  125604.367188 best:  125672.4375\n",
      "iter:  94  Current_loss:  125441.75 best:  125604.367188\n",
      "iter:  95  Current_loss:  125332.476562 best:  125441.75\n",
      "iter:  96  Current_loss:  125275.773438 best:  125332.476562\n",
      "iter:  97  Current_loss:  125120.835938 best:  125275.773438\n",
      "iter:  98  Current_loss:  125066.96875 best:  125120.835938\n",
      "iter:  99  Current_loss:  124972.359375 best:  125066.96875\n",
      "iter:  100  Current_loss:  124765.054688 best:  124972.359375\n",
      "[100]\ttrain-rmse:116419\teval-rmse:124757\n",
      "iter:  101  Current_loss:  124756.992188 best:  124765.054688\n",
      "iter:  102  Current_loss:  124692.40625 best:  124765.054688\n",
      "iter:  103  Current_loss:  124562.695312 best:  124692.40625\n",
      "iter:  104  Current_loss:  124128.164062 best:  124562.695312\n",
      "iter:  105  Current_loss:  123971.984375 best:  124128.164062\n",
      "iter:  106  Current_loss:  123918.601562 best:  123971.984375\n",
      "iter:  107  Current_loss:  123873.273438 best:  123918.601562\n",
      "iter:  108  Current_loss:  123818.742188 best:  123873.273438\n",
      "iter:  109  Current_loss:  123688.726562 best:  123818.742188\n",
      "iter:  110  Current_loss:  123391.648438 best:  123688.726562\n",
      "iter:  111  Current_loss:  123255.132812 best:  123391.648438\n",
      "iter:  112  Current_loss:  123150.210938 best:  123255.132812\n",
      "iter:  113  Current_loss:  123039.5625 best:  123150.210938\n",
      "iter:  114  Current_loss:  122876.929688 best:  123039.5625\n",
      "iter:  115  Current_loss:  122857.71875 best:  122876.929688\n",
      "iter:  116  Current_loss:  122800.742188 best:  122876.929688\n",
      "iter:  117  Current_loss:  122680.828125 best:  122800.742188\n",
      "iter:  118  Current_loss:  122653.15625 best:  122680.828125\n",
      "iter:  119  Current_loss:  122638.835938 best:  122653.15625\n",
      "iter:  120  Current_loss:  122517.804688 best:  122653.15625\n",
      "iter:  121  Current_loss:  122390.046875 best:  122517.804688\n",
      "iter:  122  Current_loss:  122171.460938 best:  122390.046875\n",
      "iter:  123  Current_loss:  122060.273438 best:  122171.460938\n",
      "iter:  124  Current_loss:  122018.6875 best:  122060.273438\n",
      "iter:  125  Current_loss:  121913.132812 best:  122018.6875\n",
      "iter:  126  Current_loss:  121911.5625 best:  121913.132812\n",
      "iter:  127  Current_loss:  121856.8125 best:  121913.132812\n",
      "iter:  128  Current_loss:  121772.070312 best:  121856.8125\n",
      "iter:  129  Current_loss:  121588.0 best:  121772.070312\n",
      "iter:  130  Current_loss:  121496.257812 best:  121588.0\n",
      "iter:  131  Current_loss:  121409.03125 best:  121496.257812\n",
      "iter:  132  Current_loss:  121350.75 best:  121409.03125\n",
      "iter:  133  Current_loss:  121259.726562 best:  121350.75\n",
      "iter:  134  Current_loss:  121107.046875 best:  121259.726562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  135  Current_loss:  121011.234375 best:  121107.046875\n",
      "iter:  136  Current_loss:  121005.46875 best:  121011.234375\n",
      "iter:  137  Current_loss:  120957.53125 best:  121011.234375\n",
      "iter:  138  Current_loss:  120903.921875 best:  120957.53125\n",
      "iter:  139  Current_loss:  120814.71875 best:  120903.921875\n",
      "iter:  140  Current_loss:  120776.023438 best:  120814.71875\n",
      "iter:  141  Current_loss:  120747.125 best:  120776.023438\n",
      "iter:  142  Current_loss:  120673.234375 best:  120747.125\n",
      "iter:  143  Current_loss:  120617.757812 best:  120673.234375\n",
      "iter:  144  Current_loss:  120559.195312 best:  120617.757812\n",
      "iter:  145  Current_loss:  120514.46875 best:  120559.195312\n",
      "iter:  146  Current_loss:  120392.351562 best:  120514.46875\n",
      "iter:  147  Current_loss:  120090.507812 best:  120392.351562\n",
      "iter:  148  Current_loss:  120051.8125 best:  120090.507812\n",
      "iter:  149  Current_loss:  120039.710938 best:  120051.8125\n",
      "iter:  150  Current_loss:  120093.492188 best:  120051.8125\n",
      "[150]\ttrain-rmse:108866\teval-rmse:119948\n",
      "iter:  151  Current_loss:  119947.914062 best:  120051.8125\n",
      "iter:  152  Current_loss:  119886.539062 best:  119947.914062\n",
      "iter:  153  Current_loss:  119832.59375 best:  119886.539062\n",
      "iter:  154  Current_loss:  119816.703125 best:  119832.59375\n",
      "iter:  155  Current_loss:  119798.828125 best:  119832.59375\n",
      "iter:  156  Current_loss:  119760.851562 best:  119798.828125\n",
      "iter:  157  Current_loss:  119661.039062 best:  119760.851562\n",
      "iter:  158  Current_loss:  119584.703125 best:  119661.039062\n",
      "iter:  159  Current_loss:  119480.15625 best:  119584.703125\n",
      "iter:  160  Current_loss:  119427.382812 best:  119480.15625\n",
      "iter:  161  Current_loss:  119415.820312 best:  119427.382812\n",
      "iter:  162  Current_loss:  119388.210938 best:  119427.382812\n",
      "iter:  163  Current_loss:  119302.101562 best:  119388.210938\n",
      "iter:  164  Current_loss:  119294.304688 best:  119302.101562\n",
      "iter:  165  Current_loss:  119259.132812 best:  119302.101562\n",
      "iter:  166  Current_loss:  119285.671875 best:  119259.132812\n",
      "iter:  167  Current_loss:  119244.007812 best:  119259.132812\n",
      "iter:  168  Current_loss:  119200.484375 best:  119259.132812\n",
      "iter:  169  Current_loss:  119206.398438 best:  119200.484375\n",
      "iter:  170  Current_loss:  119193.671875 best:  119200.484375\n",
      "iter:  171  Current_loss:  119085.039062 best:  119200.484375\n",
      "iter:  172  Current_loss:  119040.96875 best:  119085.039062\n",
      "iter:  173  Current_loss:  118943.09375 best:  119040.96875\n",
      "iter:  174  Current_loss:  118950.625 best:  118943.09375\n",
      "iter:  175  Current_loss:  118926.710938 best:  118943.09375\n",
      "iter:  176  Current_loss:  118918.359375 best:  118943.09375\n",
      "iter:  177  Current_loss:  118872.375 best:  118918.359375\n",
      "iter:  178  Current_loss:  118863.21875 best:  118872.375\n",
      "iter:  179  Current_loss:  118862.976562 best:  118872.375\n",
      "iter:  180  Current_loss:  118798.125 best:  118872.375\n",
      "iter:  181  Current_loss:  118763.179688 best:  118798.125\n",
      "iter:  182  Current_loss:  118726.460938 best:  118763.179688\n",
      "iter:  183  Current_loss:  118693.320312 best:  118726.460938\n",
      "iter:  184  Current_loss:  118666.8125 best:  118693.320312\n",
      "iter:  185  Current_loss:  118652.046875 best:  118666.8125\n",
      "iter:  186  Current_loss:  118627.546875 best:  118666.8125\n",
      "iter:  187  Current_loss:  118573.585938 best:  118627.546875\n",
      "iter:  188  Current_loss:  118537.414062 best:  118573.585938\n",
      "iter:  189  Current_loss:  118480.695312 best:  118537.414062\n",
      "iter:  190  Current_loss:  118432.796875 best:  118480.695312\n",
      "iter:  191  Current_loss:  118428.109375 best:  118432.796875\n",
      "iter:  192  Current_loss:  118418.390625 best:  118432.796875\n",
      "iter:  193  Current_loss:  118316.140625 best:  118432.796875\n",
      "iter:  194  Current_loss:  118416.5625 best:  118316.140625\n",
      "iter:  195  Current_loss:  118351.59375 best:  118316.140625\n",
      "iter:  196  Current_loss:  118142.71875 best:  118316.140625\n",
      "iter:  197  Current_loss:  118056.40625 best:  118142.71875\n",
      "iter:  198  Current_loss:  118046.679688 best:  118056.40625\n",
      "iter:  199  Current_loss:  118041.953125 best:  118056.40625\n",
      "iter:  200  Current_loss:  118029.234375 best:  118056.40625\n",
      "[200]\ttrain-rmse:104126\teval-rmse:117945\n",
      "iter:  201  Current_loss:  117945.460938 best:  118029.234375\n",
      "iter:  202  Current_loss:  117919.09375 best:  117945.460938\n",
      "iter:  203  Current_loss:  117827.367188 best:  117919.09375\n",
      "iter:  204  Current_loss:  117658.023438 best:  117827.367188\n",
      "iter:  205  Current_loss:  117580.085938 best:  117658.023438\n",
      "iter:  206  Current_loss:  117603.375 best:  117580.085938\n",
      "iter:  207  Current_loss:  117571.570312 best:  117580.085938\n",
      "iter:  208  Current_loss:  117573.859375 best:  117580.085938\n",
      "iter:  209  Current_loss:  117510.0 best:  117580.085938\n",
      "iter:  210  Current_loss:  117526.070312 best:  117510.0\n",
      "iter:  211  Current_loss:  117538.421875 best:  117510.0\n",
      "iter:  212  Current_loss:  117480.320312 best:  117510.0\n",
      "iter:  213  Current_loss:  117486.648438 best:  117480.320312\n",
      "iter:  214  Current_loss:  117532.023438 best:  117480.320312\n",
      "iter:  215  Current_loss:  117508.460938 best:  117480.320312\n",
      "iter:  216  Current_loss:  117545.71875 best:  117480.320312\n",
      "iter:  217  Current_loss:  117471.570312 best:  117480.320312\n",
      "iter:  218  Current_loss:  117369.296875 best:  117480.320312\n",
      "iter:  219  Current_loss:  117348.5625 best:  117369.296875\n",
      "iter:  220  Current_loss:  117277.046875 best:  117348.5625\n",
      "iter:  221  Current_loss:  117260.398438 best:  117277.046875\n",
      "iter:  222  Current_loss:  117294.59375 best:  117277.046875\n",
      "iter:  223  Current_loss:  117284.632812 best:  117277.046875\n",
      "iter:  224  Current_loss:  117203.875 best:  117277.046875\n",
      "iter:  225  Current_loss:  117167.671875 best:  117203.875\n",
      "iter:  226  Current_loss:  117145.289062 best:  117167.671875\n",
      "iter:  227  Current_loss:  117172.960938 best:  117145.289062\n",
      "iter:  228  Current_loss:  117081.695312 best:  117145.289062\n",
      "iter:  229  Current_loss:  117059.398438 best:  117081.695312\n",
      "iter:  230  Current_loss:  117025.296875 best:  117059.398438\n",
      "iter:  231  Current_loss:  117047.734375 best:  117025.296875\n",
      "iter:  232  Current_loss:  116964.984375 best:  117025.296875\n",
      "iter:  233  Current_loss:  116858.890625 best:  116964.984375\n",
      "iter:  234  Current_loss:  116862.039062 best:  116858.890625\n",
      "iter:  235  Current_loss:  116704.179688 best:  116858.890625\n",
      "iter:  236  Current_loss:  116639.125 best:  116704.179688\n",
      "iter:  237  Current_loss:  116608.0 best:  116639.125\n",
      "iter:  238  Current_loss:  116627.835938 best:  116608.0\n",
      "iter:  239  Current_loss:  116576.046875 best:  116608.0\n",
      "iter:  240  Current_loss:  116625.039062 best:  116576.046875\n",
      "iter:  241  Current_loss:  116603.835938 best:  116576.046875\n",
      "iter:  242  Current_loss:  116621.015625 best:  116576.046875\n",
      "iter:  243  Current_loss:  116639.3125 best:  116576.046875\n",
      "iter:  244  Current_loss:  116621.007812 best:  116576.046875\n",
      "iter:  245  Current_loss:  116598.875 best:  116576.046875\n",
      "iter:  246  Current_loss:  116589.609375 best:  116576.046875\n",
      "iter:  247  Current_loss:  116523.40625 best:  116576.046875\n",
      "iter:  248  Current_loss:  116494.9375 best:  116523.40625\n",
      "iter:  249  Current_loss:  116510.257812 best:  116494.9375\n",
      "iter:  250  Current_loss:  116485.429688 best:  116494.9375\n",
      "[250]\ttrain-rmse:100181\teval-rmse:116442\n",
      "iter:  251  Current_loss:  116442.367188 best:  116494.9375\n",
      "iter:  252  Current_loss:  116487.984375 best:  116442.367188\n",
      "iter:  253  Current_loss:  116413.742188 best:  116442.367188\n",
      "iter:  254  Current_loss:  116319.796875 best:  116413.742188\n",
      "iter:  255  Current_loss:  116239.179688 best:  116319.796875\n",
      "iter:  256  Current_loss:  116232.304688 best:  116239.179688\n",
      "iter:  257  Current_loss:  116215.703125 best:  116239.179688\n",
      "iter:  258  Current_loss:  116141.078125 best:  116215.703125\n",
      "iter:  259  Current_loss:  116116.71875 best:  116141.078125\n",
      "iter:  260  Current_loss:  116093.148438 best:  116116.71875\n",
      "iter:  261  Current_loss:  116030.726562 best:  116093.148438\n",
      "iter:  262  Current_loss:  116003.757812 best:  116030.726562\n",
      "iter:  263  Current_loss:  115985.40625 best:  116003.757812\n",
      "iter:  264  Current_loss:  115950.226562 best:  116003.757812\n",
      "iter:  265  Current_loss:  115960.367188 best:  115950.226562\n",
      "iter:  266  Current_loss:  115902.242188 best:  115950.226562\n",
      "iter:  267  Current_loss:  115909.046875 best:  115902.242188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  268  Current_loss:  115893.296875 best:  115902.242188\n",
      "iter:  269  Current_loss:  115919.476562 best:  115902.242188\n",
      "iter:  270  Current_loss:  115887.273438 best:  115902.242188\n",
      "iter:  271  Current_loss:  115909.515625 best:  115902.242188\n",
      "iter:  272  Current_loss:  115890.6875 best:  115902.242188\n",
      "iter:  273  Current_loss:  115876.273438 best:  115902.242188\n",
      "iter:  274  Current_loss:  115841.976562 best:  115876.273438\n",
      "iter:  275  Current_loss:  115760.210938 best:  115841.976562\n",
      "iter:  276  Current_loss:  115738.195312 best:  115760.210938\n",
      "iter:  277  Current_loss:  115684.4375 best:  115738.195312\n",
      "iter:  278  Current_loss:  115688.367188 best:  115684.4375\n",
      "iter:  279  Current_loss:  115675.804688 best:  115684.4375\n",
      "iter:  280  Current_loss:  115596.445312 best:  115684.4375\n",
      "iter:  281  Current_loss:  115594.835938 best:  115596.445312\n",
      "iter:  282  Current_loss:  115596.84375 best:  115596.445312\n",
      "iter:  283  Current_loss:  115574.796875 best:  115596.445312\n",
      "iter:  284  Current_loss:  115484.203125 best:  115574.796875\n",
      "iter:  285  Current_loss:  115424.835938 best:  115484.203125\n",
      "iter:  286  Current_loss:  115362.796875 best:  115424.835938\n",
      "iter:  287  Current_loss:  115313.875 best:  115362.796875\n",
      "iter:  288  Current_loss:  115217.835938 best:  115313.875\n",
      "iter:  289  Current_loss:  115201.4375 best:  115217.835938\n",
      "iter:  290  Current_loss:  115180.9375 best:  115217.835938\n",
      "iter:  291  Current_loss:  115075.890625 best:  115180.9375\n",
      "iter:  292  Current_loss:  115048.132812 best:  115075.890625\n",
      "iter:  293  Current_loss:  115134.570312 best:  115048.132812\n",
      "iter:  294  Current_loss:  115112.15625 best:  115048.132812\n",
      "iter:  295  Current_loss:  115107.179688 best:  115048.132812\n",
      "iter:  296  Current_loss:  115058.476562 best:  115048.132812\n",
      "iter:  297  Current_loss:  115061.484375 best:  115048.132812\n",
      "iter:  298  Current_loss:  115028.96875 best:  115048.132812\n",
      "iter:  299  Current_loss:  114971.796875 best:  115048.132812\n",
      "iter:  300  Current_loss:  114956.734375 best:  114971.796875\n",
      "[300]\ttrain-rmse:96636.1\teval-rmse:114935\n",
      "iter:  301  Current_loss:  114934.734375 best:  114971.796875\n",
      "iter:  302  Current_loss:  114933.375 best:  114934.734375\n",
      "iter:  303  Current_loss:  114878.90625 best:  114934.734375\n",
      "iter:  304  Current_loss:  114842.0 best:  114878.90625\n",
      "iter:  305  Current_loss:  114800.601562 best:  114842.0\n",
      "iter:  306  Current_loss:  114815.164062 best:  114800.601562\n",
      "iter:  307  Current_loss:  114774.734375 best:  114800.601562\n",
      "iter:  308  Current_loss:  114767.648438 best:  114774.734375\n",
      "iter:  309  Current_loss:  114758.679688 best:  114774.734375\n",
      "iter:  310  Current_loss:  114603.914062 best:  114774.734375\n",
      "iter:  311  Current_loss:  114578.515625 best:  114603.914062\n",
      "iter:  312  Current_loss:  114551.273438 best:  114578.515625\n",
      "iter:  313  Current_loss:  114558.1875 best:  114551.273438\n",
      "iter:  314  Current_loss:  114543.53125 best:  114551.273438\n",
      "iter:  315  Current_loss:  114425.75 best:  114551.273438\n",
      "iter:  316  Current_loss:  114404.859375 best:  114425.75\n",
      "iter:  317  Current_loss:  114405.023438 best:  114404.859375\n",
      "iter:  318  Current_loss:  114405.453125 best:  114404.859375\n",
      "iter:  319  Current_loss:  114400.390625 best:  114404.859375\n",
      "iter:  320  Current_loss:  114395.773438 best:  114404.859375\n",
      "iter:  321  Current_loss:  114375.59375 best:  114404.859375\n",
      "iter:  322  Current_loss:  114356.21875 best:  114375.59375\n",
      "iter:  323  Current_loss:  114369.328125 best:  114375.59375\n",
      "iter:  324  Current_loss:  114375.304688 best:  114375.59375\n",
      "iter:  325  Current_loss:  114398.453125 best:  114375.59375\n",
      "iter:  326  Current_loss:  114379.570312 best:  114375.59375\n",
      "iter:  327  Current_loss:  114355.046875 best:  114375.59375\n",
      "iter:  328  Current_loss:  114334.96875 best:  114355.046875\n",
      "iter:  329  Current_loss:  114326.226562 best:  114334.96875\n",
      "iter:  330  Current_loss:  114290.84375 best:  114334.96875\n",
      "iter:  331  Current_loss:  114260.8125 best:  114290.84375\n",
      "iter:  332  Current_loss:  114226.359375 best:  114260.8125\n",
      "iter:  333  Current_loss:  114205.945312 best:  114226.359375\n",
      "iter:  334  Current_loss:  114131.21875 best:  114205.945312\n",
      "iter:  335  Current_loss:  114141.265625 best:  114131.21875\n",
      "iter:  336  Current_loss:  114122.492188 best:  114131.21875\n",
      "iter:  337  Current_loss:  114114.320312 best:  114131.21875\n",
      "iter:  338  Current_loss:  114087.0625 best:  114131.21875\n",
      "iter:  339  Current_loss:  114064.414062 best:  114087.0625\n",
      "iter:  340  Current_loss:  114048.445312 best:  114064.414062\n",
      "iter:  341  Current_loss:  114035.304688 best:  114064.414062\n",
      "iter:  342  Current_loss:  114018.367188 best:  114035.304688\n",
      "iter:  343  Current_loss:  113998.671875 best:  114035.304688\n",
      "iter:  344  Current_loss:  114007.617188 best:  113998.671875\n",
      "iter:  345  Current_loss:  113931.484375 best:  113998.671875\n",
      "iter:  346  Current_loss:  113959.304688 best:  113931.484375\n",
      "iter:  347  Current_loss:  113954.492188 best:  113931.484375\n",
      "iter:  348  Current_loss:  113898.0625 best:  113931.484375\n",
      "iter:  349  Current_loss:  113880.179688 best:  113898.0625\n",
      "iter:  350  Current_loss:  113958.835938 best:  113898.0625\n",
      "[350]\ttrain-rmse:94065.6\teval-rmse:113942\n",
      "iter:  351  Current_loss:  113942.164062 best:  113898.0625\n",
      "iter:  352  Current_loss:  113939.648438 best:  113898.0625\n",
      "iter:  353  Current_loss:  113946.070312 best:  113898.0625\n",
      "iter:  354  Current_loss:  113949.40625 best:  113898.0625\n",
      "iter:  355  Current_loss:  113929.476562 best:  113898.0625\n",
      "iter:  356  Current_loss:  113900.929688 best:  113898.0625\n",
      "iter:  357  Current_loss:  113860.007812 best:  113898.0625\n",
      "iter:  358  Current_loss:  113854.265625 best:  113860.007812\n",
      "iter:  359  Current_loss:  113806.960938 best:  113860.007812\n",
      "iter:  360  Current_loss:  113840.609375 best:  113806.960938\n",
      "iter:  361  Current_loss:  113862.765625 best:  113806.960938\n",
      "iter:  362  Current_loss:  113887.007812 best:  113806.960938\n",
      "iter:  363  Current_loss:  113802.179688 best:  113806.960938\n",
      "iter:  364  Current_loss:  113724.242188 best:  113806.960938\n",
      "iter:  365  Current_loss:  113745.40625 best:  113724.242188\n",
      "iter:  366  Current_loss:  113756.0625 best:  113724.242188\n",
      "iter:  367  Current_loss:  113748.945312 best:  113724.242188\n",
      "iter:  368  Current_loss:  113716.976562 best:  113724.242188\n",
      "iter:  369  Current_loss:  113682.492188 best:  113724.242188\n",
      "iter:  370  Current_loss:  113686.351562 best:  113682.492188\n",
      "iter:  371  Current_loss:  113688.0625 best:  113682.492188\n",
      "iter:  372  Current_loss:  113728.898438 best:  113682.492188\n",
      "iter:  373  Current_loss:  113717.226562 best:  113682.492188\n",
      "iter:  374  Current_loss:  113704.484375 best:  113682.492188\n",
      "iter:  375  Current_loss:  113656.59375 best:  113682.492188\n",
      "iter:  376  Current_loss:  113670.898438 best:  113656.59375\n",
      "iter:  377  Current_loss:  113656.421875 best:  113656.59375\n",
      "iter:  378  Current_loss:  113611.101562 best:  113656.59375\n",
      "iter:  379  Current_loss:  113540.835938 best:  113611.101562\n",
      "iter:  380  Current_loss:  113547.578125 best:  113540.835938\n",
      "iter:  381  Current_loss:  113538.320312 best:  113540.835938\n",
      "iter:  382  Current_loss:  113512.984375 best:  113540.835938\n",
      "iter:  383  Current_loss:  113498.03125 best:  113512.984375\n",
      "iter:  384  Current_loss:  113483.867188 best:  113512.984375\n",
      "iter:  385  Current_loss:  113502.789062 best:  113483.867188\n",
      "iter:  386  Current_loss:  113493.414062 best:  113483.867188\n",
      "iter:  387  Current_loss:  113490.148438 best:  113483.867188\n",
      "iter:  388  Current_loss:  113481.679688 best:  113483.867188\n",
      "iter:  389  Current_loss:  113474.945312 best:  113483.867188\n",
      "iter:  390  Current_loss:  113507.859375 best:  113483.867188\n",
      "iter:  391  Current_loss:  113507.679688 best:  113483.867188\n",
      "iter:  392  Current_loss:  113492.414062 best:  113483.867188\n",
      "iter:  393  Current_loss:  113470.984375 best:  113483.867188\n",
      "iter:  394  Current_loss:  113464.671875 best:  113483.867188\n",
      "iter:  394 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  395  Current_loss:  113452.3125 best:  113483.867188\n",
      "iter:  396  Current_loss:  113479.601562 best:  113452.3125\n",
      "iter:  397  Current_loss:  113471.539062 best:  113452.3125\n",
      "iter:  398  Current_loss:  113416.304688 best:  113452.3125\n",
      "iter:  399  Current_loss:  113395.039062 best:  113416.304688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  400  Current_loss:  113377.0 best:  113395.039062\n",
      "[400]\ttrain-rmse:91872.1\teval-rmse:113352\n",
      "iter:  401  Current_loss:  113351.726562 best:  113395.039062\n",
      "iter:  402  Current_loss:  113387.84375 best:  113351.726562\n",
      "iter:  403  Current_loss:  113410.460938 best:  113351.726562\n",
      "iter:  404  Current_loss:  113398.625 best:  113351.726562\n",
      "iter:  405  Current_loss:  113346.664062 best:  113351.726562\n",
      "iter:  406  Current_loss:  113335.179688 best:  113351.726562\n",
      "iter:  407  Current_loss:  113321.617188 best:  113351.726562\n",
      "iter:  408  Current_loss:  113319.851562 best:  113321.617188\n",
      "iter:  409  Current_loss:  113310.296875 best:  113321.617188\n",
      "iter:  410  Current_loss:  113305.875 best:  113321.617188\n",
      "iter:  411  Current_loss:  113299.085938 best:  113321.617188\n",
      "iter:  412  Current_loss:  113275.460938 best:  113299.085938\n",
      "iter:  413  Current_loss:  113250.492188 best:  113275.460938\n",
      "iter:  414  Current_loss:  113207.210938 best:  113250.492188\n",
      "iter:  415  Current_loss:  113191.765625 best:  113207.210938\n",
      "iter:  416  Current_loss:  113162.539062 best:  113207.210938\n",
      "iter:  417  Current_loss:  113232.273438 best:  113162.539062\n",
      "iter:  418  Current_loss:  113230.4375 best:  113162.539062\n",
      "iter:  419  Current_loss:  113222.898438 best:  113162.539062\n",
      "iter:  420  Current_loss:  113187.039062 best:  113162.539062\n",
      "iter:  421  Current_loss:  113185.476562 best:  113162.539062\n",
      "iter:  422  Current_loss:  113139.828125 best:  113162.539062\n",
      "iter:  423  Current_loss:  113189.140625 best:  113139.828125\n",
      "iter:  424  Current_loss:  113185.640625 best:  113139.828125\n",
      "iter:  425  Current_loss:  113099.851562 best:  113139.828125\n",
      "iter:  426  Current_loss:  113123.773438 best:  113099.851562\n",
      "iter:  427  Current_loss:  113113.539062 best:  113099.851562\n",
      "iter:  428  Current_loss:  113043.078125 best:  113099.851562\n",
      "iter:  429  Current_loss:  113011.992188 best:  113043.078125\n",
      "iter:  430  Current_loss:  113013.546875 best:  113011.992188\n",
      "iter:  431  Current_loss:  112931.351562 best:  113011.992188\n",
      "iter:  432  Current_loss:  112927.0 best:  112931.351562\n",
      "iter:  433  Current_loss:  112860.78125 best:  112931.351562\n",
      "iter:  434  Current_loss:  112852.664062 best:  112860.78125\n",
      "iter:  435  Current_loss:  112833.671875 best:  112860.78125\n",
      "iter:  436  Current_loss:  112829.09375 best:  112833.671875\n",
      "iter:  437  Current_loss:  112814.03125 best:  112833.671875\n",
      "iter:  438  Current_loss:  112827.898438 best:  112833.671875\n",
      "iter:  439  Current_loss:  112779.6875 best:  112833.671875\n",
      "iter:  440  Current_loss:  112768.023438 best:  112779.6875\n",
      "iter:  441  Current_loss:  112748.8125 best:  112779.6875\n",
      "iter:  442  Current_loss:  112712.023438 best:  112748.8125\n",
      "iter:  443  Current_loss:  112671.796875 best:  112712.023438\n",
      "iter:  444  Current_loss:  112665.664062 best:  112671.796875\n",
      "iter:  445  Current_loss:  112636.34375 best:  112671.796875\n",
      "iter:  446  Current_loss:  112606.617188 best:  112636.34375\n",
      "iter:  447  Current_loss:  112550.28125 best:  112606.617188\n",
      "iter:  448  Current_loss:  112543.65625 best:  112550.28125\n",
      "iter:  449  Current_loss:  112545.335938 best:  112550.28125\n",
      "iter:  450  Current_loss:  112530.09375 best:  112550.28125\n",
      "[450]\ttrain-rmse:89880.3\teval-rmse:112474\n",
      "iter:  451  Current_loss:  112474.085938 best:  112530.09375\n",
      "iter:  452  Current_loss:  112445.78125 best:  112474.085938\n",
      "iter:  453  Current_loss:  112410.507812 best:  112445.78125\n",
      "iter:  454  Current_loss:  112379.703125 best:  112410.507812\n",
      "iter:  455  Current_loss:  112380.742188 best:  112379.703125\n",
      "iter:  456  Current_loss:  112353.015625 best:  112379.703125\n",
      "iter:  457  Current_loss:  112345.664062 best:  112353.015625\n",
      "iter:  458  Current_loss:  112310.78125 best:  112353.015625\n",
      "iter:  459  Current_loss:  112298.15625 best:  112310.78125\n",
      "iter:  460  Current_loss:  112284.929688 best:  112310.78125\n",
      "iter:  461  Current_loss:  112262.210938 best:  112284.929688\n",
      "iter:  462  Current_loss:  112258.414062 best:  112262.210938\n",
      "iter:  463  Current_loss:  112231.671875 best:  112262.210938\n",
      "iter:  464  Current_loss:  112223.703125 best:  112231.671875\n",
      "iter:  465  Current_loss:  112206.59375 best:  112231.671875\n",
      "iter:  466  Current_loss:  112195.414062 best:  112206.59375\n",
      "iter:  467  Current_loss:  112156.820312 best:  112206.59375\n",
      "iter:  468  Current_loss:  112125.804688 best:  112156.820312\n",
      "iter:  469  Current_loss:  112137.546875 best:  112125.804688\n",
      "iter:  470  Current_loss:  112128.25 best:  112125.804688\n",
      "iter:  471  Current_loss:  112098.179688 best:  112125.804688\n",
      "iter:  472  Current_loss:  112086.273438 best:  112098.179688\n",
      "iter:  473  Current_loss:  112080.304688 best:  112098.179688\n",
      "iter:  474  Current_loss:  112068.1875 best:  112098.179688\n",
      "iter:  475  Current_loss:  112072.273438 best:  112068.1875\n",
      "iter:  476  Current_loss:  112069.90625 best:  112068.1875\n",
      "iter:  477  Current_loss:  112064.40625 best:  112068.1875\n",
      "iter:  478  Current_loss:  112040.40625 best:  112068.1875\n",
      "iter:  479  Current_loss:  112026.84375 best:  112040.40625\n",
      "iter:  480  Current_loss:  112040.289062 best:  112040.40625\n",
      "iter:  481  Current_loss:  112059.578125 best:  112040.40625\n",
      "iter:  482  Current_loss:  112054.710938 best:  112040.40625\n",
      "iter:  483  Current_loss:  112050.257812 best:  112040.40625\n",
      "iter:  484  Current_loss:  112044.867188 best:  112040.40625\n",
      "iter:  485  Current_loss:  111996.148438 best:  112040.40625\n",
      "iter:  486  Current_loss:  111994.039062 best:  111996.148438\n",
      "iter:  487  Current_loss:  112026.390625 best:  111996.148438\n",
      "iter:  488  Current_loss:  112007.789062 best:  111996.148438\n",
      "iter:  489  Current_loss:  111998.148438 best:  111996.148438\n",
      "iter:  490  Current_loss:  111988.976562 best:  111996.148438\n",
      "iter:  491  Current_loss:  111990.546875 best:  111996.148438\n",
      "iter:  492  Current_loss:  111966.34375 best:  111996.148438\n",
      "iter:  493  Current_loss:  111995.265625 best:  111966.34375\n",
      "iter:  494  Current_loss:  111970.28125 best:  111966.34375\n",
      "iter:  495  Current_loss:  111970.21875 best:  111966.34375\n",
      "iter:  496  Current_loss:  111925.28125 best:  111966.34375\n",
      "iter:  497  Current_loss:  111909.109375 best:  111925.28125\n",
      "iter:  498  Current_loss:  111839.1875 best:  111925.28125\n",
      "iter:  499  Current_loss:  111805.117188 best:  111839.1875\n",
      "iter:  500  Current_loss:  111754.296875 best:  111805.117188\n",
      "[500]\ttrain-rmse:88055.1\teval-rmse:111743\n",
      "iter:  501  Current_loss:  111743.398438 best:  111754.296875\n",
      "iter:  502  Current_loss:  111743.078125 best:  111754.296875\n",
      "iter:  503  Current_loss:  111749.453125 best:  111754.296875\n",
      "iter:  504  Current_loss:  111742.601562 best:  111754.296875\n",
      "iter:  505  Current_loss:  111741.742188 best:  111754.296875\n",
      "iter:  506  Current_loss:  111736.9375 best:  111754.296875\n",
      "iter:  507  Current_loss:  111686.15625 best:  111754.296875\n",
      "iter:  508  Current_loss:  111707.40625 best:  111686.15625\n",
      "iter:  509  Current_loss:  111698.0 best:  111686.15625\n",
      "iter:  510  Current_loss:  111686.117188 best:  111686.15625\n",
      "iter:  511  Current_loss:  111684.5625 best:  111686.15625\n",
      "iter:  512  Current_loss:  111664.921875 best:  111686.15625\n",
      "iter:  513  Current_loss:  111655.585938 best:  111664.921875\n",
      "iter:  514  Current_loss:  111644.515625 best:  111664.921875\n",
      "iter:  515  Current_loss:  111627.164062 best:  111644.515625\n",
      "iter:  516  Current_loss:  111621.890625 best:  111644.515625\n",
      "iter:  517  Current_loss:  111619.296875 best:  111621.890625\n",
      "iter:  518  Current_loss:  111571.59375 best:  111621.890625\n",
      "iter:  519  Current_loss:  111560.171875 best:  111571.59375\n",
      "iter:  520  Current_loss:  111549.164062 best:  111571.59375\n",
      "iter:  521  Current_loss:  111528.4375 best:  111549.164062\n",
      "iter:  522  Current_loss:  111510.09375 best:  111528.4375\n",
      "iter:  523  Current_loss:  111463.65625 best:  111528.4375\n",
      "iter:  524  Current_loss:  111445.414062 best:  111463.65625\n",
      "iter:  525  Current_loss:  111418.210938 best:  111463.65625\n",
      "iter:  526  Current_loss:  111427.335938 best:  111418.210938\n",
      "iter:  527  Current_loss:  111380.59375 best:  111418.210938\n",
      "iter:  528  Current_loss:  111364.664062 best:  111380.59375\n",
      "iter:  529  Current_loss:  111327.164062 best:  111380.59375\n",
      "iter:  530  Current_loss:  111319.359375 best:  111327.164062\n",
      "iter:  531  Current_loss:  111286.84375 best:  111327.164062\n",
      "iter:  532  Current_loss:  111267.851562 best:  111286.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  533  Current_loss:  111265.46875 best:  111286.84375\n",
      "iter:  534  Current_loss:  111250.710938 best:  111265.46875\n",
      "iter:  535  Current_loss:  111273.09375 best:  111265.46875\n",
      "iter:  536  Current_loss:  111313.554688 best:  111265.46875\n",
      "iter:  537  Current_loss:  111280.15625 best:  111265.46875\n",
      "iter:  538  Current_loss:  111289.359375 best:  111265.46875\n",
      "iter:  539  Current_loss:  111289.570312 best:  111265.46875\n",
      "iter:  540  Current_loss:  111272.851562 best:  111265.46875\n",
      "iter:  541  Current_loss:  111269.539062 best:  111265.46875\n",
      "iter:  542  Current_loss:  111294.882812 best:  111265.46875\n",
      "iter:  543  Current_loss:  111297.546875 best:  111265.46875\n",
      "iter:  543 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  544  Current_loss:  111294.789062 best:  111265.46875\n",
      "iter:  545  Current_loss:  111276.351562 best:  111265.46875\n",
      "iter:  546  Current_loss:  111261.632812 best:  111265.46875\n",
      "iter:  547  Current_loss:  111265.976562 best:  111265.46875\n",
      "iter:  548  Current_loss:  111256.460938 best:  111265.46875\n",
      "iter:  549  Current_loss:  111273.742188 best:  111265.46875\n",
      "iter:  550  Current_loss:  111267.59375 best:  111265.46875\n",
      "[550]\ttrain-rmse:86374.9\teval-rmse:111273\n",
      "iter:  551  Current_loss:  111273.429688 best:  111265.46875\n",
      "iter:  552  Current_loss:  111256.773438 best:  111265.46875\n",
      "iter:  553  Current_loss:  111254.09375 best:  111265.46875\n",
      "iter:  553 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  554  Current_loss:  111289.5 best:  111265.46875\n",
      "iter:  555  Current_loss:  111290.757812 best:  111265.46875\n",
      "iter:  556  Current_loss:  111304.140625 best:  111265.46875\n",
      "iter:  557  Current_loss:  111317.234375 best:  111265.46875\n",
      "iter:  558  Current_loss:  111316.132812 best:  111265.46875\n",
      "iter:  559  Current_loss:  111276.351562 best:  111265.46875\n",
      "iter:  560  Current_loss:  111262.765625 best:  111265.46875\n",
      "iter:  561  Current_loss:  111268.710938 best:  111265.46875\n",
      "iter:  562  Current_loss:  111266.554688 best:  111265.46875\n",
      "iter:  563  Current_loss:  111247.609375 best:  111265.46875\n",
      "iter:  563 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  564  Current_loss:  111228.601562 best:  111265.46875\n",
      "iter:  565  Current_loss:  111192.757812 best:  111228.601562\n",
      "iter:  566  Current_loss:  111194.164062 best:  111192.757812\n",
      "iter:  567  Current_loss:  111186.5625 best:  111192.757812\n",
      "iter:  568  Current_loss:  111161.078125 best:  111192.757812\n",
      "iter:  569  Current_loss:  111203.304688 best:  111161.078125\n",
      "iter:  570  Current_loss:  111159.640625 best:  111161.078125\n",
      "iter:  571  Current_loss:  111134.578125 best:  111161.078125\n",
      "iter:  572  Current_loss:  111134.375 best:  111134.578125\n",
      "iter:  573  Current_loss:  111139.375 best:  111134.578125\n",
      "iter:  574  Current_loss:  111156.804688 best:  111134.578125\n",
      "iter:  575  Current_loss:  111166.953125 best:  111134.578125\n",
      "iter:  576  Current_loss:  111162.632812 best:  111134.578125\n",
      "iter:  577  Current_loss:  111155.765625 best:  111134.578125\n",
      "iter:  578  Current_loss:  111160.773438 best:  111134.578125\n",
      "iter:  579  Current_loss:  111149.320312 best:  111134.578125\n",
      "iter:  580  Current_loss:  111160.554688 best:  111134.578125\n",
      "iter:  581  Current_loss:  111132.117188 best:  111134.578125\n",
      "iter:  581 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  582  Current_loss:  111127.445312 best:  111134.578125\n",
      "iter:  583  Current_loss:  111139.53125 best:  111134.578125\n",
      "iter:  584  Current_loss:  111145.515625 best:  111134.578125\n",
      "iter:  585  Current_loss:  111123.757812 best:  111134.578125\n",
      "iter:  586  Current_loss:  111122.0625 best:  111134.578125\n",
      "iter:  587  Current_loss:  111112.796875 best:  111134.578125\n",
      "iter:  588  Current_loss:  111158.226562 best:  111112.796875\n",
      "iter:  589  Current_loss:  111159.703125 best:  111112.796875\n",
      "iter:  590  Current_loss:  111169.03125 best:  111112.796875\n",
      "iter:  591  Current_loss:  111184.609375 best:  111112.796875\n",
      "iter:  592  Current_loss:  111186.359375 best:  111112.796875\n",
      "iter:  593  Current_loss:  111204.09375 best:  111112.796875\n",
      "iter:  594  Current_loss:  111185.859375 best:  111112.796875\n",
      "iter:  595  Current_loss:  111182.164062 best:  111112.796875\n",
      "iter:  596  Current_loss:  111170.203125 best:  111112.796875\n",
      "iter:  597  Current_loss:  111163.546875 best:  111112.796875\n",
      "iter:  597 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  598  Current_loss:  111172.195312 best:  111112.796875\n",
      "iter:  599  Current_loss:  111168.0625 best:  111112.796875\n",
      "iter:  600  Current_loss:  111165.210938 best:  111112.796875\n",
      "[600]\ttrain-rmse:85121.9\teval-rmse:111133\n",
      "iter:  601  Current_loss:  111133.4375 best:  111112.796875\n",
      "iter:  602  Current_loss:  111146.90625 best:  111112.796875\n",
      "iter:  603  Current_loss:  111163.15625 best:  111112.796875\n",
      "iter:  604  Current_loss:  111160.234375 best:  111112.796875\n",
      "iter:  605  Current_loss:  111141.78125 best:  111112.796875\n",
      "iter:  606  Current_loss:  111108.804688 best:  111112.796875\n",
      "iter:  607  Current_loss:  111075.664062 best:  111112.796875\n",
      "iter:  608  Current_loss:  111059.65625 best:  111075.664062\n",
      "iter:  609  Current_loss:  111050.976562 best:  111075.664062\n",
      "iter:  610  Current_loss:  111048.710938 best:  111050.976562\n",
      "iter:  611  Current_loss:  111046.296875 best:  111050.976562\n",
      "iter:  612  Current_loss:  111044.1875 best:  111050.976562\n",
      "iter:  613  Current_loss:  111039.96875 best:  111050.976562\n",
      "iter:  614  Current_loss:  111057.507812 best:  111050.976562\n",
      "iter:  615  Current_loss:  111053.679688 best:  111050.976562\n",
      "iter:  616  Current_loss:  111059.132812 best:  111050.976562\n",
      "iter:  617  Current_loss:  111060.671875 best:  111050.976562\n",
      "iter:  618  Current_loss:  111022.1875 best:  111050.976562\n",
      "iter:  619  Current_loss:  110989.632812 best:  111022.1875\n",
      "iter:  620  Current_loss:  110936.1875 best:  110989.632812\n",
      "iter:  621  Current_loss:  110950.171875 best:  110936.1875\n",
      "iter:  622  Current_loss:  110973.179688 best:  110936.1875\n",
      "iter:  623  Current_loss:  110957.523438 best:  110936.1875\n",
      "iter:  624  Current_loss:  110949.390625 best:  110936.1875\n",
      "iter:  625  Current_loss:  110959.757812 best:  110936.1875\n",
      "iter:  626  Current_loss:  110941.367188 best:  110936.1875\n",
      "iter:  627  Current_loss:  110946.6875 best:  110936.1875\n",
      "iter:  628  Current_loss:  110948.96875 best:  110936.1875\n",
      "iter:  629  Current_loss:  110955.210938 best:  110936.1875\n",
      "iter:  630  Current_loss:  110966.65625 best:  110936.1875\n",
      "iter:  630 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  631  Current_loss:  110966.796875 best:  110936.1875\n",
      "iter:  632  Current_loss:  110965.085938 best:  110936.1875\n",
      "iter:  633  Current_loss:  110968.1875 best:  110936.1875\n",
      "iter:  634  Current_loss:  110980.640625 best:  110936.1875\n",
      "iter:  635  Current_loss:  110974.109375 best:  110936.1875\n",
      "iter:  636  Current_loss:  110952.25 best:  110936.1875\n",
      "iter:  637  Current_loss:  110946.40625 best:  110936.1875\n",
      "iter:  638  Current_loss:  110929.023438 best:  110936.1875\n",
      "iter:  639  Current_loss:  110898.90625 best:  110936.1875\n",
      "iter:  640  Current_loss:  110893.84375 best:  110898.90625\n",
      "iter:  641  Current_loss:  110886.15625 best:  110898.90625\n",
      "iter:  642  Current_loss:  110877.296875 best:  110898.90625\n",
      "iter:  643  Current_loss:  110869.867188 best:  110877.296875\n",
      "iter:  644  Current_loss:  110875.445312 best:  110877.296875\n",
      "iter:  645  Current_loss:  110858.15625 best:  110877.296875\n",
      "iter:  646  Current_loss:  110837.585938 best:  110877.296875\n",
      "iter:  647  Current_loss:  110839.90625 best:  110837.585938\n",
      "iter:  648  Current_loss:  110824.078125 best:  110837.585938\n",
      "iter:  649  Current_loss:  110826.851562 best:  110837.585938\n",
      "iter:  650  Current_loss:  110843.820312 best:  110837.585938\n",
      "[650]\ttrain-rmse:83648.1\teval-rmse:110838\n",
      "iter:  651  Current_loss:  110838.117188 best:  110837.585938\n",
      "iter:  652  Current_loss:  110825.84375 best:  110837.585938\n",
      "iter:  653  Current_loss:  110818.40625 best:  110837.585938\n",
      "iter:  654  Current_loss:  110810.546875 best:  110837.585938\n",
      "iter:  655  Current_loss:  110773.671875 best:  110810.546875\n",
      "iter:  656  Current_loss:  110778.335938 best:  110773.671875\n",
      "iter:  657  Current_loss:  110745.828125 best:  110773.671875\n",
      "iter:  658  Current_loss:  110748.40625 best:  110745.828125\n",
      "iter:  659  Current_loss:  110762.234375 best:  110745.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  660  Current_loss:  110732.226562 best:  110745.828125\n",
      "iter:  661  Current_loss:  110731.34375 best:  110745.828125\n",
      "iter:  662  Current_loss:  110708.351562 best:  110745.828125\n",
      "iter:  663  Current_loss:  110689.375 best:  110708.351562\n",
      "iter:  664  Current_loss:  110669.460938 best:  110708.351562\n",
      "iter:  665  Current_loss:  110671.71875 best:  110669.460938\n",
      "iter:  666  Current_loss:  110672.789062 best:  110669.460938\n",
      "iter:  667  Current_loss:  110675.734375 best:  110669.460938\n",
      "iter:  668  Current_loss:  110673.429688 best:  110669.460938\n",
      "iter:  669  Current_loss:  110675.25 best:  110669.460938\n",
      "iter:  670  Current_loss:  110643.953125 best:  110669.460938\n",
      "iter:  671  Current_loss:  110649.789062 best:  110643.953125\n",
      "iter:  672  Current_loss:  110671.960938 best:  110643.953125\n",
      "iter:  673  Current_loss:  110668.484375 best:  110643.953125\n",
      "iter:  674  Current_loss:  110681.890625 best:  110643.953125\n",
      "iter:  675  Current_loss:  110679.351562 best:  110643.953125\n",
      "iter:  676  Current_loss:  110672.398438 best:  110643.953125\n",
      "iter:  677  Current_loss:  110660.953125 best:  110643.953125\n",
      "iter:  678  Current_loss:  110660.554688 best:  110643.953125\n",
      "iter:  679  Current_loss:  110680.859375 best:  110643.953125\n",
      "iter:  680  Current_loss:  110688.09375 best:  110643.953125\n",
      "iter:  680 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  681  Current_loss:  110678.875 best:  110643.953125\n",
      "iter:  682  Current_loss:  110687.632812 best:  110643.953125\n",
      "iter:  683  Current_loss:  110672.132812 best:  110643.953125\n",
      "iter:  684  Current_loss:  110650.953125 best:  110643.953125\n",
      "iter:  685  Current_loss:  110652.921875 best:  110643.953125\n",
      "iter:  686  Current_loss:  110657.421875 best:  110643.953125\n",
      "iter:  687  Current_loss:  110648.726562 best:  110643.953125\n",
      "iter:  688  Current_loss:  110634.5 best:  110643.953125\n",
      "iter:  689  Current_loss:  110627.375 best:  110643.953125\n",
      "iter:  690  Current_loss:  110613.445312 best:  110643.953125\n",
      "iter:  691  Current_loss:  110631.0625 best:  110613.445312\n",
      "iter:  692  Current_loss:  110634.351562 best:  110613.445312\n",
      "iter:  693  Current_loss:  110652.570312 best:  110613.445312\n",
      "iter:  694  Current_loss:  110661.859375 best:  110613.445312\n",
      "iter:  695  Current_loss:  110661.140625 best:  110613.445312\n",
      "iter:  696  Current_loss:  110629.203125 best:  110613.445312\n",
      "iter:  697  Current_loss:  110628.828125 best:  110613.445312\n",
      "iter:  698  Current_loss:  110596.054688 best:  110613.445312\n",
      "iter:  699  Current_loss:  110597.632812 best:  110613.445312\n",
      "iter:  700  Current_loss:  110614.804688 best:  110613.445312\n",
      "iter:  700 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "[700]\ttrain-rmse:82346.7\teval-rmse:110612\n",
      "iter:  701  Current_loss:  110612.015625 best:  110613.445312\n",
      "iter:  702  Current_loss:  110592.429688 best:  110613.445312\n",
      "iter:  703  Current_loss:  110579.023438 best:  110592.429688\n",
      "iter:  704  Current_loss:  110547.265625 best:  110592.429688\n",
      "iter:  705  Current_loss:  110545.617188 best:  110547.265625\n",
      "iter:  706  Current_loss:  110505.171875 best:  110547.265625\n",
      "iter:  707  Current_loss:  110498.492188 best:  110505.171875\n",
      "iter:  708  Current_loss:  110503.109375 best:  110505.171875\n",
      "iter:  709  Current_loss:  110469.632812 best:  110505.171875\n",
      "iter:  710  Current_loss:  110505.164062 best:  110469.632812\n",
      "iter:  711  Current_loss:  110501.570312 best:  110469.632812\n",
      "iter:  712  Current_loss:  110498.601562 best:  110469.632812\n",
      "iter:  713  Current_loss:  110516.070312 best:  110469.632812\n",
      "iter:  714  Current_loss:  110526.039062 best:  110469.632812\n",
      "iter:  715  Current_loss:  110521.054688 best:  110469.632812\n",
      "iter:  716  Current_loss:  110523.273438 best:  110469.632812\n",
      "iter:  717  Current_loss:  110516.664062 best:  110469.632812\n",
      "iter:  718  Current_loss:  110512.96875 best:  110469.632812\n",
      "iter:  719  Current_loss:  110515.109375 best:  110469.632812\n",
      "iter:  719 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  720  Current_loss:  110515.085938 best:  110469.632812\n",
      "iter:  721  Current_loss:  110512.414062 best:  110469.632812\n",
      "iter:  722  Current_loss:  110488.914062 best:  110469.632812\n",
      "iter:  723  Current_loss:  110489.5 best:  110469.632812\n",
      "iter:  724  Current_loss:  110489.140625 best:  110469.632812\n",
      "iter:  725  Current_loss:  110492.101562 best:  110469.632812\n",
      "iter:  726  Current_loss:  110492.117188 best:  110469.632812\n",
      "iter:  727  Current_loss:  110490.460938 best:  110469.632812\n",
      "iter:  728  Current_loss:  110489.664062 best:  110469.632812\n",
      "iter:  729  Current_loss:  110525.882812 best:  110469.632812\n",
      "iter:  729 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  730  Current_loss:  110525.039062 best:  110469.632812\n",
      "iter:  731  Current_loss:  110545.90625 best:  110469.632812\n",
      "iter:  732  Current_loss:  110531.796875 best:  110469.632812\n",
      "iter:  733  Current_loss:  110514.414062 best:  110469.632812\n",
      "iter:  734  Current_loss:  110492.679688 best:  110469.632812\n",
      "iter:  735  Current_loss:  110467.875 best:  110469.632812\n",
      "iter:  736  Current_loss:  110457.117188 best:  110469.632812\n",
      "iter:  737  Current_loss:  110455.476562 best:  110469.632812\n",
      "iter:  738  Current_loss:  110442.171875 best:  110469.632812\n",
      "iter:  739  Current_loss:  110452.085938 best:  110442.171875\n",
      "iter:  740  Current_loss:  110442.476562 best:  110442.171875\n",
      "iter:  741  Current_loss:  110456.453125 best:  110442.171875\n",
      "iter:  742  Current_loss:  110467.515625 best:  110442.171875\n",
      "iter:  743  Current_loss:  110467.765625 best:  110442.171875\n",
      "iter:  744  Current_loss:  110466.148438 best:  110442.171875\n",
      "iter:  745  Current_loss:  110450.40625 best:  110442.171875\n",
      "iter:  746  Current_loss:  110446.257812 best:  110442.171875\n",
      "iter:  747  Current_loss:  110438.09375 best:  110442.171875\n",
      "iter:  748  Current_loss:  110407.132812 best:  110442.171875\n",
      "iter:  749  Current_loss:  110412.453125 best:  110407.132812\n",
      "iter:  750  Current_loss:  110398.6875 best:  110407.132812\n",
      "[750]\ttrain-rmse:81269.6\teval-rmse:110400\n",
      "iter:  751  Current_loss:  110399.96875 best:  110407.132812\n",
      "iter:  752  Current_loss:  110401.40625 best:  110407.132812\n",
      "iter:  753  Current_loss:  110398.867188 best:  110407.132812\n",
      "iter:  754  Current_loss:  110398.867188 best:  110407.132812\n",
      "iter:  755  Current_loss:  110422.75 best:  110407.132812\n",
      "iter:  756  Current_loss:  110413.664062 best:  110407.132812\n",
      "iter:  757  Current_loss:  110404.8125 best:  110407.132812\n",
      "iter:  758  Current_loss:  110404.765625 best:  110407.132812\n",
      "iter:  758 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  759  Current_loss:  110403.65625 best:  110407.132812\n",
      "iter:  760  Current_loss:  110374.320312 best:  110407.132812\n",
      "iter:  761  Current_loss:  110345.296875 best:  110374.320312\n",
      "iter:  762  Current_loss:  110355.914062 best:  110345.296875\n",
      "iter:  763  Current_loss:  110374.171875 best:  110345.296875\n",
      "iter:  764  Current_loss:  110379.46875 best:  110345.296875\n",
      "iter:  765  Current_loss:  110354.960938 best:  110345.296875\n",
      "iter:  766  Current_loss:  110355.335938 best:  110345.296875\n",
      "iter:  767  Current_loss:  110349.5 best:  110345.296875\n",
      "iter:  768  Current_loss:  110343.804688 best:  110345.296875\n",
      "iter:  769  Current_loss:  110341.203125 best:  110345.296875\n",
      "iter:  770  Current_loss:  110362.492188 best:  110345.296875\n",
      "iter:  771  Current_loss:  110335.765625 best:  110345.296875\n",
      "iter:  771 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  772  Current_loss:  110325.21875 best:  110345.296875\n",
      "iter:  773  Current_loss:  110301.398438 best:  110325.21875\n",
      "iter:  774  Current_loss:  110300.960938 best:  110301.398438\n",
      "iter:  775  Current_loss:  110297.265625 best:  110301.398438\n",
      "iter:  776  Current_loss:  110325.804688 best:  110301.398438\n",
      "iter:  777  Current_loss:  110323.257812 best:  110301.398438\n",
      "iter:  778  Current_loss:  110313.695312 best:  110301.398438\n",
      "iter:  779  Current_loss:  110354.398438 best:  110301.398438\n",
      "iter:  780  Current_loss:  110346.671875 best:  110301.398438\n",
      "iter:  781  Current_loss:  110348.007812 best:  110301.398438\n",
      "iter:  782  Current_loss:  110357.359375 best:  110301.398438\n",
      "iter:  783  Current_loss:  110394.570312 best:  110301.398438\n",
      "iter:  783 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  784  Current_loss:  110393.625 best:  110301.398438\n",
      "iter:  785  Current_loss:  110371.039062 best:  110301.398438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  786  Current_loss:  110355.625 best:  110301.398438\n",
      "iter:  787  Current_loss:  110352.8125 best:  110301.398438\n",
      "iter:  788  Current_loss:  110316.148438 best:  110301.398438\n",
      "iter:  789  Current_loss:  110294.203125 best:  110301.398438\n",
      "iter:  790  Current_loss:  110278.539062 best:  110301.398438\n",
      "iter:  791  Current_loss:  110283.8125 best:  110278.539062\n",
      "iter:  792  Current_loss:  110304.820312 best:  110278.539062\n",
      "iter:  793  Current_loss:  110308.3125 best:  110278.539062\n",
      "iter:  794  Current_loss:  110274.46875 best:  110278.539062\n",
      "iter:  795  Current_loss:  110240.515625 best:  110278.539062\n",
      "iter:  796  Current_loss:  110220.796875 best:  110240.515625\n",
      "iter:  797  Current_loss:  110227.460938 best:  110240.515625\n",
      "iter:  798  Current_loss:  110190.640625 best:  110240.515625\n",
      "iter:  799  Current_loss:  110183.859375 best:  110190.640625\n",
      "iter:  800  Current_loss:  110176.992188 best:  110190.640625\n",
      "[800]\ttrain-rmse:80117.3\teval-rmse:110147\n",
      "iter:  801  Current_loss:  110147.359375 best:  110190.640625\n",
      "iter:  802  Current_loss:  110147.320312 best:  110147.359375\n",
      "iter:  803  Current_loss:  110150.8125 best:  110147.359375\n",
      "iter:  804  Current_loss:  110138.023438 best:  110147.359375\n",
      "iter:  805  Current_loss:  110194.523438 best:  110147.359375\n",
      "iter:  806  Current_loss:  110203.304688 best:  110147.359375\n",
      "iter:  807  Current_loss:  110205.648438 best:  110147.359375\n",
      "iter:  808  Current_loss:  110225.320312 best:  110147.359375\n",
      "iter:  809  Current_loss:  110243.8125 best:  110147.359375\n",
      "iter:  810  Current_loss:  110229.28125 best:  110147.359375\n",
      "iter:  811  Current_loss:  110209.515625 best:  110147.359375\n",
      "iter:  811 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  812  Current_loss:  110210.046875 best:  110147.359375\n",
      "iter:  813  Current_loss:  110206.625 best:  110147.359375\n",
      "iter:  814  Current_loss:  110197.984375 best:  110147.359375\n",
      "iter:  815  Current_loss:  110196.070312 best:  110147.359375\n",
      "iter:  816  Current_loss:  110196.195312 best:  110147.359375\n",
      "iter:  817  Current_loss:  110199.015625 best:  110147.359375\n",
      "iter:  818  Current_loss:  110202.390625 best:  110147.359375\n",
      "iter:  819  Current_loss:  110207.195312 best:  110147.359375\n",
      "iter:  820  Current_loss:  110207.523438 best:  110147.359375\n",
      "iter:  821  Current_loss:  110190.132812 best:  110147.359375\n",
      "iter:  821 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  822  Current_loss:  110189.15625 best:  110147.359375\n",
      "iter:  823  Current_loss:  110182.875 best:  110147.359375\n",
      "iter:  824  Current_loss:  110185.5625 best:  110147.359375\n",
      "iter:  825  Current_loss:  110184.398438 best:  110147.359375\n",
      "iter:  826  Current_loss:  110173.09375 best:  110147.359375\n",
      "iter:  827  Current_loss:  110155.835938 best:  110147.359375\n",
      "iter:  828  Current_loss:  110167.898438 best:  110147.359375\n",
      "iter:  829  Current_loss:  110183.195312 best:  110147.359375\n",
      "iter:  830  Current_loss:  110184.789062 best:  110147.359375\n",
      "iter:  831  Current_loss:  110161.5 best:  110147.359375\n",
      "iter:  831 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  832  Current_loss:  110173.164062 best:  110147.359375\n",
      "iter:  833  Current_loss:  110194.234375 best:  110147.359375\n",
      "iter:  834  Current_loss:  110182.539062 best:  110147.359375\n",
      "iter:  835  Current_loss:  110192.039062 best:  110147.359375\n",
      "iter:  836  Current_loss:  110161.929688 best:  110147.359375\n",
      "iter:  837  Current_loss:  110147.835938 best:  110147.359375\n",
      "iter:  838  Current_loss:  110129.601562 best:  110147.359375\n",
      "iter:  839  Current_loss:  110129.046875 best:  110147.359375\n",
      "iter:  840  Current_loss:  110115.210938 best:  110147.359375\n",
      "iter:  841  Current_loss:  110113.710938 best:  110115.210938\n",
      "iter:  842  Current_loss:  110110.140625 best:  110115.210938\n",
      "iter:  843  Current_loss:  110109.984375 best:  110115.210938\n",
      "iter:  844  Current_loss:  110083.78125 best:  110115.210938\n",
      "iter:  845  Current_loss:  110079.09375 best:  110083.78125\n",
      "iter:  846  Current_loss:  110080.53125 best:  110083.78125\n",
      "iter:  847  Current_loss:  110112.40625 best:  110083.78125\n",
      "iter:  848  Current_loss:  110125.992188 best:  110083.78125\n",
      "iter:  849  Current_loss:  110123.210938 best:  110083.78125\n",
      "iter:  850  Current_loss:  110107.789062 best:  110083.78125\n",
      "[850]\ttrain-rmse:79188.2\teval-rmse:110128\n",
      "iter:  851  Current_loss:  110127.71875 best:  110083.78125\n",
      "iter:  852  Current_loss:  110108.570312 best:  110083.78125\n",
      "iter:  853  Current_loss:  110122.265625 best:  110083.78125\n",
      "iter:  854  Current_loss:  110132.757812 best:  110083.78125\n",
      "iter:  854 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  855  Current_loss:  110124.75 best:  110083.78125\n",
      "iter:  856  Current_loss:  110098.390625 best:  110083.78125\n",
      "iter:  857  Current_loss:  110118.398438 best:  110083.78125\n",
      "iter:  858  Current_loss:  110170.859375 best:  110083.78125\n",
      "iter:  859  Current_loss:  110159.03125 best:  110083.78125\n",
      "iter:  860  Current_loss:  110161.296875 best:  110083.78125\n",
      "iter:  861  Current_loss:  110157.796875 best:  110083.78125\n",
      "iter:  862  Current_loss:  110232.148438 best:  110083.78125\n",
      "iter:  863  Current_loss:  110224.070312 best:  110083.78125\n",
      "iter:  864  Current_loss:  110259.78125 best:  110083.78125\n",
      "iter:  864 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  865  Current_loss:  110259.5 best:  110083.78125\n",
      "iter:  866  Current_loss:  110251.476562 best:  110083.78125\n",
      "iter:  867  Current_loss:  110256.4375 best:  110083.78125\n",
      "iter:  868  Current_loss:  110251.1875 best:  110083.78125\n",
      "iter:  869  Current_loss:  110229.273438 best:  110083.78125\n",
      "iter:  870  Current_loss:  110227.09375 best:  110083.78125\n",
      "iter:  871  Current_loss:  110214.140625 best:  110083.78125\n",
      "iter:  872  Current_loss:  110211.0625 best:  110083.78125\n",
      "iter:  873  Current_loss:  110172.375 best:  110083.78125\n",
      "iter:  874  Current_loss:  110166.867188 best:  110083.78125\n",
      "iter:  874 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  875  Current_loss:  110168.867188 best:  110083.78125\n",
      "iter:  876  Current_loss:  110166.515625 best:  110083.78125\n",
      "iter:  877  Current_loss:  110139.820312 best:  110083.78125\n",
      "iter:  878  Current_loss:  110134.835938 best:  110083.78125\n",
      "iter:  879  Current_loss:  110112.5 best:  110083.78125\n",
      "iter:  880  Current_loss:  110112.929688 best:  110083.78125\n",
      "iter:  881  Current_loss:  110142.429688 best:  110083.78125\n",
      "iter:  882  Current_loss:  110125.96875 best:  110083.78125\n",
      "iter:  883  Current_loss:  110120.453125 best:  110083.78125\n",
      "iter:  884  Current_loss:  110115.015625 best:  110083.78125\n",
      "iter:  884 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  885  Current_loss:  110106.546875 best:  110083.78125\n",
      "iter:  886  Current_loss:  110111.851562 best:  110083.78125\n",
      "iter:  887  Current_loss:  110129.796875 best:  110083.78125\n",
      "iter:  888  Current_loss:  110114.375 best:  110083.78125\n",
      "iter:  889  Current_loss:  110086.375 best:  110083.78125\n",
      "iter:  890  Current_loss:  110075.515625 best:  110083.78125\n",
      "iter:  891  Current_loss:  110059.140625 best:  110083.78125\n",
      "iter:  892  Current_loss:  110045.8125 best:  110059.140625\n",
      "iter:  893  Current_loss:  110030.734375 best:  110059.140625\n",
      "iter:  894  Current_loss:  110037.546875 best:  110030.734375\n",
      "iter:  895  Current_loss:  110040.742188 best:  110030.734375\n",
      "iter:  896  Current_loss:  110032.40625 best:  110030.734375\n",
      "iter:  897  Current_loss:  110051.867188 best:  110030.734375\n",
      "iter:  898  Current_loss:  110077.804688 best:  110030.734375\n",
      "iter:  899  Current_loss:  110071.882812 best:  110030.734375\n",
      "iter:  900  Current_loss:  110073.523438 best:  110030.734375\n",
      "[900]\ttrain-rmse:78147.5\teval-rmse:110072\n",
      "iter:  901  Current_loss:  110071.921875 best:  110030.734375\n",
      "iter:  902  Current_loss:  110068.132812 best:  110030.734375\n",
      "iter:  903  Current_loss:  110060.0 best:  110030.734375\n",
      "iter:  903 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  904  Current_loss:  110064.1875 best:  110030.734375\n",
      "iter:  905  Current_loss:  110065.648438 best:  110030.734375\n",
      "iter:  906  Current_loss:  110064.351562 best:  110030.734375\n",
      "iter:  907  Current_loss:  110050.390625 best:  110030.734375\n",
      "iter:  908  Current_loss:  110040.828125 best:  110030.734375\n",
      "iter:  909  Current_loss:  110041.5625 best:  110030.734375\n",
      "iter:  910  Current_loss:  110058.0 best:  110030.734375\n",
      "iter:  911  Current_loss:  110066.335938 best:  110030.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  912  Current_loss:  110059.046875 best:  110030.734375\n",
      "iter:  913  Current_loss:  110052.710938 best:  110030.734375\n",
      "iter:  913 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  914  Current_loss:  110040.40625 best:  110030.734375\n",
      "iter:  915  Current_loss:  110013.148438 best:  110030.734375\n",
      "iter:  916  Current_loss:  110005.242188 best:  110030.734375\n",
      "iter:  917  Current_loss:  110007.429688 best:  110005.242188\n",
      "iter:  918  Current_loss:  110013.09375 best:  110005.242188\n",
      "iter:  919  Current_loss:  110025.25 best:  110005.242188\n",
      "iter:  920  Current_loss:  110026.695312 best:  110005.242188\n",
      "iter:  921  Current_loss:  109995.726562 best:  110005.242188\n",
      "iter:  922  Current_loss:  109994.5 best:  110005.242188\n",
      "iter:  923  Current_loss:  109978.554688 best:  110005.242188\n",
      "iter:  924  Current_loss:  109993.921875 best:  109978.554688\n",
      "iter:  925  Current_loss:  109974.421875 best:  109978.554688\n",
      "iter:  926  Current_loss:  109972.992188 best:  109978.554688\n",
      "iter:  927  Current_loss:  109975.523438 best:  109978.554688\n",
      "iter:  928  Current_loss:  109988.75 best:  109978.554688\n",
      "iter:  929  Current_loss:  109983.59375 best:  109978.554688\n",
      "iter:  930  Current_loss:  109976.882812 best:  109978.554688\n",
      "iter:  931  Current_loss:  109979.085938 best:  109978.554688\n",
      "iter:  932  Current_loss:  109979.710938 best:  109978.554688\n",
      "iter:  933  Current_loss:  109963.820312 best:  109978.554688\n",
      "iter:  933 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  934  Current_loss:  109978.164062 best:  109978.554688\n",
      "iter:  935  Current_loss:  109958.90625 best:  109978.554688\n",
      "iter:  936  Current_loss:  109944.90625 best:  109978.554688\n",
      "iter:  937  Current_loss:  109952.375 best:  109944.90625\n",
      "iter:  938  Current_loss:  109935.914062 best:  109944.90625\n",
      "iter:  939  Current_loss:  109904.867188 best:  109944.90625\n",
      "iter:  940  Current_loss:  109880.0 best:  109904.867188\n",
      "iter:  941  Current_loss:  109876.148438 best:  109880.0\n",
      "iter:  942  Current_loss:  109872.585938 best:  109880.0\n",
      "iter:  943  Current_loss:  109881.78125 best:  109880.0\n",
      "iter:  944  Current_loss:  109876.289062 best:  109880.0\n",
      "iter:  945  Current_loss:  109868.414062 best:  109880.0\n",
      "iter:  946  Current_loss:  109856.859375 best:  109880.0\n",
      "iter:  947  Current_loss:  109865.492188 best:  109856.859375\n",
      "iter:  948  Current_loss:  109861.085938 best:  109856.859375\n",
      "iter:  949  Current_loss:  109867.46875 best:  109856.859375\n",
      "iter:  950  Current_loss:  109872.757812 best:  109856.859375\n",
      "[950]\ttrain-rmse:77142.4\teval-rmse:109878\n",
      "iter:  951  Current_loss:  109877.5 best:  109856.859375\n",
      "iter:  952  Current_loss:  109865.90625 best:  109856.859375\n",
      "iter:  953  Current_loss:  109862.070312 best:  109856.859375\n",
      "iter:  954  Current_loss:  109853.3125 best:  109856.859375\n",
      "iter:  955  Current_loss:  109854.046875 best:  109856.859375\n",
      "iter:  956  Current_loss:  109847.8125 best:  109856.859375\n",
      "iter:  956 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  957  Current_loss:  109845.023438 best:  109856.859375\n",
      "iter:  958  Current_loss:  109840.703125 best:  109856.859375\n",
      "iter:  959  Current_loss:  109834.414062 best:  109856.859375\n",
      "iter:  960  Current_loss:  109839.179688 best:  109834.414062\n",
      "iter:  961  Current_loss:  109847.65625 best:  109834.414062\n",
      "iter:  962  Current_loss:  109846.773438 best:  109834.414062\n",
      "iter:  963  Current_loss:  109834.101562 best:  109834.414062\n",
      "iter:  964  Current_loss:  109835.203125 best:  109834.414062\n",
      "iter:  965  Current_loss:  109829.867188 best:  109834.414062\n",
      "iter:  966  Current_loss:  109830.734375 best:  109834.414062\n",
      "iter:  967  Current_loss:  109815.289062 best:  109834.414062\n",
      "iter:  968  Current_loss:  109836.898438 best:  109834.414062\n",
      "iter:  969  Current_loss:  109825.3125 best:  109834.414062\n",
      "iter:  969 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  970  Current_loss:  109821.945312 best:  109834.414062\n",
      "iter:  971  Current_loss:  109848.078125 best:  109834.414062\n",
      "iter:  972  Current_loss:  109834.851562 best:  109834.414062\n",
      "iter:  973  Current_loss:  109838.796875 best:  109834.414062\n",
      "iter:  974  Current_loss:  109833.78125 best:  109834.414062\n",
      "iter:  975  Current_loss:  109824.765625 best:  109834.414062\n",
      "iter:  976  Current_loss:  109811.59375 best:  109834.414062\n",
      "iter:  977  Current_loss:  109812.0625 best:  109811.59375\n",
      "iter:  978  Current_loss:  109803.5625 best:  109811.59375\n",
      "iter:  979  Current_loss:  109797.78125 best:  109811.59375\n",
      "iter:  980  Current_loss:  109793.765625 best:  109811.59375\n",
      "iter:  981  Current_loss:  109794.632812 best:  109811.59375\n",
      "iter:  982  Current_loss:  109790.882812 best:  109811.59375\n",
      "iter:  983  Current_loss:  109781.898438 best:  109790.882812\n",
      "iter:  984  Current_loss:  109769.101562 best:  109790.882812\n",
      "iter:  985  Current_loss:  109750.117188 best:  109769.101562\n",
      "iter:  986  Current_loss:  109747.398438 best:  109769.101562\n",
      "iter:  987  Current_loss:  109766.25 best:  109747.398438\n",
      "iter:  988  Current_loss:  109748.992188 best:  109747.398438\n",
      "iter:  989  Current_loss:  109748.78125 best:  109747.398438\n",
      "iter:  990  Current_loss:  109744.929688 best:  109747.398438\n",
      "iter:  991  Current_loss:  109745.9375 best:  109747.398438\n",
      "iter:  992  Current_loss:  109723.554688 best:  109747.398438\n",
      "iter:  993  Current_loss:  109723.234375 best:  109723.554688\n",
      "iter:  994  Current_loss:  109721.75 best:  109723.554688\n",
      "iter:  995  Current_loss:  109703.71875 best:  109723.554688\n",
      "iter:  996  Current_loss:  109695.023438 best:  109723.554688\n",
      "iter:  997  Current_loss:  109702.773438 best:  109695.023438\n",
      "iter:  998  Current_loss:  109704.570312 best:  109695.023438\n",
      "iter:  999  Current_loss:  109720.8125 best:  109695.023438\n",
      "[999]\ttrain-rmse:76249.1\teval-rmse:109730\n"
     ]
    }
   ],
   "source": [
    "model1 = run_gbm(dtrain, deval, params,progress, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBRegressor(n_estimators=num_epochs, learning_rate=0.1)\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=[\"error\", \"rmse\"], eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  995  min customEval: 109695.023438\n",
      "iter:  972  min Eval: 109787.789062\n"
     ]
    }
   ],
   "source": [
    "# retrieve performance metrics\n",
    "results = model.evals_result()\n",
    "#results1 = model1.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(50, epochs)\n",
    "\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(10, 10))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'][50::], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'][50::], label='Eval')\n",
    "ax.plot(x_axis, progress['train']['rmse'][50::], label = 'customTrain')\n",
    "ax.plot(x_axis, progress['eval']['rmse'][50::], label = 'customEval')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "\n",
    "pyplot.show()\n",
    "print(\"iter: \", progress['eval']['rmse'].index(min(progress['eval']['rmse'])), \" min customEval:\", min(progress['eval']['rmse']))\n",
    "print(\"iter: \", results['validation_1']['rmse'].index(min(results['validation_1']['rmse'])), \" min Eval:\", min(results['validation_1']['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classification error\n",
    "# fig, ax = pyplot.subplots()\n",
    "# ax.plot(x_axis, results['validation_0']['error'][25::], label='Train')\n",
    "# ax.plot(x_axis, results['validation_1']['error'][25::], label='Test')\n",
    "# ax.legend()\n",
    "# pyplot.ylabel('Classification Error')\n",
    "# pyplot.title('XGBoost Classification Error')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

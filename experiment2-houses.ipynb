{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agavrilenko/anaconda3/lib/python3.6/site-packages/xgboost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.dirname(xgb.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round: int\n",
      "        Number of boosting iterations.\n",
      "    evals: list of pairs (DMatrix, string)\n",
      "        List of items to be evaluated during training, this allows user to watch\n",
      "        performance on the validation set.\n",
      "    obj : function\n",
      "        Customized objective function.\n",
      "    feval : function\n",
      "        Customized evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Validation error needs to decrease at least\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        If there's more than one, will use the last.\n",
      "        Returns the model from the last iteration (not the best one).\n",
      "        If early stopping occurs, the model will have three additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "        (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "        ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "    evals_result: dict\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval : bool or int\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    learning_rates: list or function (deprecated - use callback API instead)\n",
      "        List of learning rate for each boosting round\n",
      "        or a customized function that calculates eta in terms of\n",
      "        current number of round and the total number of boosting round (e.g. yields\n",
      "        learning rate decay)\n",
      "    xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(xgboost.training.train)\n",
    "help(xgb.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df_usa = pd.read_csv('data/houses/housesalesprediction/kc_house_data.csv', delimiter=\",\")\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "\n",
    "df_usa.drop(['id', 'date'], axis=1, inplace=True)\n",
    "X = df_usa.drop(\"price\",axis=1).values\n",
    "y = df_usa[\"price\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=3)\n",
    "\n",
    "# split data into X and y\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# # split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data = X_train, label = y_train, silent= True)\n",
    "\n",
    "deval = xgb.DMatrix(data = X_test, label = y_test, silent= True)\n",
    "# evals_list = []\n",
    "# evals_list.append((deval, \"eval matrix\"))\n",
    "num_epochs = 600\n",
    "params = {}\n",
    "params['num_boost_round'] = num_epochs\n",
    "params['early_stopping_rounds'] = 1000\n",
    "#params['verbose_eval'] = 1\n",
    "params['eval_metric'] = 'rmse'\n",
    "params['evals_result'] = {}\n",
    "params['max_depth'] = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = []\n",
    "tresholds = []\n",
    "learning_rates.append(0.1)\n",
    "counter = 0\n",
    "best = []\n",
    "progress = dict()\n",
    "wait = []\n",
    "def reduceLRonPlateau(i,n):\n",
    "\n",
    "    factor = 0.5\n",
    "    min_lr = 1e-30\n",
    "    min_delta = 1000\n",
    "    patience = 10\n",
    "    verbose = 0\n",
    "    cooldown = 3\n",
    "    cooldown_counter = 0  # Cooldown counter.\n",
    "    \n",
    "    #best = 0\n",
    "    mode = 'min'  \n",
    "    monitor_op = None\n",
    "    old_lr = learning_rates[0]\n",
    "    new_lr = learning_rates[0] \n",
    "    \n",
    "    def in_cooldown():\n",
    "            return cooldown_counter > 0\n",
    "    \n",
    "    def _reset(mode):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\"\"\"\n",
    "#         if mode not in ['auto', 'min', 'max']:\n",
    "#             warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n",
    "#                           'fallback to auto mode.' % (mode),\n",
    "#                           RuntimeWarning)\n",
    "#             mode = 'auto'\n",
    "#         if (mode == 'min' or\n",
    "#            (mode == 'auto' and 'acc' not in monitor)):\n",
    "#             monitor_op = lambda a, b: np.less(a, b - min_delta)\n",
    "#             best = np.Inf\n",
    "#         else:\n",
    "#             monitor_op = lambda a, b: np.greater(a, b + min_delta)\n",
    "#             best = -np.Inf\n",
    "            \n",
    "        cooldown_counter = 0\n",
    "        del wait[:]\n",
    "        return wait, cooldown_counter\n",
    "    \n",
    "    def checker(wait,best, new_lr, learning_rates):\n",
    "        if (i == 0):\n",
    "            wait, counter = _reset(mode)\n",
    "            print(\"initialization...\")\n",
    "            print(\"lr: \", learning_rates[0])\n",
    "            best.append(100000000)\n",
    "            \n",
    "        if (i>0):\n",
    "            old_lr = learning_rates[i-1]\n",
    "            #new_lr = learning_rates[i-1]\n",
    "            current = progress['eval']['rmse'][i-1]\n",
    "            if in_cooldown():\n",
    "                cooldown_counter -= 1\n",
    "                del wait[:]\n",
    "              \n",
    "            print(\"iter: \", i, \" Current_loss: \", current, \"best: \", best[i-1])\n",
    "            if np.less(current, best[i-1] - min_delta):\n",
    "                best.append(current)\n",
    "                del wait[:]\n",
    "             \n",
    "            elif not in_cooldown():\n",
    "                #rint(\"not in_cooldown\")\n",
    "                best.append(best[-1])\n",
    "                wait.append(1)\n",
    "                if len(wait) >= patience:\n",
    "                    \n",
    "                    if old_lr > min_lr:\n",
    "                        new_lr = old_lr * factor\n",
    "                        new_lr = max(new_lr, min_lr)\n",
    "                        print(\"iter: \", i, \"reducing lr -- \", \" old_lr: \", old_lr, \" new_lr: \", new_lr)\n",
    "                        \n",
    "#                         if self.verbose > 0:\n",
    "#                             print('\\nEpoch %05d: ReduceLROnPlateau reducing '\n",
    "#                                   'learning rate to %s.' % (epoch + 1, new_lr))\n",
    "                        cooldown_counter = cooldown\n",
    "                        del wait[:]\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "       # print(\"step \",progress['train']['logloss'][i-1], \"i = \", i)\n",
    "\n",
    "            \n",
    "    \n",
    "    new_lr = checker(wait,best,new_lr,learning_rates)\n",
    "    learning_rates.append(new_lr)\n",
    "        \n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_gbm(dtrain, dvalid, param, evals_result, learning_rate):\n",
    "    # check training arguments in param\n",
    "    n_round = param.get('num_boost_round', 100)\n",
    "    early_stop = param.get('early_stopping_rounds', 0)\n",
    "    verbose_eval = param.get('verbose_eval', 50)\n",
    "    # specify validations set to watch performance\n",
    "    watchlist = [(dtrain,'train') ,(deval,'eval')]\n",
    "    #callbacks_list = [learning_rates]\n",
    "\n",
    "    bst = xgb.train(params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=n_round,\n",
    "            evals=watchlist,\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval = verbose_eval,\n",
    "            evals_result = evals_result, \n",
    "            callbacks = [xgb.callback.reset_learning_rate(reduceLRonPlateau)])\n",
    "                   \n",
    "    return bst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization...\n",
      "lr:  0.1\n",
      "[0]\ttrain-rmse:597658\teval-rmse:590266\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 1000 rounds.\n",
      "iter:  1  Current_loss:  590265.5 best:  100000000\n",
      "iter:  2  Current_loss:  539130.9375 best:  590265.5\n",
      "iter:  3  Current_loss:  493740.15625 best:  539130.9375\n",
      "iter:  4  Current_loss:  453324.34375 best:  493740.15625\n",
      "iter:  5  Current_loss:  417079.59375 best:  453324.34375\n",
      "iter:  6  Current_loss:  385368.84375 best:  417079.59375\n",
      "iter:  7  Current_loss:  357464.09375 best:  385368.84375\n",
      "iter:  8  Current_loss:  332269.125 best:  357464.09375\n",
      "iter:  9  Current_loss:  310484.65625 best:  332269.125\n",
      "iter:  10  Current_loss:  290920.0 best:  310484.65625\n",
      "iter:  11  Current_loss:  273306.21875 best:  290920.0\n",
      "iter:  12  Current_loss:  258614.453125 best:  273306.21875\n",
      "iter:  13  Current_loss:  245829.46875 best:  258614.453125\n",
      "iter:  14  Current_loss:  233888.078125 best:  245829.46875\n",
      "iter:  15  Current_loss:  222381.3125 best:  233888.078125\n",
      "iter:  16  Current_loss:  212981.96875 best:  222381.3125\n",
      "iter:  17  Current_loss:  205294.609375 best:  212981.96875\n",
      "iter:  18  Current_loss:  198489.578125 best:  205294.609375\n",
      "iter:  19  Current_loss:  191642.0625 best:  198489.578125\n",
      "iter:  20  Current_loss:  186847.484375 best:  191642.0625\n",
      "iter:  21  Current_loss:  182536.53125 best:  191642.0625\n",
      "iter:  22  Current_loss:  178683.59375 best:  182536.53125\n",
      "iter:  23  Current_loss:  175017.453125 best:  182536.53125\n",
      "iter:  24  Current_loss:  171210.4375 best:  175017.453125\n",
      "iter:  25  Current_loss:  168738.21875 best:  175017.453125\n",
      "iter:  26  Current_loss:  165576.28125 best:  168738.21875\n",
      "iter:  27  Current_loss:  163297.0 best:  168738.21875\n",
      "iter:  28  Current_loss:  161055.546875 best:  163297.0\n",
      "iter:  29  Current_loss:  159055.078125 best:  163297.0\n",
      "iter:  30  Current_loss:  157250.828125 best:  163297.0\n",
      "iter:  31  Current_loss:  155680.75 best:  157250.828125\n",
      "iter:  32  Current_loss:  154154.921875 best:  157250.828125\n",
      "iter:  33  Current_loss:  152804.734375 best:  157250.828125\n",
      "iter:  34  Current_loss:  151355.109375 best:  157250.828125\n",
      "iter:  35  Current_loss:  150350.78125 best:  151355.109375\n",
      "iter:  36  Current_loss:  149572.34375 best:  151355.109375\n",
      "iter:  37  Current_loss:  148521.203125 best:  151355.109375\n",
      "iter:  38  Current_loss:  147606.015625 best:  151355.109375\n",
      "iter:  39  Current_loss:  146937.578125 best:  151355.109375\n",
      "iter:  40  Current_loss:  145837.609375 best:  151355.109375\n",
      "iter:  41  Current_loss:  144997.125 best:  145837.609375\n",
      "iter:  42  Current_loss:  143939.5 best:  145837.609375\n",
      "iter:  43  Current_loss:  143286.421875 best:  145837.609375\n",
      "iter:  44  Current_loss:  141782.203125 best:  145837.609375\n",
      "iter:  45  Current_loss:  140610.578125 best:  145837.609375\n",
      "iter:  46  Current_loss:  140050.09375 best:  140610.578125\n",
      "iter:  47  Current_loss:  139330.984375 best:  140610.578125\n",
      "iter:  48  Current_loss:  138988.71875 best:  140610.578125\n",
      "iter:  49  Current_loss:  138296.78125 best:  140610.578125\n",
      "iter:  50  Current_loss:  137717.46875 best:  140610.578125\n",
      "[50]\ttrain-rmse:131990\teval-rmse:136758\n",
      "iter:  51  Current_loss:  136757.765625 best:  140610.578125\n",
      "iter:  52  Current_loss:  136271.234375 best:  140610.578125\n",
      "iter:  53  Current_loss:  135987.40625 best:  140610.578125\n",
      "iter:  54  Current_loss:  135562.40625 best:  140610.578125\n",
      "iter:  55  Current_loss:  134720.0 best:  135562.40625\n",
      "iter:  56  Current_loss:  134227.1875 best:  135562.40625\n",
      "iter:  57  Current_loss:  133810.171875 best:  135562.40625\n",
      "iter:  58  Current_loss:  133503.6875 best:  135562.40625\n",
      "iter:  59  Current_loss:  132845.625 best:  135562.40625\n",
      "iter:  60  Current_loss:  132548.171875 best:  135562.40625\n",
      "iter:  61  Current_loss:  132390.65625 best:  135562.40625\n",
      "iter:  62  Current_loss:  131861.21875 best:  135562.40625\n",
      "iter:  63  Current_loss:  131589.96875 best:  135562.40625\n",
      "iter:  64  Current_loss:  131402.6875 best:  135562.40625\n",
      "iter:  64 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  65  Current_loss:  131267.21875 best:  135562.40625\n",
      "iter:  66  Current_loss:  130758.421875 best:  135562.40625\n",
      "iter:  67  Current_loss:  130568.195312 best:  135562.40625\n",
      "iter:  68  Current_loss:  130520.960938 best:  135562.40625\n",
      "iter:  69  Current_loss:  130287.828125 best:  130520.960938\n",
      "iter:  70  Current_loss:  129954.40625 best:  130520.960938\n",
      "iter:  71  Current_loss:  129816.8125 best:  130520.960938\n",
      "iter:  72  Current_loss:  129630.710938 best:  130520.960938\n",
      "iter:  73  Current_loss:  129297.648438 best:  130520.960938\n",
      "iter:  74  Current_loss:  129100.640625 best:  130520.960938\n",
      "iter:  75  Current_loss:  128997.382812 best:  130520.960938\n",
      "iter:  76  Current_loss:  128637.992188 best:  130520.960938\n",
      "iter:  77  Current_loss:  128563.484375 best:  130520.960938\n",
      "iter:  78  Current_loss:  128268.273438 best:  130520.960938\n",
      "iter:  78 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  79  Current_loss:  128183.351562 best:  130520.960938\n",
      "iter:  80  Current_loss:  128027.101562 best:  130520.960938\n",
      "iter:  81  Current_loss:  127952.210938 best:  130520.960938\n",
      "iter:  82  Current_loss:  127750.265625 best:  130520.960938\n",
      "iter:  83  Current_loss:  127547.53125 best:  130520.960938\n",
      "iter:  84  Current_loss:  127323.335938 best:  130520.960938\n",
      "iter:  85  Current_loss:  127232.015625 best:  130520.960938\n",
      "iter:  86  Current_loss:  127076.3125 best:  130520.960938\n",
      "iter:  87  Current_loss:  126749.046875 best:  130520.960938\n",
      "iter:  88  Current_loss:  126691.351562 best:  130520.960938\n",
      "iter:  88 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  89  Current_loss:  126642.445312 best:  130520.960938\n",
      "iter:  90  Current_loss:  126473.507812 best:  130520.960938\n",
      "iter:  91  Current_loss:  126495.078125 best:  130520.960938\n",
      "iter:  92  Current_loss:  126308.640625 best:  130520.960938\n",
      "iter:  93  Current_loss:  126181.0 best:  130520.960938\n",
      "iter:  94  Current_loss:  125967.484375 best:  130520.960938\n",
      "iter:  95  Current_loss:  125516.804688 best:  130520.960938\n",
      "iter:  96  Current_loss:  125396.523438 best:  125516.804688\n",
      "iter:  97  Current_loss:  125221.617188 best:  125516.804688\n",
      "iter:  98  Current_loss:  125099.90625 best:  125516.804688\n",
      "iter:  99  Current_loss:  124990.726562 best:  125516.804688\n",
      "iter:  100  Current_loss:  124855.320312 best:  125516.804688\n",
      "[100]\ttrain-rmse:117325\teval-rmse:124831\n",
      "iter:  101  Current_loss:  124830.578125 best:  125516.804688\n",
      "iter:  102  Current_loss:  124527.570312 best:  125516.804688\n",
      "iter:  103  Current_loss:  124369.195312 best:  125516.804688\n",
      "iter:  104  Current_loss:  124246.507812 best:  125516.804688\n",
      "iter:  105  Current_loss:  124181.976562 best:  125516.804688\n",
      "iter:  105 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  106  Current_loss:  124139.898438 best:  125516.804688\n",
      "iter:  107  Current_loss:  124034.609375 best:  125516.804688\n",
      "iter:  108  Current_loss:  123946.78125 best:  125516.804688\n",
      "iter:  109  Current_loss:  123734.9375 best:  125516.804688\n",
      "iter:  110  Current_loss:  123709.773438 best:  125516.804688\n",
      "iter:  111  Current_loss:  123652.28125 best:  125516.804688\n",
      "iter:  112  Current_loss:  123293.34375 best:  125516.804688\n",
      "iter:  113  Current_loss:  123229.617188 best:  125516.804688\n",
      "iter:  114  Current_loss:  123126.226562 best:  125516.804688\n",
      "iter:  115  Current_loss:  123183.695312 best:  125516.804688\n",
      "iter:  115 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  116  Current_loss:  123152.875 best:  125516.804688\n",
      "iter:  117  Current_loss:  123050.765625 best:  125516.804688\n",
      "iter:  118  Current_loss:  123002.578125 best:  125516.804688\n",
      "iter:  119  Current_loss:  122987.84375 best:  125516.804688\n",
      "iter:  120  Current_loss:  122912.773438 best:  125516.804688\n",
      "iter:  121  Current_loss:  122741.96875 best:  125516.804688\n",
      "iter:  122  Current_loss:  122629.34375 best:  125516.804688\n",
      "iter:  123  Current_loss:  122548.835938 best:  125516.804688\n",
      "iter:  124  Current_loss:  122533.609375 best:  125516.804688\n",
      "iter:  125  Current_loss:  122420.242188 best:  125516.804688\n",
      "iter:  125 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  126  Current_loss:  122345.890625 best:  125516.804688\n",
      "iter:  127  Current_loss:  122411.007812 best:  125516.804688\n",
      "iter:  128  Current_loss:  122259.578125 best:  125516.804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  129  Current_loss:  122188.664062 best:  125516.804688\n",
      "iter:  130  Current_loss:  122062.96875 best:  125516.804688\n",
      "iter:  131  Current_loss:  121801.53125 best:  125516.804688\n",
      "iter:  132  Current_loss:  121785.734375 best:  125516.804688\n",
      "iter:  133  Current_loss:  121844.1875 best:  125516.804688\n",
      "iter:  134  Current_loss:  121782.195312 best:  125516.804688\n",
      "iter:  135  Current_loss:  121703.859375 best:  125516.804688\n",
      "iter:  135 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  136  Current_loss:  121693.5 best:  125516.804688\n",
      "iter:  137  Current_loss:  121655.804688 best:  125516.804688\n",
      "iter:  138  Current_loss:  121549.171875 best:  125516.804688\n",
      "iter:  139  Current_loss:  121345.851562 best:  125516.804688\n",
      "iter:  140  Current_loss:  121284.0625 best:  125516.804688\n",
      "iter:  141  Current_loss:  121308.84375 best:  125516.804688\n",
      "iter:  142  Current_loss:  121341.820312 best:  125516.804688\n",
      "iter:  143  Current_loss:  121200.296875 best:  125516.804688\n",
      "iter:  144  Current_loss:  121091.734375 best:  125516.804688\n",
      "iter:  145  Current_loss:  121036.828125 best:  125516.804688\n",
      "iter:  145 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  146  Current_loss:  121046.304688 best:  125516.804688\n",
      "iter:  147  Current_loss:  121022.90625 best:  125516.804688\n",
      "iter:  148  Current_loss:  120963.5625 best:  125516.804688\n",
      "iter:  149  Current_loss:  120867.546875 best:  125516.804688\n",
      "iter:  150  Current_loss:  120843.320312 best:  125516.804688\n",
      "[150]\ttrain-rmse:110110\teval-rmse:120813\n",
      "iter:  151  Current_loss:  120812.734375 best:  125516.804688\n",
      "iter:  152  Current_loss:  120719.210938 best:  125516.804688\n",
      "iter:  153  Current_loss:  120675.398438 best:  125516.804688\n",
      "iter:  154  Current_loss:  120669.179688 best:  125516.804688\n",
      "iter:  155  Current_loss:  120603.851562 best:  125516.804688\n",
      "iter:  155 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  156  Current_loss:  120584.085938 best:  125516.804688\n",
      "iter:  157  Current_loss:  120571.5625 best:  125516.804688\n",
      "iter:  158  Current_loss:  120505.195312 best:  125516.804688\n",
      "iter:  159  Current_loss:  120406.6875 best:  120505.195312\n",
      "iter:  160  Current_loss:  120428.125 best:  120505.195312\n",
      "iter:  161  Current_loss:  120410.867188 best:  120505.195312\n",
      "iter:  162  Current_loss:  120355.21875 best:  120505.195312\n",
      "iter:  163  Current_loss:  120318.6875 best:  120505.195312\n",
      "iter:  164  Current_loss:  120233.273438 best:  120505.195312\n",
      "iter:  165  Current_loss:  120206.804688 best:  120505.195312\n",
      "iter:  166  Current_loss:  120118.257812 best:  120505.195312\n",
      "iter:  167  Current_loss:  120050.507812 best:  120505.195312\n",
      "iter:  168  Current_loss:  120031.414062 best:  120505.195312\n",
      "iter:  168 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  169  Current_loss:  120021.546875 best:  120505.195312\n",
      "iter:  170  Current_loss:  119958.96875 best:  120505.195312\n",
      "iter:  171  Current_loss:  119884.695312 best:  120505.195312\n",
      "iter:  172  Current_loss:  119814.0625 best:  120505.195312\n",
      "iter:  173  Current_loss:  119714.15625 best:  120505.195312\n",
      "iter:  174  Current_loss:  119718.5 best:  120505.195312\n",
      "iter:  175  Current_loss:  119560.28125 best:  120505.195312\n",
      "iter:  176  Current_loss:  119604.546875 best:  120505.195312\n",
      "iter:  177  Current_loss:  119570.882812 best:  120505.195312\n",
      "iter:  178  Current_loss:  119467.296875 best:  120505.195312\n",
      "iter:  178 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  179  Current_loss:  119322.695312 best:  120505.195312\n",
      "iter:  180  Current_loss:  119249.085938 best:  120505.195312\n",
      "iter:  181  Current_loss:  119117.164062 best:  120505.195312\n",
      "iter:  182  Current_loss:  119004.234375 best:  120505.195312\n",
      "iter:  183  Current_loss:  118915.234375 best:  120505.195312\n",
      "iter:  184  Current_loss:  118887.585938 best:  120505.195312\n",
      "iter:  185  Current_loss:  119009.039062 best:  120505.195312\n",
      "iter:  186  Current_loss:  118893.460938 best:  120505.195312\n",
      "iter:  187  Current_loss:  118849.890625 best:  120505.195312\n",
      "iter:  188  Current_loss:  118789.15625 best:  120505.195312\n",
      "iter:  188 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  189  Current_loss:  118777.890625 best:  120505.195312\n",
      "iter:  190  Current_loss:  118714.875 best:  120505.195312\n",
      "iter:  191  Current_loss:  118752.429688 best:  120505.195312\n",
      "iter:  192  Current_loss:  118708.75 best:  120505.195312\n",
      "iter:  193  Current_loss:  118777.164062 best:  120505.195312\n",
      "iter:  194  Current_loss:  118734.46875 best:  120505.195312\n",
      "iter:  195  Current_loss:  118672.6875 best:  120505.195312\n",
      "iter:  196  Current_loss:  118617.695312 best:  120505.195312\n",
      "iter:  197  Current_loss:  118529.234375 best:  120505.195312\n",
      "iter:  198  Current_loss:  118468.25 best:  120505.195312\n",
      "iter:  198 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  199  Current_loss:  118429.726562 best:  120505.195312\n",
      "iter:  200  Current_loss:  118441.015625 best:  120505.195312\n",
      "[200]\ttrain-rmse:105530\teval-rmse:118430\n",
      "iter:  201  Current_loss:  118430.125 best:  120505.195312\n",
      "iter:  202  Current_loss:  118417.59375 best:  120505.195312\n",
      "iter:  203  Current_loss:  118336.09375 best:  120505.195312\n",
      "iter:  204  Current_loss:  118307.054688 best:  120505.195312\n",
      "iter:  205  Current_loss:  118245.367188 best:  120505.195312\n",
      "iter:  206  Current_loss:  118196.9375 best:  120505.195312\n",
      "iter:  207  Current_loss:  118173.15625 best:  120505.195312\n",
      "iter:  208  Current_loss:  118110.632812 best:  120505.195312\n",
      "iter:  208 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  209  Current_loss:  118061.25 best:  120505.195312\n",
      "iter:  210  Current_loss:  118100.671875 best:  120505.195312\n",
      "iter:  211  Current_loss:  118037.007812 best:  120505.195312\n",
      "iter:  212  Current_loss:  117994.484375 best:  120505.195312\n",
      "iter:  213  Current_loss:  117951.734375 best:  120505.195312\n",
      "iter:  214  Current_loss:  117928.640625 best:  120505.195312\n",
      "iter:  215  Current_loss:  117776.71875 best:  120505.195312\n",
      "iter:  216  Current_loss:  117772.304688 best:  120505.195312\n",
      "iter:  217  Current_loss:  117744.523438 best:  120505.195312\n",
      "iter:  218  Current_loss:  117698.078125 best:  120505.195312\n",
      "iter:  218 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  219  Current_loss:  117715.984375 best:  120505.195312\n",
      "iter:  220  Current_loss:  117709.617188 best:  120505.195312\n",
      "iter:  221  Current_loss:  117702.546875 best:  120505.195312\n",
      "iter:  222  Current_loss:  117757.773438 best:  120505.195312\n",
      "iter:  223  Current_loss:  117546.789062 best:  120505.195312\n",
      "iter:  224  Current_loss:  117572.953125 best:  120505.195312\n",
      "iter:  225  Current_loss:  117602.078125 best:  120505.195312\n",
      "iter:  226  Current_loss:  117582.804688 best:  120505.195312\n",
      "iter:  227  Current_loss:  117563.882812 best:  120505.195312\n",
      "iter:  228  Current_loss:  117559.359375 best:  120505.195312\n",
      "iter:  228 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  229  Current_loss:  117543.648438 best:  120505.195312\n",
      "iter:  230  Current_loss:  117522.367188 best:  120505.195312\n",
      "iter:  231  Current_loss:  117503.796875 best:  120505.195312\n",
      "iter:  232  Current_loss:  117491.820312 best:  120505.195312\n",
      "iter:  233  Current_loss:  117442.757812 best:  120505.195312\n",
      "iter:  234  Current_loss:  117463.351562 best:  120505.195312\n",
      "iter:  235  Current_loss:  117350.929688 best:  120505.195312\n",
      "iter:  236  Current_loss:  117298.8125 best:  120505.195312\n",
      "iter:  237  Current_loss:  117231.835938 best:  120505.195312\n",
      "iter:  238  Current_loss:  117210.109375 best:  120505.195312\n",
      "iter:  238 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  239  Current_loss:  117210.601562 best:  120505.195312\n",
      "iter:  240  Current_loss:  117200.6875 best:  120505.195312\n",
      "iter:  241  Current_loss:  117178.296875 best:  120505.195312\n",
      "iter:  242  Current_loss:  117186.117188 best:  120505.195312\n",
      "iter:  243  Current_loss:  117082.40625 best:  120505.195312\n",
      "iter:  244  Current_loss:  117061.40625 best:  120505.195312\n",
      "iter:  245  Current_loss:  117054.367188 best:  120505.195312\n",
      "iter:  246  Current_loss:  116905.875 best:  120505.195312\n",
      "iter:  247  Current_loss:  116914.984375 best:  120505.195312\n",
      "iter:  248  Current_loss:  116851.726562 best:  120505.195312\n",
      "iter:  248 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  249  Current_loss:  116830.03125 best:  120505.195312\n",
      "iter:  250  Current_loss:  116818.609375 best:  120505.195312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttrain-rmse:101218\teval-rmse:116882\n",
      "iter:  251  Current_loss:  116882.125 best:  120505.195312\n",
      "iter:  252  Current_loss:  116839.601562 best:  120505.195312\n",
      "iter:  253  Current_loss:  116743.695312 best:  120505.195312\n",
      "iter:  254  Current_loss:  116725.875 best:  120505.195312\n",
      "iter:  255  Current_loss:  116657.882812 best:  120505.195312\n",
      "iter:  256  Current_loss:  116585.234375 best:  120505.195312\n",
      "iter:  257  Current_loss:  116570.09375 best:  120505.195312\n",
      "iter:  258  Current_loss:  116543.007812 best:  120505.195312\n",
      "iter:  258 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  259  Current_loss:  116552.382812 best:  120505.195312\n",
      "iter:  260  Current_loss:  116534.34375 best:  120505.195312\n",
      "iter:  261  Current_loss:  116508.320312 best:  120505.195312\n",
      "iter:  262  Current_loss:  116516.289062 best:  120505.195312\n",
      "iter:  263  Current_loss:  116492.796875 best:  120505.195312\n",
      "iter:  264  Current_loss:  116472.625 best:  120505.195312\n",
      "iter:  265  Current_loss:  116459.546875 best:  120505.195312\n",
      "iter:  266  Current_loss:  116453.257812 best:  120505.195312\n",
      "iter:  267  Current_loss:  116460.3125 best:  120505.195312\n",
      "iter:  268  Current_loss:  116379.601562 best:  120505.195312\n",
      "iter:  268 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  269  Current_loss:  116382.601562 best:  120505.195312\n",
      "iter:  270  Current_loss:  116370.507812 best:  120505.195312\n",
      "iter:  271  Current_loss:  116273.484375 best:  120505.195312\n",
      "iter:  272  Current_loss:  116219.796875 best:  120505.195312\n",
      "iter:  273  Current_loss:  116196.90625 best:  120505.195312\n",
      "iter:  274  Current_loss:  116211.257812 best:  120505.195312\n",
      "iter:  275  Current_loss:  116168.09375 best:  120505.195312\n",
      "iter:  276  Current_loss:  116076.703125 best:  120505.195312\n",
      "iter:  277  Current_loss:  116068.890625 best:  120505.195312\n",
      "iter:  278  Current_loss:  116080.179688 best:  120505.195312\n",
      "iter:  278 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  279  Current_loss:  116057.882812 best:  120505.195312\n",
      "iter:  280  Current_loss:  116029.398438 best:  120505.195312\n",
      "iter:  281  Current_loss:  116012.726562 best:  120505.195312\n",
      "iter:  282  Current_loss:  115939.929688 best:  120505.195312\n",
      "iter:  283  Current_loss:  115905.046875 best:  120505.195312\n",
      "iter:  284  Current_loss:  115866.015625 best:  120505.195312\n",
      "iter:  285  Current_loss:  115829.84375 best:  120505.195312\n",
      "iter:  286  Current_loss:  115826.0625 best:  120505.195312\n",
      "iter:  287  Current_loss:  115778.351562 best:  120505.195312\n",
      "iter:  288  Current_loss:  115770.015625 best:  120505.195312\n",
      "iter:  288 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  289  Current_loss:  115730.226562 best:  120505.195312\n",
      "iter:  290  Current_loss:  115742.039062 best:  120505.195312\n",
      "iter:  291  Current_loss:  115680.992188 best:  120505.195312\n",
      "iter:  292  Current_loss:  115605.554688 best:  120505.195312\n",
      "iter:  293  Current_loss:  115546.148438 best:  120505.195312\n",
      "iter:  294  Current_loss:  115491.625 best:  120505.195312\n",
      "iter:  295  Current_loss:  115458.710938 best:  115491.625\n",
      "iter:  296  Current_loss:  115439.0625 best:  115491.625\n",
      "iter:  297  Current_loss:  115417.28125 best:  115491.625\n",
      "iter:  298  Current_loss:  115405.867188 best:  115491.625\n",
      "iter:  299  Current_loss:  115354.90625 best:  115491.625\n",
      "iter:  300  Current_loss:  115324.664062 best:  115491.625\n",
      "[300]\ttrain-rmse:97859.9\teval-rmse:115300\n",
      "iter:  301  Current_loss:  115300.171875 best:  115491.625\n",
      "iter:  302  Current_loss:  115241.132812 best:  115491.625\n",
      "iter:  303  Current_loss:  115226.625 best:  115491.625\n",
      "iter:  304  Current_loss:  115225.609375 best:  115491.625\n",
      "iter:  304 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  305  Current_loss:  115234.257812 best:  115491.625\n",
      "iter:  306  Current_loss:  115205.78125 best:  115491.625\n",
      "iter:  307  Current_loss:  115128.90625 best:  115491.625\n",
      "iter:  308  Current_loss:  115106.90625 best:  115491.625\n",
      "iter:  309  Current_loss:  115091.898438 best:  115491.625\n",
      "iter:  310  Current_loss:  115078.28125 best:  115491.625\n",
      "iter:  311  Current_loss:  115056.171875 best:  115491.625\n",
      "iter:  312  Current_loss:  115010.773438 best:  115491.625\n",
      "iter:  313  Current_loss:  114977.21875 best:  115491.625\n",
      "iter:  314  Current_loss:  114990.984375 best:  115491.625\n",
      "iter:  314 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  315  Current_loss:  114991.757812 best:  115491.625\n",
      "iter:  316  Current_loss:  114954.257812 best:  115491.625\n",
      "iter:  317  Current_loss:  114894.375 best:  115491.625\n",
      "iter:  318  Current_loss:  114859.570312 best:  115491.625\n",
      "iter:  319  Current_loss:  114831.796875 best:  115491.625\n",
      "iter:  320  Current_loss:  114839.242188 best:  115491.625\n",
      "iter:  321  Current_loss:  114837.59375 best:  115491.625\n",
      "iter:  322  Current_loss:  114865.523438 best:  115491.625\n",
      "iter:  323  Current_loss:  114858.789062 best:  115491.625\n",
      "iter:  324  Current_loss:  114884.445312 best:  115491.625\n",
      "iter:  324 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  325  Current_loss:  114870.671875 best:  115491.625\n",
      "iter:  326  Current_loss:  114874.757812 best:  115491.625\n",
      "iter:  327  Current_loss:  114878.804688 best:  115491.625\n",
      "iter:  328  Current_loss:  114873.492188 best:  115491.625\n",
      "iter:  329  Current_loss:  114881.070312 best:  115491.625\n",
      "iter:  330  Current_loss:  114847.34375 best:  115491.625\n",
      "iter:  331  Current_loss:  114846.515625 best:  115491.625\n",
      "iter:  332  Current_loss:  114869.5625 best:  115491.625\n",
      "iter:  333  Current_loss:  114856.679688 best:  115491.625\n",
      "iter:  334  Current_loss:  114830.445312 best:  115491.625\n",
      "iter:  334 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  335  Current_loss:  114796.171875 best:  115491.625\n",
      "iter:  336  Current_loss:  114752.453125 best:  115491.625\n",
      "iter:  337  Current_loss:  114736.78125 best:  115491.625\n",
      "iter:  338  Current_loss:  114702.296875 best:  115491.625\n",
      "iter:  339  Current_loss:  114709.046875 best:  115491.625\n",
      "iter:  340  Current_loss:  114726.71875 best:  115491.625\n",
      "iter:  341  Current_loss:  114697.414062 best:  115491.625\n",
      "iter:  342  Current_loss:  114700.570312 best:  115491.625\n",
      "iter:  343  Current_loss:  114686.578125 best:  115491.625\n",
      "iter:  344  Current_loss:  114666.953125 best:  115491.625\n",
      "iter:  344 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  345  Current_loss:  114634.929688 best:  115491.625\n",
      "iter:  346  Current_loss:  114591.398438 best:  115491.625\n",
      "iter:  347  Current_loss:  114568.84375 best:  115491.625\n",
      "iter:  348  Current_loss:  114559.414062 best:  115491.625\n",
      "iter:  349  Current_loss:  114567.25 best:  115491.625\n",
      "iter:  350  Current_loss:  114517.578125 best:  115491.625\n",
      "[350]\ttrain-rmse:95396.3\teval-rmse:114536\n",
      "iter:  351  Current_loss:  114535.570312 best:  115491.625\n",
      "iter:  352  Current_loss:  114526.039062 best:  115491.625\n",
      "iter:  353  Current_loss:  114507.015625 best:  115491.625\n",
      "iter:  354  Current_loss:  114485.75 best:  115491.625\n",
      "iter:  354 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  355  Current_loss:  114469.1875 best:  115491.625\n",
      "iter:  356  Current_loss:  114405.453125 best:  115491.625\n",
      "iter:  357  Current_loss:  114373.726562 best:  115491.625\n",
      "iter:  358  Current_loss:  114356.179688 best:  115491.625\n",
      "iter:  359  Current_loss:  114352.359375 best:  115491.625\n",
      "iter:  360  Current_loss:  114353.59375 best:  115491.625\n",
      "iter:  361  Current_loss:  114394.25 best:  115491.625\n",
      "iter:  362  Current_loss:  114391.75 best:  115491.625\n",
      "iter:  363  Current_loss:  114396.851562 best:  115491.625\n",
      "iter:  364  Current_loss:  114367.460938 best:  115491.625\n",
      "iter:  364 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  365  Current_loss:  114349.9375 best:  115491.625\n",
      "iter:  366  Current_loss:  114270.421875 best:  115491.625\n",
      "iter:  367  Current_loss:  114285.0 best:  115491.625\n",
      "iter:  368  Current_loss:  114243.054688 best:  115491.625\n",
      "iter:  369  Current_loss:  114240.570312 best:  115491.625\n",
      "iter:  370  Current_loss:  114214.09375 best:  115491.625\n",
      "iter:  371  Current_loss:  114200.164062 best:  115491.625\n",
      "iter:  372  Current_loss:  114136.523438 best:  115491.625\n",
      "iter:  373  Current_loss:  114002.8125 best:  115491.625\n",
      "iter:  374  Current_loss:  113984.34375 best:  115491.625\n",
      "iter:  374 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  375  Current_loss:  113943.78125 best:  115491.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  376  Current_loss:  113953.679688 best:  115491.625\n",
      "iter:  377  Current_loss:  113941.9375 best:  115491.625\n",
      "iter:  378  Current_loss:  113935.109375 best:  115491.625\n",
      "iter:  379  Current_loss:  113891.929688 best:  115491.625\n",
      "iter:  380  Current_loss:  113926.835938 best:  115491.625\n",
      "iter:  381  Current_loss:  113912.34375 best:  115491.625\n",
      "iter:  382  Current_loss:  113900.289062 best:  115491.625\n",
      "iter:  383  Current_loss:  113864.765625 best:  115491.625\n",
      "iter:  384  Current_loss:  113860.023438 best:  115491.625\n",
      "iter:  384 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  385  Current_loss:  113855.835938 best:  115491.625\n",
      "iter:  386  Current_loss:  113793.078125 best:  115491.625\n",
      "iter:  387  Current_loss:  113740.046875 best:  115491.625\n",
      "iter:  388  Current_loss:  113746.523438 best:  115491.625\n",
      "iter:  389  Current_loss:  113721.25 best:  115491.625\n",
      "iter:  390  Current_loss:  113676.78125 best:  115491.625\n",
      "iter:  391  Current_loss:  113659.34375 best:  115491.625\n",
      "iter:  392  Current_loss:  113628.140625 best:  115491.625\n",
      "iter:  393  Current_loss:  113589.648438 best:  115491.625\n",
      "iter:  394  Current_loss:  113578.75 best:  115491.625\n",
      "iter:  394 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  395  Current_loss:  113562.078125 best:  115491.625\n",
      "iter:  396  Current_loss:  113549.648438 best:  115491.625\n",
      "iter:  397  Current_loss:  113519.460938 best:  115491.625\n",
      "iter:  398  Current_loss:  113505.296875 best:  115491.625\n",
      "iter:  399  Current_loss:  113461.8125 best:  115491.625\n",
      "iter:  400  Current_loss:  113466.578125 best:  115491.625\n",
      "[400]\ttrain-rmse:93239.5\teval-rmse:113429\n",
      "iter:  401  Current_loss:  113428.820312 best:  115491.625\n",
      "iter:  402  Current_loss:  113417.78125 best:  115491.625\n",
      "iter:  403  Current_loss:  113394.84375 best:  115491.625\n",
      "iter:  404  Current_loss:  113387.695312 best:  115491.625\n",
      "iter:  404 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  405  Current_loss:  113406.164062 best:  115491.625\n",
      "iter:  406  Current_loss:  113401.71875 best:  115491.625\n",
      "iter:  407  Current_loss:  113416.945312 best:  115491.625\n",
      "iter:  408  Current_loss:  113415.296875 best:  115491.625\n",
      "iter:  409  Current_loss:  113360.914062 best:  115491.625\n",
      "iter:  410  Current_loss:  113340.0 best:  115491.625\n",
      "iter:  411  Current_loss:  113313.25 best:  115491.625\n",
      "iter:  412  Current_loss:  113283.117188 best:  115491.625\n",
      "iter:  413  Current_loss:  113260.882812 best:  115491.625\n",
      "iter:  414  Current_loss:  113290.617188 best:  115491.625\n",
      "iter:  414 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  415  Current_loss:  113232.671875 best:  115491.625\n",
      "iter:  416  Current_loss:  113220.953125 best:  115491.625\n",
      "iter:  417  Current_loss:  113180.890625 best:  115491.625\n",
      "iter:  418  Current_loss:  113142.734375 best:  115491.625\n",
      "iter:  419  Current_loss:  113128.78125 best:  115491.625\n",
      "iter:  420  Current_loss:  113175.539062 best:  115491.625\n",
      "iter:  421  Current_loss:  113162.367188 best:  115491.625\n",
      "iter:  422  Current_loss:  113148.234375 best:  115491.625\n",
      "iter:  423  Current_loss:  113042.976562 best:  115491.625\n",
      "iter:  424  Current_loss:  113035.390625 best:  115491.625\n",
      "iter:  424 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  425  Current_loss:  113039.835938 best:  115491.625\n",
      "iter:  426  Current_loss:  113028.351562 best:  115491.625\n",
      "iter:  427  Current_loss:  112919.15625 best:  115491.625\n",
      "iter:  428  Current_loss:  112904.929688 best:  115491.625\n",
      "iter:  429  Current_loss:  112881.0 best:  115491.625\n",
      "iter:  430  Current_loss:  112898.023438 best:  115491.625\n",
      "iter:  431  Current_loss:  112816.617188 best:  115491.625\n",
      "iter:  432  Current_loss:  112786.429688 best:  115491.625\n",
      "iter:  433  Current_loss:  112748.5 best:  115491.625\n",
      "iter:  434  Current_loss:  112739.570312 best:  115491.625\n",
      "iter:  434 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  435  Current_loss:  112709.164062 best:  115491.625\n",
      "iter:  436  Current_loss:  112700.445312 best:  115491.625\n",
      "iter:  437  Current_loss:  112727.476562 best:  115491.625\n",
      "iter:  438  Current_loss:  112712.976562 best:  115491.625\n",
      "iter:  439  Current_loss:  112697.226562 best:  115491.625\n",
      "iter:  440  Current_loss:  112694.429688 best:  115491.625\n",
      "iter:  441  Current_loss:  112685.460938 best:  115491.625\n",
      "iter:  442  Current_loss:  112674.109375 best:  115491.625\n",
      "iter:  443  Current_loss:  112660.320312 best:  115491.625\n",
      "iter:  444  Current_loss:  112633.992188 best:  115491.625\n",
      "iter:  444 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  445  Current_loss:  112631.617188 best:  115491.625\n",
      "iter:  446  Current_loss:  112601.523438 best:  115491.625\n",
      "iter:  447  Current_loss:  112555.015625 best:  115491.625\n",
      "iter:  448  Current_loss:  112542.84375 best:  115491.625\n",
      "iter:  449  Current_loss:  112519.625 best:  115491.625\n",
      "iter:  450  Current_loss:  112502.53125 best:  115491.625\n",
      "[450]\ttrain-rmse:91126.2\teval-rmse:112493\n",
      "iter:  451  Current_loss:  112492.679688 best:  115491.625\n",
      "iter:  452  Current_loss:  112467.320312 best:  115491.625\n",
      "iter:  453  Current_loss:  112450.648438 best:  115491.625\n",
      "iter:  454  Current_loss:  112367.960938 best:  115491.625\n",
      "iter:  454 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  455  Current_loss:  112369.632812 best:  115491.625\n",
      "iter:  456  Current_loss:  112356.390625 best:  115491.625\n",
      "iter:  457  Current_loss:  112360.1875 best:  115491.625\n",
      "iter:  458  Current_loss:  112373.117188 best:  115491.625\n",
      "iter:  459  Current_loss:  112373.382812 best:  115491.625\n",
      "iter:  460  Current_loss:  112382.140625 best:  115491.625\n",
      "iter:  461  Current_loss:  112364.789062 best:  115491.625\n",
      "iter:  462  Current_loss:  112337.460938 best:  115491.625\n",
      "iter:  463  Current_loss:  112361.039062 best:  115491.625\n",
      "iter:  464  Current_loss:  112311.734375 best:  115491.625\n",
      "iter:  464 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  465  Current_loss:  112307.484375 best:  115491.625\n",
      "iter:  466  Current_loss:  112307.8125 best:  115491.625\n",
      "iter:  467  Current_loss:  112307.953125 best:  115491.625\n",
      "iter:  468  Current_loss:  112308.625 best:  115491.625\n",
      "iter:  469  Current_loss:  112316.171875 best:  115491.625\n",
      "iter:  470  Current_loss:  112284.304688 best:  115491.625\n",
      "iter:  471  Current_loss:  112260.351562 best:  115491.625\n",
      "iter:  472  Current_loss:  112240.398438 best:  115491.625\n",
      "iter:  473  Current_loss:  112279.84375 best:  115491.625\n",
      "iter:  474  Current_loss:  112268.671875 best:  115491.625\n",
      "iter:  474 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  475  Current_loss:  112272.0 best:  115491.625\n",
      "iter:  476  Current_loss:  112268.351562 best:  115491.625\n",
      "iter:  477  Current_loss:  112281.46875 best:  115491.625\n",
      "iter:  478  Current_loss:  112291.007812 best:  115491.625\n",
      "iter:  479  Current_loss:  112281.789062 best:  115491.625\n",
      "iter:  480  Current_loss:  112286.617188 best:  115491.625\n",
      "iter:  481  Current_loss:  112270.742188 best:  115491.625\n",
      "iter:  482  Current_loss:  112292.898438 best:  115491.625\n",
      "iter:  483  Current_loss:  112311.421875 best:  115491.625\n",
      "iter:  484  Current_loss:  112269.734375 best:  115491.625\n",
      "iter:  484 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  485  Current_loss:  112266.039062 best:  115491.625\n",
      "iter:  486  Current_loss:  112285.242188 best:  115491.625\n",
      "iter:  487  Current_loss:  112291.28125 best:  115491.625\n",
      "iter:  488  Current_loss:  112276.8125 best:  115491.625\n",
      "iter:  489  Current_loss:  112257.773438 best:  115491.625\n",
      "iter:  490  Current_loss:  112228.617188 best:  115491.625\n",
      "iter:  491  Current_loss:  112182.570312 best:  115491.625\n",
      "iter:  492  Current_loss:  112175.34375 best:  115491.625\n",
      "iter:  493  Current_loss:  112153.054688 best:  115491.625\n",
      "iter:  494  Current_loss:  112157.351562 best:  115491.625\n",
      "iter:  494 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  495  Current_loss:  112148.351562 best:  115491.625\n",
      "iter:  496  Current_loss:  112137.625 best:  115491.625\n",
      "iter:  497  Current_loss:  112117.09375 best:  115491.625\n",
      "iter:  498  Current_loss:  112073.507812 best:  115491.625\n",
      "iter:  499  Current_loss:  112039.460938 best:  115491.625\n",
      "iter:  500  Current_loss:  112022.484375 best:  115491.625\n",
      "[500]\ttrain-rmse:89164.8\teval-rmse:112017\n",
      "iter:  501  Current_loss:  112017.28125 best:  115491.625\n",
      "iter:  502  Current_loss:  111964.671875 best:  115491.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  503  Current_loss:  111972.726562 best:  115491.625\n",
      "iter:  504  Current_loss:  111971.117188 best:  115491.625\n",
      "iter:  504 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  505  Current_loss:  111974.375 best:  115491.625\n",
      "iter:  506  Current_loss:  111925.015625 best:  115491.625\n",
      "iter:  507  Current_loss:  111921.359375 best:  115491.625\n",
      "iter:  508  Current_loss:  111908.15625 best:  115491.625\n",
      "iter:  509  Current_loss:  111883.15625 best:  115491.625\n",
      "iter:  510  Current_loss:  111858.710938 best:  115491.625\n",
      "iter:  511  Current_loss:  111826.039062 best:  115491.625\n",
      "iter:  512  Current_loss:  111800.164062 best:  115491.625\n",
      "iter:  513  Current_loss:  111800.765625 best:  115491.625\n",
      "iter:  514  Current_loss:  111789.023438 best:  115491.625\n",
      "iter:  514 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  515  Current_loss:  111783.492188 best:  115491.625\n",
      "iter:  516  Current_loss:  111785.53125 best:  115491.625\n",
      "iter:  517  Current_loss:  111777.4375 best:  115491.625\n",
      "iter:  518  Current_loss:  111773.546875 best:  115491.625\n",
      "iter:  519  Current_loss:  111772.609375 best:  115491.625\n",
      "iter:  520  Current_loss:  111739.75 best:  115491.625\n",
      "iter:  521  Current_loss:  111709.273438 best:  115491.625\n",
      "iter:  522  Current_loss:  111708.757812 best:  115491.625\n",
      "iter:  523  Current_loss:  111685.023438 best:  115491.625\n",
      "iter:  524  Current_loss:  111688.789062 best:  115491.625\n",
      "iter:  524 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  525  Current_loss:  111686.65625 best:  115491.625\n",
      "iter:  526  Current_loss:  111689.382812 best:  115491.625\n",
      "iter:  527  Current_loss:  111662.679688 best:  115491.625\n",
      "iter:  528  Current_loss:  111648.671875 best:  115491.625\n",
      "iter:  529  Current_loss:  111636.546875 best:  115491.625\n",
      "iter:  530  Current_loss:  111629.21875 best:  115491.625\n",
      "iter:  531  Current_loss:  111597.125 best:  115491.625\n",
      "iter:  532  Current_loss:  111538.0625 best:  115491.625\n",
      "iter:  533  Current_loss:  111502.890625 best:  115491.625\n",
      "iter:  534  Current_loss:  111498.742188 best:  115491.625\n",
      "iter:  534 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  535  Current_loss:  111478.632812 best:  115491.625\n",
      "iter:  536  Current_loss:  111474.9375 best:  115491.625\n",
      "iter:  537  Current_loss:  111525.484375 best:  115491.625\n",
      "iter:  538  Current_loss:  111476.75 best:  115491.625\n",
      "iter:  539  Current_loss:  111478.734375 best:  115491.625\n",
      "iter:  540  Current_loss:  111470.8125 best:  115491.625\n",
      "iter:  541  Current_loss:  111427.484375 best:  115491.625\n",
      "iter:  542  Current_loss:  111442.445312 best:  115491.625\n",
      "iter:  543  Current_loss:  111448.90625 best:  115491.625\n",
      "iter:  544  Current_loss:  111446.015625 best:  115491.625\n",
      "iter:  544 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  545  Current_loss:  111432.382812 best:  115491.625\n",
      "iter:  546  Current_loss:  111424.09375 best:  115491.625\n",
      "iter:  547  Current_loss:  111413.640625 best:  115491.625\n",
      "iter:  548  Current_loss:  111425.203125 best:  115491.625\n",
      "iter:  549  Current_loss:  111377.625 best:  115491.625\n",
      "iter:  550  Current_loss:  111373.835938 best:  115491.625\n",
      "[550]\ttrain-rmse:87295.3\teval-rmse:111316\n",
      "iter:  551  Current_loss:  111316.164062 best:  115491.625\n",
      "iter:  552  Current_loss:  111329.609375 best:  115491.625\n",
      "iter:  553  Current_loss:  111288.6875 best:  115491.625\n",
      "iter:  554  Current_loss:  111257.617188 best:  115491.625\n",
      "iter:  554 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  555  Current_loss:  111259.585938 best:  115491.625\n",
      "iter:  556  Current_loss:  111293.617188 best:  115491.625\n",
      "iter:  557  Current_loss:  111277.921875 best:  115491.625\n",
      "iter:  558  Current_loss:  111295.445312 best:  115491.625\n",
      "iter:  559  Current_loss:  111284.65625 best:  115491.625\n",
      "iter:  560  Current_loss:  111270.78125 best:  115491.625\n",
      "iter:  561  Current_loss:  111206.007812 best:  115491.625\n",
      "iter:  562  Current_loss:  111192.664062 best:  115491.625\n",
      "iter:  563  Current_loss:  111179.960938 best:  115491.625\n",
      "iter:  564  Current_loss:  111158.914062 best:  115491.625\n",
      "iter:  564 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  565  Current_loss:  111160.179688 best:  115491.625\n",
      "iter:  566  Current_loss:  111150.101562 best:  115491.625\n",
      "iter:  567  Current_loss:  111148.320312 best:  115491.625\n",
      "iter:  568  Current_loss:  111114.882812 best:  115491.625\n",
      "iter:  569  Current_loss:  111093.0 best:  115491.625\n",
      "iter:  570  Current_loss:  111084.820312 best:  115491.625\n",
      "iter:  571  Current_loss:  111080.976562 best:  115491.625\n",
      "iter:  572  Current_loss:  111089.265625 best:  115491.625\n",
      "iter:  573  Current_loss:  111089.484375 best:  115491.625\n",
      "iter:  574  Current_loss:  111079.085938 best:  115491.625\n",
      "iter:  574 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  575  Current_loss:  111083.125 best:  115491.625\n",
      "iter:  576  Current_loss:  111089.695312 best:  115491.625\n",
      "iter:  577  Current_loss:  111086.015625 best:  115491.625\n",
      "iter:  578  Current_loss:  111088.34375 best:  115491.625\n",
      "iter:  579  Current_loss:  111084.648438 best:  115491.625\n",
      "iter:  580  Current_loss:  111076.90625 best:  115491.625\n",
      "iter:  581  Current_loss:  111089.867188 best:  115491.625\n",
      "iter:  582  Current_loss:  111082.09375 best:  115491.625\n",
      "iter:  583  Current_loss:  111084.328125 best:  115491.625\n",
      "iter:  584  Current_loss:  111096.992188 best:  115491.625\n",
      "iter:  584 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  585  Current_loss:  111092.921875 best:  115491.625\n",
      "iter:  586  Current_loss:  111091.054688 best:  115491.625\n",
      "iter:  587  Current_loss:  111088.453125 best:  115491.625\n",
      "iter:  588  Current_loss:  111052.671875 best:  115491.625\n",
      "iter:  589  Current_loss:  111011.023438 best:  115491.625\n",
      "iter:  590  Current_loss:  110996.515625 best:  115491.625\n",
      "iter:  591  Current_loss:  110976.015625 best:  115491.625\n",
      "iter:  592  Current_loss:  110990.929688 best:  115491.625\n",
      "iter:  593  Current_loss:  111005.648438 best:  115491.625\n",
      "iter:  594  Current_loss:  110987.507812 best:  115491.625\n",
      "iter:  594 reducing lr --   old_lr:  0.1  new_lr:  0.05\n",
      "iter:  595  Current_loss:  110990.46875 best:  115491.625\n",
      "iter:  596  Current_loss:  110971.390625 best:  115491.625\n",
      "iter:  597  Current_loss:  110941.78125 best:  115491.625\n",
      "iter:  598  Current_loss:  110929.195312 best:  115491.625\n",
      "iter:  599  Current_loss:  110903.859375 best:  115491.625\n",
      "[599]\ttrain-rmse:85665.5\teval-rmse:110910\n"
     ]
    }
   ],
   "source": [
    "model1 = run_gbm(dtrain, deval, params,progress, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=600, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBRegressor(n_estimators=num_epochs, learning_rate=0.1)\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=[\"error\", \"rmse\"], eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min customEval: 110903.859375\n",
      "min Eval: 111064.328125\n"
     ]
    }
   ],
   "source": [
    "# retrieve performance metrics\n",
    "results = model.evals_result()\n",
    "#results1 = model1.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(25, epochs)\n",
    "\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(10, 10))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'][25::], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'][25::], label='Eval')\n",
    "ax.plot(x_axis, progress['train']['rmse'][25::], label = 'customTrain')\n",
    "ax.plot(x_axis, progress['eval']['rmse'][25::], label = 'customEval')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "\n",
    "pyplot.show()\n",
    "print(\"min customEval:\", min(progress['eval']['rmse']))\n",
    "print(\"min Eval:\", min(results['validation_1']['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classification error\n",
    "# fig, ax = pyplot.subplots()\n",
    "# ax.plot(x_axis, results['validation_0']['error'][25::], label='Train')\n",
    "# ax.plot(x_axis, results['validation_1']['error'][25::], label='Test')\n",
    "# ax.legend()\n",
    "# pyplot.ylabel('Classification Error')\n",
    "# pyplot.title('XGBoost Classification Error')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
